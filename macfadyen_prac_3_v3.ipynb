{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nithecs-biomath/mini-schools/blob/main/macfadyen_prac_3_v3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fa473b21",
      "metadata": {
        "id": "fa473b21"
      },
      "source": [
        "<style>\n",
        "/* tighten heading–paragraph gap */\n",
        "h1, h2, h3, h4, h5, h6 { margin-bottom: 0.25rem; }\n",
        "\n",
        "/* tighten gap above any paragraph that follows a heading */\n",
        "h1 + p, h2 + p, h3 + p, h4 + p, h5 + p, h6 + p { margin-top: 0.25rem; }\n",
        "\n",
        "/* justify body text */\n",
        "p { text-align: justify; }\n",
        "</style>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e7c75b46",
      "metadata": {
        "id": "e7c75b46"
      },
      "source": [
        "# Monitoring Biodiversity with Data Cubes: Techniques and Applications for Open Science\n",
        "\n",
        "**Sandra MacFadyen** (macfadyen@sun.ac.za)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2300ec4b",
      "metadata": {
        "id": "2300ec4b"
      },
      "source": [
        "### Ecological Complexity and Biodiversity Mini-School\n",
        "Hosted by the [National Institute for Theoretical and Computational Sciences](https://nithecs.ac.za/) (NITheCS) through the Complexity in Biological Systems (E5) research programme, this four-part Mini-school—under Work Package 1: Ecological Complexity and Biodiversity—equips participants with practical and conceptual skills to harness the power of environmental data cubes for biodiversity monitoring and ecological modelling.  \n",
        "\n",
        "The course is designed for postgraduate students, early-career researchers, conservation professionals, and environmental data scientists interested in open, scalable, and reproducible workflows. It introduces participants to the use of multi-dimensional spatiotemporal data structures—known as \"Data Cubes\"—to monitor ecological patterns and processes across space and time. By combining open-access species-occurrence records from the [Global Biodiversity Information Facility](https://www.gbif.org/) (GBIF), environmental, and trait data with cloud-based tools like [Google Earth Engine](https://earthengine.google.com/) through [Google Colab](https://colab.google/), participants will learn how to build and apply biodiversity data cubes for analytical and decision-support purposes.\n",
        "Rooted in the principles of Open Science, this Mini-school emphasises transparency, accessibility, and collaborative research. Each session aligns with one or more of the UN [Sustainable Development Goals](https://sdgs.un.org/goals) (SDGs), highlighting the societal relevance of biodiversity informatics.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2d81cf7f",
      "metadata": {
        "id": "2d81cf7f"
      },
      "source": [
        "### Work Package 1: Ecological Complexity and Biodiversity\n",
        "This Mini-school is presented as part of Work Package 1 (WP1): Ecological Complexity and Biodiversity. WP1 explores how biodiversity patterns and ecological processes emerge, interact, and change across spatial, temporal, and biological scales. It aims to develop quantitative frameworks and open tools to better understand and monitor biodiversity in a changing world.\n",
        "WP1 is structured around five core objectives:\n",
        "\n",
        "1.  **Biodiversity Entropy and Symmetry Across Scales**: Develop a unified theoretical framework to harmonise biodiversity metrics across scales, reducing bias and exploring eco-evolutionary processes through the lens of entropy and symmetry.\n",
        "2.\t**Structural Emergence in Complex Adaptive Networks**: Investigate the stability, resilience, and emergent properties of adaptive ecological networks, from microbiomes to food webs, in response to both internal dynamics and external environmental shifts.\n",
        "3.\t**Biodiversity Dynamics in Protected Landscapes**: Quantify, model, and map biodiversity—from genes to ecosystems—across protected areas to understand ecological functioning and inform conservation policy and planning.\n",
        "4.\t**Spatiotemporal Dynamics and Adaptive Interactions in Ecosystems**: Model species distributions, biome transitions, and ecological disequilibria under climate change, using biodiversity metrics, movement data, and evolutionary game theory to explore adaptive interactions.\n",
        "5.\t**Innovative Computational Tools for Biodiversity Monitoring**: Develop open, scalable software tools—including a Biodiversity Informatics Hub and smart sensor systems—to support long-term biodiversity monitoring in African landscapes. This also includes building a research network across African protected areas to co-design training and capacity-building tailored to regional conservation needs.\n",
        "\n",
        "This Mini-school contributes directly to objectives 3, 4, and 5.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3ab0a0bc",
      "metadata": {
        "id": "3ab0a0bc"
      },
      "source": [
        "### Biodiversity\n",
        "Biodiversity—the variety of life on Earth—is the foundation of ecosystem health and resilience. It encompasses the diversity of genes, species, and ecosystems, and includes the complex interactions among organisms and their environments. This variety supports essential ecosystem functions such as nutrient cycling, pollination, and climate regulation, making biodiversity critical not only for nature but also for human well-being.\n",
        "Biodiversity operates across multiple dimensions and scales:\n",
        "\n",
        "-   **Composition**, **structure**, and **function** are key facets of biodiversity. Composition includes the identity and variety of species and genes present. Structure refers to the spatial organisation of biodiversity—species abundance, distribution patterns, and habitat arrangement. Function encompasses ecological processes and interactions such as energy flow and food web dynamics.\n",
        "-   **Genetic diversity** refers to the variation of genes within species, enabling populations to adapt to changing conditions.\n",
        "-   **Species diversity** reflects the number and variety of species in a given ecosystem or region.\n",
        "-   **Ecosystem diversity** captures the range of different habitats—such as forests, wetlands, savannas, and coral reefs—that sustain life and ecosystem services.\n",
        "\n",
        "\n",
        "Global threats like habitat loss, climate change, invasive species, and land degradation are causing rapid biodiversity declines. Monitoring biodiversity change across appropriate spatial and temporal scales has therefore become an urgent global priority. However, to respond effectively, we need globally relevant, standardised and scalable solutions that can track biodiversity patterns over time, inform conservation policy, and support ecosystem management.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "83e1e754",
      "metadata": {
        "id": "83e1e754"
      },
      "source": [
        "### **Practical Session 3: Ecological Modelling with Data Cubes**\n",
        "> In this hands-on session, participants will explore analytical approaches for modelling species distributions and environmental associations using biodiversity data cubes. Focusing on SDG 13 (Climate Action), the session will guide participants in investigating climate-related biodiversity responses and potential adaptation strategies.\n",
        "By the end of the session, participants will be able to:\n",
        ">\n",
        "> 1.\tConstruct a biodiversity data cube using open-access species occurrence data from GBIF, processed via Google Colab and Earth Engine.\n",
        "> 2.\tVisualise ecological patterns interactively using spatial dashboards and maps.\n",
        "> 3.\tAnalyse species diversity in relation to climatic and environmental variables, such as rainfall, temperature, and elevation, using summary statistics and visualisations.\n",
        ">\n",
        "> This session bridges computational ecology with applied conservation science, demonstrating how data-driven tools can inform real-world biodiversity management.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "475163a0",
      "metadata": {
        "id": "475163a0"
      },
      "source": [
        "---\n",
        "#### 1.\tSETUP\n",
        "> This section prepares the working environment for the data cube analysis. It ensures that all necessary packages are installed and available, initializes access to external services (e.g. Google Colab and Earth Engine), and connects to storage locations where datasets are kept or generated.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "caa34377",
      "metadata": {
        "id": "caa34377"
      },
      "source": [
        "##### 1.2.\tInstall packages\n",
        "Begin by installing the required Python libraries used throughout the workflow. These include geospatial, data science, and biodiversity informatics packages essential for data processing, analysis, and visualization.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a02fc324",
      "metadata": {
        "id": "a02fc324"
      },
      "outputs": [],
      "source": [
        "# -- Install core libraries (5–6 min)\n",
        "# !pip -q install geopandas folium geemap earthengine-api xarray rioxarray rasterio netcdf4 scikit-learn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "da91171e",
      "metadata": {
        "id": "da91171e"
      },
      "outputs": [],
      "source": [
        "# !pip -q install contextily localtileserver pygbif matplotlib_scalebar google.colab xarray_leaflet"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8a6e717b",
      "metadata": {
        "id": "8a6e717b"
      },
      "source": [
        "##### 1.3.\tLoad packages\n",
        "Once installed, the relevant packages are imported into the working session. This step makes their functions available for use in the script and ensures compatibility between modules.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "671b9979",
      "metadata": {
        "id": "671b9979"
      },
      "outputs": [],
      "source": [
        "# ‑‑ Imports\n",
        "import io\n",
        "import os\n",
        "import zipfile\n",
        "import requests\n",
        "import time   # polite pause between calls\n",
        "\n",
        "import pandas as pd\n",
        "import geopandas as gpd\n",
        "import numpy as np\n",
        "import xarray as xr\n",
        "import folium, branca\n",
        "import ee, geemap\n",
        "import matplotlib.pyplot as plt\n",
        "import contextily as cx # optional: adds basemap\n",
        "import rasterio\n",
        "import localtileserver\n",
        "\n",
        "from io import BytesIO\n",
        "from sklearn.linear_model import LinearRegression\n",
        "# from google.colab import files\n",
        "from pygbif import occurrences\n",
        "from shapely.geometry import box, Polygon\n",
        "from matplotlib_scalebar.scalebar import ScaleBar  # optional\n",
        "from matplotlib.colors import LinearSegmentedColormap, Normalize\n",
        "\n",
        "from rasterio.transform import from_origin\n",
        "from rasterio.features import rasterize, geometry_mask\n",
        "from rasterio.plot import show"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bc9855a6",
      "metadata": {
        "id": "bc9855a6"
      },
      "source": [
        "##### 1.4.\tInitialise packages\n",
        "Some packages require additional setup or authentication steps—such as logging in to Google services or initializing Earth Engine—to enable access to cloud-based resources and tools.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2c7ca9e1",
      "metadata": {
        "id": "2c7ca9e1"
      },
      "outputs": [],
      "source": [
        "# -- Initialise Google Earth Engine (requires one‑off authentication)\n",
        "# Trigger the authentication flow\n",
        "ee.Authenticate()\n",
        "\n",
        "# -- Initialize the Project (see https://console.cloud.google.com/)\n",
        "# ee.Initialize(project='ee-biomath')\n",
        "ee.Initialize(project='ee-nithecs')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2eccb33d",
      "metadata": {
        "id": "2eccb33d"
      },
      "source": [
        "##### 1.5.\tMount drive\n",
        "This step links a cloud storage account (e.g. Google Drive) to the session. It allows read/write access to local folders in the cloud, which are used to store input data, intermediate outputs, and final results.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2fe0a992",
      "metadata": {
        "id": "2fe0a992"
      },
      "outputs": [],
      "source": [
        "# Mount Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "400b0aab",
      "metadata": {
        "id": "400b0aab"
      },
      "source": [
        "---\n",
        "#### 2.\tLOAD OCCURRENCE DATA\n",
        "> Species occurrence data form the foundation of biodiversity data cubes because they provide direct, spatially explicit records of where and when organisms have been observed. These data underpin our ability to quantify species distributions, assess patterns of species richness, and monitor changes in biodiversity over time. When structured appropriately, occurrence records can be aggregated into grid-based data cubes that support scalable, repeatable analyses across large regions and multiple time steps.\n",
        "> This section introduces several ways to access or import species occurrence data from both local and remote sources. Local data may include curated datasets collected from field surveys, research networks, or institutional archives. Online sources—such as the Global Biodiversity Information Facility (GBIF) or data repositories hosted on GitHub—offer access to millions of open-access records contributed by museums, monitoring programmes, and citizen science initiatives. Whether working with structured CSV files, APIs, or live database queries, users will learn how to load, clean, and format occurrence data for integration into biodiversity data cubes.\n",
        "> By combining these occurrence records with environmental and spatial metadata, we can begin to explore the composition, structure, and function of biodiversity across different ecological contexts—key elements discussed earlier in understanding biodiversity patterns and processes.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bd853b11",
      "metadata": {
        "id": "bd853b11"
      },
      "source": [
        "##### 2.1.\tRead from local drive\n",
        "Use pre-downloaded datasets stored in a local or mounted drive. This approach is useful for working offline or when using curated data from previous projects.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "00179f2e",
      "metadata": {
        "id": "00179f2e"
      },
      "outputs": [],
      "source": [
        "# Pick a local CSV/XLSX for upload\n",
        "uploaded = files.upload()\n",
        "# Expecting: table.csv\n",
        "local_fn = next(iter(uploaded))\n",
        "df_local = pd.read_csv(local_fn)\n",
        "print(df_local.head())"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bb520c84",
      "metadata": {
        "id": "bb520c84"
      },
      "source": [
        "##### 2.2.\tImport from Github\n",
        "Access datasets that have been shared via a public [GitHub](https://github.com/) repository. This promotes reproducibility and ensures that everyone uses the same standard input files.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fdb2abc5",
      "metadata": {
        "id": "fdb2abc5"
      },
      "outputs": [],
      "source": [
        "url = \"https://raw.githubusercontent.com/nithecs-biomath/mini-schools/main/data/sample_data_SA.csv\"\n",
        "# df_git = pd.read_csv(url) # Quick version\n",
        "# Explicit tab delimiter or # Option 2 – same result, slightly shorter # df_git = pd.read_table(url)\n",
        "df_git = pd.read_csv(url, sep='\\t')\n",
        "print(f\"Rows: {len(df_git):,}\")\n",
        "df_git.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3742f35c",
      "metadata": {
        "id": "3742f35c"
      },
      "source": [
        "##### 2.3.\tAccess GBIF data\n",
        "Query the [Global Biodiversity Information Facility](https://www.gbif.org/) (GBIF) directly from within the session. This allows users to search, filter, and download up-to-date species occurrence records for a target taxon and region."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Function `load_from_url_zip()`"
      ],
      "metadata": {
        "id": "3iIE5vpyhQr-"
      },
      "id": "3iIE5vpyhQr-"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "31483e3d",
      "metadata": {
        "id": "31483e3d"
      },
      "outputs": [],
      "source": [
        "# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
        "# Create a new function - load_from_url_zip()\n",
        "# Downloads a GBIF (or any) ZIP file from URL, extracts first tab-delimited file\n",
        "# Returns the contents as a pandas.DataFrame\n",
        "def load_from_url_zip(zip_url):\n",
        "    response = requests.get(zip_url)\n",
        "    if response.status_code == 200:\n",
        "        zip_file = BytesIO(response.content)\n",
        "        with zipfile.ZipFile(zip_file, 'r') as z:\n",
        "            for filename in z.namelist():\n",
        "                if filename.endswith('.csv'):  # Adjust based on file content\n",
        "                    with z.open(filename) as f:\n",
        "                        # Skip bad lines if there's a format error\n",
        "                        df = pd.read_csv(f, sep='\\t', on_bad_lines='skip')  # Use sep='\\t' for tab-separated values\n",
        "    else:\n",
        "        print(f\"Failed to download the file. Status code: {response.status_code}\")\n",
        "        return None\n",
        "    return df\n",
        "# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Run `load_from_url_zip()`"
      ],
      "metadata": {
        "id": "GwLRc7llhjQ4"
      },
      "id": "GwLRc7llhjQ4"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e6cca454",
      "metadata": {
        "id": "e6cca454"
      },
      "outputs": [],
      "source": [
        "# Run load_from_url_zip()\n",
        "# URL path\n",
        "url_zip = 'https://api.gbif.org/v1/occurrence/download/request/0006880-241024112534372.zip' # 81,825 occurrences of Lepidoptera\n",
        "# url_zip = 'https://api.gbif.org/v1/occurrence/download/request/0021941-250525065834625.zip' # 2,337 occurrences of Ceratotherium simum\n",
        "# Run function\n",
        "df_url = load_from_url_zip(url_zip)\n",
        "\n",
        "# Specify the columns to keep\n",
        "df_url = df_url[['year', 'month', 'family', 'speciesKey', 'species', 'decimalLatitude', 'decimalLongitude']]\n",
        "print(f\"Rows: {len(df_url):,}\")\n",
        "df_url.head(4)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "33e730aa",
      "metadata": {
        "id": "33e730aa"
      },
      "source": [
        "---\n",
        "#### 3.\tSET AREA OF INTEREST AND GRID SIZE\n",
        "> This section defines the geographic scope of the analysis and sets the resolution for spatial aggregation. It forms the spatial structure for data cube construction.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1f1272b5",
      "metadata": {
        "id": "1f1272b5"
      },
      "source": [
        "##### 3.1.\tRead boundary shapefile\n",
        "Import a shapefile that outlines the study area. This serves as the spatial boundary within which all analyses will be performed.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "94eb3d95",
      "metadata": {
        "id": "94eb3d95"
      },
      "outputs": [],
      "source": [
        "# ---------------------------------------------------------------------\n",
        "# 1.  INPUTS (already prepared in earlier steps) -----------------------\n",
        "# grid_polys  : GeoDataFrame of quarter-degree cells\n",
        "#               (must contain a numeric 'spp_rich' column)\n",
        "# boundary_gdf: SA national boundary  (any polygon layer in EPSG:4326)\n",
        "# pts_gdf     : occurrence points     (optional, for scatter overlay)\n",
        "\n",
        "# ------------------------------------------------------------------\n",
        "# 1) upload all four files (.shp, .shx, .dbf, .prj)\n",
        "# files.upload()\n",
        "\n",
        "# (optional) dissolve into a single outline so you don’t plot internal borders\n",
        "# boundary_gdf = boundary_gdf.dissolve().reset_index(drop=True)\n",
        "\n",
        "# 2) read – they now reside in the current working directory\n",
        "# boundary_gdf = gpd.read_file(\"south_africa_provinces_lesotho_eswatini.shp\").to_crs(\"EPSG:4326\")\n",
        "boundary_gdf = gpd.read_file(\"https://raw.githubusercontent.com/nithecs-biomath/mini-schools/main/data/south_africa_provinces_lesotho_eswatini.shp\").to_crs(\"EPSG:4326\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "de1e79fa",
      "metadata": {
        "id": "de1e79fa"
      },
      "source": [
        "##### 3.2.\tGenerate grid\n",
        "Divide the study area into regularly spaced square or rectangular grid cells. These cells act as analytical units for summarising biodiversity metrics.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Function `make_grid_polygon()`"
      ],
      "metadata": {
        "id": "KkjSft4_h9dc"
      },
      "id": "KkjSft4_h9dc"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "52debf49",
      "metadata": {
        "id": "52debf49"
      },
      "outputs": [],
      "source": [
        "# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
        "# Create `make_grid_polygon()` function\n",
        "# --------------------------------------------------------------------\n",
        "# 0.  INPUTS  ---------------------------------------------------------\n",
        "# df_raw : original occurrence table\n",
        "# grid_res = 0.25            # 0.25° OR 250   (metres) depending on unit\n",
        "# grid_unit = \"deg\"          # \"deg\" | \"m\"\n",
        "# --------------------------------------------------------------------\n",
        "\n",
        "def make_grid_polygon(bounds, res, unit, crs):\n",
        "    \"\"\"\n",
        "    Create a GeoDataFrame of grid polygons covering *bounds*.\n",
        "    bounds : (xmin, ymin, xmax, ymax)\n",
        "    res    : resolution (deg or metres)\n",
        "    unit   : \"deg\" or \"m\"\n",
        "    crs    : CRS of the bounds\n",
        "    \"\"\"\n",
        "    xmin, ymin, xmax, ymax = bounds\n",
        "    # align bounds to multiples of res\n",
        "    if unit == \"deg\":\n",
        "        xmin = np.floor(xmin / res) * res\n",
        "        ymin = np.floor(ymin / res) * res\n",
        "        xmax = np.ceil(xmax / res) * res\n",
        "        ymax = np.ceil(ymax / res) * res\n",
        "    else:  # metres – bounds already projected\n",
        "        xmin = np.floor(xmin / res) * res\n",
        "        ymin = np.floor(ymin / res) * res\n",
        "        xmax = np.ceil(xmax / res) * res\n",
        "        ymax = np.ceil(ymax / res) * res\n",
        "\n",
        "    xs = np.arange(xmin, xmax, res)\n",
        "    ys = np.arange(ymin, ymax, res)\n",
        "    polys, ids, cx, cy = [], [], [], []\n",
        "    for i, x0 in enumerate(xs):\n",
        "        for j, y0 in enumerate(ys):\n",
        "            poly = box(x0, y0, x0 + res, y0 + res)\n",
        "            polys.append(poly)\n",
        "            ids.append(f\"g_{i}_{j}\")\n",
        "            cx.append(x0 + res / 2)\n",
        "            cy.append(y0 + res / 2)\n",
        "\n",
        "    gdf = gpd.GeoDataFrame(\n",
        "        {\"grid_id\": ids,\n",
        "         \"centroid_lon\": cx if unit == \"deg\" else None,\n",
        "         \"centroid_lat\": cy if unit == \"deg\" else None},\n",
        "        geometry=polys,\n",
        "        crs=crs\n",
        "    )\n",
        "    return gdf\n",
        "# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Function `species_grid_summary()`"
      ],
      "metadata": {
        "id": "TdEOplXziMt1"
      },
      "id": "TdEOplXziMt1"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e8716a2c",
      "metadata": {
        "id": "e8716a2c"
      },
      "outputs": [],
      "source": [
        "# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
        "# Create `species_grid_summary()` function that uses `make_grid_polygon()`\n",
        "# species_grid_summary()\n",
        "def species_grid_summary(df_raw,\n",
        "                         res=0.25,\n",
        "                         unit=\"deg\",\n",
        "                         lat=\"decimalLatitude\",\n",
        "                         lon=\"decimalLongitude\",\n",
        "                         year=\"year\",\n",
        "                         month=\"month\",\n",
        "                         species=\"species\",\n",
        "                         crs_deg=\"EPSG:4326\"):\n",
        "    \"\"\"\n",
        "    Builds a regular grid, assigns each record to a cell, and\n",
        "    returns:\n",
        "        grid_counts – tidy table (grid_id, year, month, species, count, lon, lat)\n",
        "        grid_gdf    – polygons + centroids + spp_rich + obs_sum\n",
        "    \"\"\"\n",
        "    # 1. Subset & drop NA coords--------------------------\n",
        "    cols = [year, month, species, lat, lon]\n",
        "    df = df_raw[cols].dropna(subset=[lat, lon]).copy()\n",
        "\n",
        "    # 2. Build GeoDataFrame of points (always start in EPSG:4326)--------------------------\n",
        "    gdf_pts = gpd.GeoDataFrame(\n",
        "        df,\n",
        "        geometry=gpd.points_from_xy(df[lon], df[lat]),\n",
        "        crs=crs_deg\n",
        "    )\n",
        "\n",
        "    # 3. If grid in metres, project to metric CRS (Web-Mercator here)--------------------------\n",
        "    if unit == \"m\":\n",
        "        gdf_pts = gdf_pts.to_crs(\"EPSG:3857\")\n",
        "\n",
        "    # 4. Generate grid polygons--------------------------\n",
        "    grid_gdf = make_grid_polygon(\n",
        "        bounds=gdf_pts.total_bounds,\n",
        "        res=res,\n",
        "        unit=unit,\n",
        "        crs=gdf_pts.crs\n",
        "    )\n",
        "\n",
        "    # 5. Spatial join: assign grid_id to each point--------------------------\n",
        "    joined = gpd.sjoin(gdf_pts,\n",
        "                       grid_gdf[[\"grid_id\", \"geometry\"]],\n",
        "                       how=\"inner\")\n",
        "\n",
        "    # 6. Aggregate counts: Per-species counts per grid × year × month--------------------------\n",
        "    grid_counts = (joined\n",
        "                   .groupby([\"grid_id\", year, month, species])\n",
        "                   .size()\n",
        "                   .reset_index(name=\"count\"))\n",
        "\n",
        "    # 7. Attach centroids in WGS-84 for easy plotting / export --------------------------\n",
        "    #  (always store lon/lat in WGS84 for clarity)\n",
        "    if unit == \"m\":\n",
        "        grid_centers = (grid_gdf\n",
        "                        .to_crs(\"EPSG:4326\")\n",
        "                        .assign(centroid=lambda d: d.geometry.centroid)\n",
        "                        .assign(\n",
        "                            centroid_lon=lambda d: d.centroid.x,\n",
        "                            centroid_lat=lambda d: d.centroid.y)[\n",
        "                            [\"grid_id\", \"centroid_lon\", \"centroid_lat\"]])\n",
        "    else:\n",
        "        grid_centers = grid_gdf[[\"grid_id\", \"centroid_lon\", \"centroid_lat\"]]\n",
        "\n",
        "    grid_counts = grid_counts.merge(grid_centers, on=\"grid_id\", how=\"left\")\n",
        "\n",
        "    # 8.  Calculate species richness & effort per cell --------------------------\n",
        "    richness = (grid_counts\n",
        "                .groupby(\"grid_id\")[\"species\"]\n",
        "                .nunique()\n",
        "                .reset_index(name=\"spp_rich\"))\n",
        "\n",
        "    effort   = (grid_counts\n",
        "                .groupby(\"grid_id\")[\"count\"]\n",
        "                .sum()\n",
        "                .reset_index(name=\"obs_sum\"))\n",
        "\n",
        "    grid_stats = richness.merge(effort, on=\"grid_id\")\n",
        "\n",
        "    # # Merge into grid_counts and fill empty cells with zeros--------------------------\n",
        "    # grid_counts = (grid_counts\n",
        "    #                .merge(grid_stats, on=\"grid_id\", how=\"left\")\n",
        "    #                .fillna({\"spp_rich\": 0, \"obs_sum\": 0}))\n",
        "\n",
        "    # Merge into grid polygons and fill empty cells with zeros--------------------------\n",
        "    grid_gdf = (grid_gdf\n",
        "                .merge(grid_stats, on=\"grid_id\", how=\"left\")\n",
        "                .fillna({\"spp_rich\": 0, \"obs_sum\": 0}))\n",
        "\n",
        "    return grid_counts, grid_gdf\n",
        "# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Run `species_grid_summary()` which calls `make_grid_polygon()`"
      ],
      "metadata": {
        "id": "m3fQiExJiR7z"
      },
      "id": "m3fQiExJiR7z"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6e2ac983",
      "metadata": {
        "id": "6e2ac983"
      },
      "outputs": [],
      "source": [
        "# --------------------------------------------------------------------\n",
        "# EXAMPLE USE  -------------------------------------------------------\n",
        "# df_raw = pd.read_csv(\"occurrence_table.tsv\", sep=\"\\t\")\n",
        "\n",
        "grid_counts, grid_polys = species_grid_summary(\n",
        "    df_url,\n",
        "    res=0.25,       # 0.25° (~28 km at equator)\n",
        "    unit=\"deg\"      # or \"m\" for metres (projects data to EPSG:3857)\n",
        ")\n",
        "\n",
        "print(grid_counts.head())\n",
        "print(grid_polys.head())"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "757681c3",
      "metadata": {
        "id": "757681c3"
      },
      "source": [
        "\n",
        "##### 3.3.\tVisualise static map\n",
        "Display the grid overlay on top of the study region using a map viewer. This step ensures the grid is correctly aligned and helps users explore the spatial configuration."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "15e58b6b",
      "metadata": {
        "id": "15e58b6b"
      },
      "outputs": [],
      "source": [
        "# ------------------------------------------------------------------\n",
        "# 1.  Aggregate counts per grid-cell centroid -----------------------\n",
        "cent_df = (grid_counts\n",
        "           .groupby(['centroid_lat', 'centroid_lon'], as_index=False)['count']\n",
        "           .sum()\n",
        "           .rename(columns={'count': 'sum_cnt'}))\n",
        "# ---------------------------------------------------------------\n",
        "# 1.  Square-root–transform richness\n",
        "cent_df[\"sqrt_sum_cnt\"] = np.sqrt(cent_df[\"sum_cnt\"])\n",
        "\n",
        "# ------------------------------------------------------------------\n",
        "# 2.  Custom colour ramp  (yellow→green→blue→red→black) -------------\n",
        "cmap = LinearSegmentedColormap.from_list(\n",
        "    \"YGBRB\", [\"yellow\", \"green\", \"blue\", \"red\", \"black\"], N=256)\n",
        "norm = Normalize(vmin=cent_df.sqrt_sum_cnt.min(),\n",
        "                 vmax=cent_df.sqrt_sum_cnt.max())\n",
        "\n",
        "# 3.  Scale point sizes (area ∝ √count) -----------------------------\n",
        "sizes = np.sqrt(cent_df.sqrt_sum_cnt)\n",
        "sizes = sizes * 200 / sizes.max()          # tweak the 200 factor to taste\n",
        "\n",
        "# ------------------------------------------------------------------\n",
        "# 4.  Create figure/axes *before* adding layers ---------------------\n",
        "fig, ax = plt.subplots(figsize=(8, 7))\n",
        "\n",
        "# 5.  Grid & boundary outlines  -------------------------------------\n",
        "grid_polys.to_crs(\"EPSG:4326\").boundary.plot(\n",
        "    ax=ax, color=\"lightgrey\", linewidth=0.4, zorder=1)\n",
        "\n",
        "boundary_gdf.to_crs(\"EPSG:4326\").boundary.plot(\n",
        "    ax=ax, color=\"green\", linewidth=1.0, zorder=2)\n",
        "\n",
        "# ------------------------------------------------------------------\n",
        "# 6.  Centroid scatter ----------------------------------------------\n",
        "sc = ax.scatter(\n",
        "    cent_df.centroid_lon,\n",
        "    cent_df.centroid_lat,\n",
        "    s=sizes,\n",
        "    c=cent_df.sqrt_sum_cnt,\n",
        "    cmap=cmap,\n",
        "    norm=norm,\n",
        "    edgecolor=\"k\",\n",
        "    linewidth=0.3,\n",
        "    alpha=0.85,\n",
        "    zorder=3\n",
        ")\n",
        "\n",
        "# ------------------------------------------------------------------\n",
        "# 7.  Cosmetics ------------------------------------------------------\n",
        "ax.set_title(\"Sampling effort per grid cell (sum of observations)\")\n",
        "ax.set_xlabel(\"Longitude\")\n",
        "ax.set_ylabel(\"Latitude\")\n",
        "ax.set_aspect(\"equal\")\n",
        "\n",
        "fig.colorbar(sc, ax=ax, shrink=0.75, label=\"Total observations (sum_cnt)\")\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2c087c51",
      "metadata": {
        "id": "2c087c51"
      },
      "source": [
        "---\n",
        "#### 4.\tSUMMARISE OCCURRENCES BY GRID ID\n",
        "> Once the spatial structure is set, species records are aggregated to grid cells. This section describes the derivation of key biodiversity indicators at the grid-cell level.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f05771de",
      "metadata": {
        "id": "f05771de"
      },
      "source": [
        "##### 4.1.\tCount total observations ~ Sampling effort\n",
        "Calculate the number of records per grid cell. This metric approximates sampling effort and is useful for identifying areas with data biases or coverage gaps.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "917be5b6",
      "metadata": {
        "id": "917be5b6"
      },
      "outputs": [],
      "source": [
        "# prompt: Transpose the data.frame and sum species richness across years (columns)\n",
        "# i.e. 'rounded_latitude' and 'rounded_longitude' will not be retained.\n",
        "# And plot results as a line chart, showing how species_richness changes over time.\n",
        "# Also include the total number of observations per year.\n",
        "\n",
        "# Calculate total observations per year (Corrected)\n",
        "obs_count = grid_counts.groupby(['centroid_lat', 'centroid_lon', 'year'])['count'].sum().unstack()\n",
        "total_observations_per_year = obs_count.sum(axis=0)  # Sum across all locations for each year\n",
        "total_observations_per_year.head(4)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1fcea60b",
      "metadata": {
        "id": "1fcea60b"
      },
      "source": [
        "##### 4.2.\tCalculate Species Richness\n",
        "Count the number of unique species observed in each grid cell. Species richness is a basic but widely used indicator of biodiversity.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a06319f3",
      "metadata": {
        "id": "a06319f3"
      },
      "outputs": [],
      "source": [
        "# prompt: Using dataframe summary:\n",
        "# Now count the number of unique species by 'rounded_latitude' and 'rounded_longitude' and 'year'.\n",
        "# The result should be a table with 'rounded_latitude' and 'rounded_longitude', and a\n",
        "# column for each year with the count of unique species i.e. species richness per year.\n",
        "# Also return zero instead of NaN for years without species.\n",
        "\n",
        "# Group by rounded_latitude, rounded_longitude, and year, and count unique species\n",
        "species_richness = grid_counts.groupby(['centroid_lat', 'centroid_lon', 'year'])['species'].nunique().unstack()\n",
        "\n",
        "# Fill NaN values with 0\n",
        "species_richness = species_richness.fillna(0)\n",
        "\n",
        "# Display the resulting table\n",
        "# print(species_richness)\n",
        "species_richness.head(4)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1538b8c6",
      "metadata": {
        "id": "1538b8c6"
      },
      "source": [
        "##### 4.3.\tResults - Line charts\n",
        "Plot time series or summary statistics (e.g. number of observations or richness) using line charts to explore trends across time or spatial resolution.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ad33af0f",
      "metadata": {
        "id": "ad33af0f"
      },
      "outputs": [],
      "source": [
        "# prompt: Transpose the data.frame and sum species richness across years (columns)\n",
        "# i.e. 'rounded_latitude' and 'rounded_longitude' will not be retained.\n",
        "# And plot results as a line chart, showing how species_richness changes over time\n",
        "\n",
        "# Reset the index to bring back 'qds_lat' and 'qds_lon' as columns\n",
        "species_richness = species_richness.reset_index()\n",
        "\n",
        "# Transpose the data and sum species richness across years\n",
        "species_richness_transposed = species_richness.set_index(['centroid_lat', 'centroid_lon']).transpose()\n",
        "species_richness_sum = species_richness_transposed.sum(axis=1)\n",
        "\n",
        "# Filter out non-numeric index values before plotting\n",
        "numeric_index = species_richness_sum.index[species_richness_sum.index.map(lambda x: isinstance(x, (int, float)))]\n",
        "species_richness_sum_filtered = species_richness_sum[numeric_index]\n",
        "\n",
        "\n",
        "# # Create a line chart of only Species Richness over time\n",
        "# plt.figure(figsize=(10, 6))\n",
        "# # Plot using the filtered data\n",
        "# plt.plot(species_richness_sum_filtered.index, species_richness_sum_filtered.values)\n",
        "# plt.xlabel('Year')\n",
        "# plt.ylabel('Species Richness')\n",
        "# plt.title('Species Richness Change Over Time')\n",
        "# plt.grid(True)\n",
        "# plt.show()\n",
        "\n",
        "# Create a line chart with two lines: Species Richness and Total Observations\n",
        "fig, ax1 = plt.subplots(figsize=(10, 6))\n",
        "\n",
        "# Plot Species Richness\n",
        "ax1.plot(species_richness_sum_filtered.index, species_richness_sum_filtered.values, 'b-', label='Species Richness')\n",
        "ax1.set_xlabel('Year')\n",
        "ax1.set_ylabel('Species Richness', color='b')\n",
        "ax1.tick_params('y', labelcolor='b')\n",
        "\n",
        "# Create a second y-axis for Total Observations\n",
        "ax2 = ax1.twinx()\n",
        "ax2.plot(total_observations_per_year.index, total_observations_per_year.values, 'r-', label='Total Observations')\n",
        "ax2.set_ylabel('Total Observations', color='r')\n",
        "ax2.tick_params('y', labelcolor='r')\n",
        "\n",
        "# Add title, grid, and legend\n",
        "plt.title('Species Richness and Total Observations Over Time')\n",
        "plt.grid(True)\n",
        "fig.tight_layout()  # Adjust layout to prevent overlapping labels\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "15c08322",
      "metadata": {
        "id": "15c08322"
      },
      "outputs": [],
      "source": [
        "# 1 – See the first few roads?\n",
        "print(species_richness_transposed.head())\n",
        "\n",
        "# 2 – What kind of object?\n",
        "type(species_richness_transposed)\n",
        "# e.g. pandas.core.frame.DataFrame\n",
        "\n",
        "# 3 – High-level structure\n",
        "species_richness_transposed.info()      # rows, columns, dtypes, memory\n",
        "\n",
        "# 4 – Dimensions and column data types\n",
        "species_richness_transposed.shape       # (n_rows, n_cols)\n",
        "species_richness_transposed.dtypes      # dtype of every column\n",
        "\n",
        "# 5 – Column names (if you need them)\n",
        "# list(species_richness_transposed.columns)\n",
        "\n",
        "# 6 – Numeric summary (optional)\n",
        "species_richness_transposed.describe().T"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a4f49c0b",
      "metadata": {
        "id": "a4f49c0b"
      },
      "source": [
        "##### 4.4.\tResults – Static Map\n",
        "Visualise spatial patterns of sampling effort and species richness using map outputs. These maps can reveal biodiversity hotspots and under-sampled areas."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "19c76d16",
      "metadata": {
        "id": "19c76d16"
      },
      "outputs": [],
      "source": [
        "# Build GeoDataFrame of points (always start in EPSG:4326)--------------------------\n",
        "crs_wgs84 = \"EPSG:4326\"\n",
        "gdf_pts = gpd.GeoDataFrame(grid_counts,\n",
        "                           geometry=gpd.points_from_xy(grid_counts['centroid_lon'],\n",
        "                                                       grid_counts['centroid_lat']),\n",
        "                                                       crs=crs_wgs84)\n",
        "gdf_pts.head(4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "514e0157",
      "metadata": {
        "id": "514e0157"
      },
      "outputs": [],
      "source": [
        "# ---------------------------------------------------------------------\n",
        "# 1.  Same CRS for everything  -----------------------------------------\n",
        "crs_wgs84 = \"EPSG:4326\"\n",
        "grid_polys  = grid_polys.to_crs(crs_wgs84)\n",
        "boundary_gdf = boundary_gdf.to_crs(crs_wgs84)\n",
        "# gdf_pts = gdf_pts.to_crs(crs_wgs84)\n",
        "\n",
        "# keep only the part of each cell that intersects the boundary\n",
        "grid_clipped = gpd.overlay(\n",
        "    grid_polys,                       # quarter-degree grid\n",
        "    boundary_gdf[[\"geometry\"]],       # national outline\n",
        "    how=\"intersection\"                # returns the intersecting polygons\n",
        ")\n",
        "\n",
        "# ---------------------------------------------------------------\n",
        "# 2.  Square-root–transform richness\n",
        "grid_clipped[\"spp_rich_sqrt\"] = np.sqrt(grid_clipped[\"spp_rich\"])\n",
        "\n",
        "# ---------------------------------------------------------------\n",
        "# 3.  Custom colour ramp (white-yellow-green-blue-red)\n",
        "# my_colors = [\"white\", \"yellow\", \"green\", \"blue\", \"red\", \"purple\", \"black\"]\n",
        "my_colors = [\"white\", \"yellow\", \"green\", \"blue\", \"red\", \"black\"]\n",
        "my_cmap   = LinearSegmentedColormap.from_list(\"wygbr\", my_colors, N=256)\n",
        "\n",
        "# ---------------------------------------------------------------------\n",
        "# 4.  Plot  -------------------------------------------------------------\n",
        "fig, ax = plt.subplots(figsize=(9, 7))\n",
        "\n",
        "# a) richness grid  (viridis _reversed_)\n",
        "grid_clipped.plot(\n",
        "    column=\"spp_rich_sqrt\", # \"spp_rich_sqrt\",\n",
        "    ax=ax,\n",
        "    cmap=my_cmap,           # <- the “_r” variant flips the ramp\n",
        "    linewidth=0,\n",
        "    legend=True,\n",
        "    legend_kwds={\"label\": \"Species richness\", \"shrink\": 0.7},\n",
        "    vmin=0,\n",
        "    vmax=grid_clipped.spp_rich_sqrt.max() # grid_clipped.spp_rich_sqrt.max()\n",
        ")\n",
        "\n",
        "# b) boundary outline (black, no fill)\n",
        "boundary_gdf.boundary.plot(\n",
        "    ax=ax, color=\"black\", linewidth=1.2, label=\"Boundary\"\n",
        ")\n",
        "\n",
        "# c) observation points (optional; light grey so grid colours dominate)\n",
        "# gdf_pts.plot(ax=ax, markersize=4, color=\"white\", edgecolor=\"grey\", linewidth=0.3, alpha=0.7)\n",
        "\n",
        "# ---------------------------------------------------------------------\n",
        "# 5.  Cosmetic tweaks  -------------------------------------------------\n",
        "ax.set_title(\"Square-rooted Species Richness (Quarter-Degree Cells)\",\n",
        "             fontsize=16, pad=12)\n",
        "ax.set_xlabel(\"\")      # no ticks\n",
        "ax.set_ylabel(\"\")\n",
        "ax.set_xticks([])\n",
        "ax.set_yticks([])\n",
        "ax.legend(loc=\"upper right\")\n",
        "\n",
        "# optional: add a scalebar and/or basemap\n",
        "# ScaleBar requires the axis to be in projected metres; quick hack:\n",
        "# scale_ax = ax.inset_axes([0.02, 0.05, 0.25, 0.01])\n",
        "# scalebar = ScaleBar(dx=1, units=\"m\", dimension=\"si-length\")\n",
        "# scale_ax.add_artist(scalebar); scale_ax.set_axis_off()\n",
        "#\n",
        "# cx.add_basemap(ax, crs=crs_wgs84, zoom=6, source=cx.providers.Stamen.TerrainBackground)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "76cdb0e4",
      "metadata": {
        "id": "76cdb0e4"
      },
      "outputs": [],
      "source": [
        "import rasterio\n",
        "from rasterio.features import rasterize\n",
        "from affine import Affine\n",
        "import numpy as np\n",
        "\n",
        "# ---------------------------------------------------------------\n",
        "# 2.  Square-root–transform richness\n",
        "grid_polys[\"spp_rich_sqrt\"] = np.sqrt(grid_polys[\"spp_rich\"])\n",
        "\n",
        "# 1. Define your output grid:\n",
        "xmin, ymin, xmax, ymax = grid_polys.total_bounds\n",
        "n_cols = 500  # or whatever resolution you need\n",
        "n_rows = int(n_cols * (ymax-ymin)/(xmax-xmin))\n",
        "transform = Affine((xmax-xmin)/n_cols, 0, xmin,\n",
        "                   0, -(ymax-ymin)/n_rows, ymax)\n",
        "\n",
        "# 2. Build (geometry, value) pairs\n",
        "shapes = (\n",
        "    (geom, value) for geom, value in\n",
        "    zip(grid_polys.geometry, grid_polys['spp_rich_sqrt'])\n",
        ")\n",
        "\n",
        "# 3. Rasterize to an array\n",
        "arr = rasterize(\n",
        "    shapes,\n",
        "    out_shape=(n_rows, n_cols),\n",
        "    transform=transform,\n",
        "    fill=0,\n",
        "    dtype='float32'\n",
        ")\n",
        "\n",
        "# 4. Create the directory if it doesn't exist ---\n",
        "output_dir = \"data\"\n",
        "if not os.path.exists(output_dir):\n",
        "    os.makedirs(output_dir)\n",
        "\n",
        "# 5. Save to disk\n",
        "with rasterio.open(\n",
        "    os.path.join(output_dir, \"spp_rich_sqrt.tif\"), \"w\", # Use os.path.join for path\n",
        "    driver=\"GTiff\",\n",
        "    height=n_rows, width=n_cols,\n",
        "    count=1, dtype=arr.dtype,\n",
        "    crs=\"EPSG:4326\",\n",
        "    transform=transform\n",
        ") as dst:\n",
        "    dst.write(arr, 1)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "18a64330",
      "metadata": {
        "id": "18a64330"
      },
      "outputs": [],
      "source": [
        "import rasterio\n",
        "import numpy as np\n",
        "import folium\n",
        "from folium.raster_layers import ImageOverlay\n",
        "import matplotlib.colors as mcolors\n",
        "\n",
        "# 1. Read your GeoTIFF ----------------------------------------------------------------\n",
        "with rasterio.open(\"data/spp_rich_sqrt.tif\") as src:\n",
        "    data = src.read(1)\n",
        "    bounds = src.bounds\n",
        "\n",
        "# 2. Build a Matplotlib ListedColormap & normalize ------------------------------------\n",
        "my_colors = [\"white\", \"yellow\", \"green\", \"blue\", \"red\", \"black\"]\n",
        "cmap = mcolors.ListedColormap(my_colors)\n",
        "norm = mcolors.Normalize(vmin=data.min(), vmax=data.max())\n",
        "\n",
        "# 3. Map data → RGBA array ------------------------------------------------------------\n",
        "#    This gives an (H, W, 4) array with values in [0,1]\n",
        "rgba_img = cmap(norm(data))\n",
        "\n",
        "# 4. Flip so north is up --------------------------------------------------------------\n",
        "#    NOTE: If your raster is already north-up you can skip this.\n",
        "# rgba_img = np.flipud(rgba_img)\n",
        "\n",
        "# 5. Create the map and overlay -------------------------------------------------------\n",
        "m = folium.Map(\n",
        "    location=[ (bounds.top + bounds.bottom)/2,\n",
        "               (bounds.left + bounds.right)/2 ],\n",
        "    zoom_start=6\n",
        ")\n",
        "\n",
        "ImageOverlay(\n",
        "    image=rgba_img,\n",
        "    bounds=[[bounds.bottom, bounds.left],\n",
        "            [bounds.top,    bounds.right]],\n",
        "    # origin='lower',              # treat first row as southern edge\n",
        "    opacity=0.7,\n",
        "    name=\"√ Species Richness\"\n",
        ").add_to(m)\n",
        "\n",
        "folium.LayerControl().add_to(m)\n",
        "m\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b14dd44b",
      "metadata": {
        "id": "b14dd44b"
      },
      "source": [
        "---\n",
        "#### 5.\tCREATE YOUR FIRST DATA CUBES\n",
        "> In this section, participants will construct biodiversity data cubes—structured as three-dimensional arrays that integrate space, time, and ecological variables. Each cube captures how a specific biodiversity metric (e.g. sampling effort or species richness) varies across geographic locations (latitude × longitude) over multiple time steps (e.g. years or months). This structure allows for efficient storage, querying, and analysis of large spatiotemporal datasets.\n",
        "> As illustrated in the image, the data cube is organised with spatial layers (rows and columns representing grid cells) stacked across a time axis (e.g. Year 1 to Year 5). Each “slice” of the cube represents a complete spatial layer for one time step, and the full cube enables users to trace changes through time at any given location or across the entire landscape.\n",
        "Visualisation plays a key role in understanding and communicating these dynamics. Once the cube is built, maps and animated plots can be used to display how biodiversity indicators change over time and space. This can reveal trends such as increasing sampling effort in certain areas, emerging biodiversity hotspots, or declining richness in response to environmental pressures.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a68437de",
      "metadata": {
        "id": "a68437de"
      },
      "source": [
        "##### 5.1.\tSpatio-Temporal Data Cube >> Sampling Effort\n",
        "Here, participants will build a data cube that records the number of occurrence records (i.e. sampling effort) for each grid cell in each time period. Each layer in the cube corresponds to a different year or time step, and each cell holds the count of records observed in that space-time unit. This cube helps identify spatial sampling biases and temporal coverage gaps.\n",
        "Visualization of this cube can include animated heatmaps or time-series plots that show changes in sampling intensity across the landscape and over time. These outputs are useful for diagnosing data quality and guiding future sampling campaigns.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cfbc1edb",
      "metadata": {
        "id": "cfbc1edb"
      },
      "outputs": [],
      "source": [
        "# Group by rounded_latitude, rounded_longitude, and year, and count unique species\n",
        "obs_cnt = grid_counts.groupby(['grid_id' ,'centroid_lat', 'centroid_lon', 'year'])['count'].sum().unstack()\n",
        "\n",
        "# Fill NaN values with 0\n",
        "obs_cnt = obs_cnt.fillna(0)\n",
        "\n",
        "# Display the resulting table\n",
        "obs_cnt.head(4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "aa6e146b",
      "metadata": {
        "id": "aa6e146b"
      },
      "outputs": [],
      "source": [
        "# ------------------------------------------------------------------\n",
        "# 1.  Join the tabular data to the polygons ------------------------\n",
        "#     (drops the centroid columns; they were only metadata here)\n",
        "years = list(range(1998, 2024))  # 1998–2023 inclusive\n",
        "gpObs = (grid_polys\n",
        "      .set_index('grid_id')\n",
        "      .join(obs_cnt))\n",
        "\n",
        "# ------------------------------------------------------------------\n",
        "# 2.  Derive grid geometry (origin, pixel size, array shape) -------\n",
        "minx, miny, maxx, maxy = gpObs.total_bounds\n",
        "dx = gpObs.geometry.iloc[0].bounds[2] - gpObs.geometry.iloc[0].bounds[0]   # cell width\n",
        "dy = gpObs.geometry.iloc[0].bounds[3] - gpObs.geometry.iloc[0].bounds[1]   # cell height\n",
        "transform = from_origin(minx, maxy, dx, dy)\n",
        "width  = int(np.round((maxx - minx) / dx))\n",
        "height = int(np.round((maxy - miny) / dy))\n",
        "\n",
        "# ------------------------------------------------------------------\n",
        "# 3.  Rasterise one 2-D array per year -----------------------------\n",
        "nodata = -9999.0     # pick any nodata flag that will never occur in your data\n",
        "arraysObs = {}\n",
        "for yr in years:\n",
        "    shapes = ((geom, value) for geom, value in zip(gpObs.geometry, gpObs[yr]))\n",
        "    arrObs = rasterize(\n",
        "        shapes=shapes,\n",
        "        out_shape=(height, width),\n",
        "        transform=transform,\n",
        "        fill=nodata,\n",
        "        dtype='float32')\n",
        "    arraysObs[yr] = arrObs\n",
        "\n",
        "# --- Preview the first 4 layers with boundary & transparency --------------\n",
        "preview_years = years[:4]                       # 1998-2001\n",
        "fig, axes = plt.subplots(2, 2, figsize=(10, 10))\n",
        "\n",
        "for ax, yr in zip(axes.flat, preview_years):\n",
        "    # (a) mask nodata so it draws with alpha = 0\n",
        "    masked = np.ma.masked_equal(arraysObs[yr], nodata)\n",
        "\n",
        "    # (b) plot the raster\n",
        "    show(masked,\n",
        "         transform=transform,\n",
        "         ax=ax,\n",
        "         cmap='viridis_r',\n",
        "         title=str(yr),\n",
        "         vmin=0,\n",
        "         vmax=max(masked.max(), 1))             # keep common colour scale\n",
        "\n",
        "    # (c) overlay the boundary (no fill, black outline)\n",
        "    boundary_gdf.boundary.plot(\n",
        "        ax=ax,\n",
        "        color='black',\n",
        "        linewidth=1.2,\n",
        "        label='Boundary'\n",
        "    )\n",
        "\n",
        "    ax.set_axis_off()                           # optional: remove tick marks\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ------------------------------------------------------------------\n",
        "# 3·bis  Build a geometry mask from boundary_gdf -------------------\n",
        "#        (run after Step 3, before Step 4)\n",
        "\n",
        "# 3.1  Ensure both layers share a CRS\n",
        "boundary_aligned = boundary_gdf.to_crs(crs_wgs84)\n",
        "\n",
        "# 3.2  True inside the boundary, False outside\n",
        "mask = geometry_mask(\n",
        "    boundary_aligned.geometry,\n",
        "    out_shape=(height, width),\n",
        "    transform=transform,\n",
        "    invert=True)          # invert=True ⇒ mask is True *inside*"
      ],
      "metadata": {
        "id": "FS5NClsJmHXR"
      },
      "id": "FS5NClsJmHXR",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fb1c7eb1",
      "metadata": {
        "id": "fb1c7eb1"
      },
      "outputs": [],
      "source": [
        "# ------------------------------------------------------------------\n",
        "# 4.  Write multi-band GeoTIFF, masking outside the boundary -------\n",
        "obs_tif = 'data/sampling_effort_1998_2023_masked.tif'\n",
        "with rasterio.open(\n",
        "        obs_tif,\n",
        "        'w',\n",
        "        driver='GTiff',\n",
        "        width=width,\n",
        "        height=height,\n",
        "        count=len(years),\n",
        "        crs=gpObs.crs,\n",
        "        transform=transform,\n",
        "        dtype='float32',\n",
        "        nodata=nodata,\n",
        "        compress='lzw') as dstObs:\n",
        "\n",
        "    for i, yr in enumerate(years, start=1):\n",
        "        # keep values >0 inside boundary; everything else → nodata\n",
        "        valid = (mask) & (arraysObs[yr] > 0)\n",
        "        clipped = np.where(valid, arraysObs[yr], nodata).astype('float32')\n",
        "\n",
        "        dstObs.write(clipped, i)\n",
        "        dstObs.set_band_description(i, str(yr))\n",
        "\n",
        "print(f\"✓ Wrote masked GeoTIFF {obs_tif} (values ≤0 set to nodata)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ac7da0b0",
      "metadata": {
        "id": "ac7da0b0"
      },
      "source": [
        "##### 5.2.\tSpatio-Temporal Data Cube >> Species Richness\n",
        "This cube stores species richness values (i.e. the number of unique species recorded) for each grid cell over multiple time steps. It enables users to explore biodiversity patterns across both spatial and temporal dimensions, facilitating the detection of shifts in species composition, local extinctions, or emerging biodiversity hotspots.\n",
        "Visual outputs from this cube may include interactive maps, animated sequences, or 3D plots showing how richness evolves over time. These tools support interpretation of complex spatiotemporal patterns and are valuable for ecological forecasting, conservation prioritisation, and communicating results to stakeholders."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9b957dee",
      "metadata": {
        "id": "9b957dee"
      },
      "outputs": [],
      "source": [
        "# Group by rounded_latitude, rounded_longitude, and year, and count unique species\n",
        "spp_rich = grid_counts.groupby(['grid_id' ,'centroid_lat', 'centroid_lon', 'year'])['species'].nunique().unstack()\n",
        "\n",
        "# Fill NaN values with 0\n",
        "spp_rich = spp_rich.fillna(0)\n",
        "\n",
        "# Display the resulting table\n",
        "# print(spp_rich)\n",
        "spp_rich.head(4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5f1baf37",
      "metadata": {
        "id": "5f1baf37"
      },
      "outputs": [],
      "source": [
        "# ------------------------------------------------------------------\n",
        "# 1.  Join the tabular data to the polygons ------------------------\n",
        "#     (drops the centroid columns; they were only metadata here)\n",
        "years = list(range(1998, 2024))  # 1998–2023 inclusive\n",
        "gp = (grid_polys\n",
        "      .set_index('grid_id')\n",
        "      .join(spp_rich))\n",
        "\n",
        "# ------------------------------------------------------------------\n",
        "# 2.  Derive grid geometry (origin, pixel size, array shape) -------\n",
        "minx, miny, maxx, maxy = gp.total_bounds\n",
        "dx = gp.geometry.iloc[0].bounds[2] - gp.geometry.iloc[0].bounds[0]   # cell width\n",
        "dy = gp.geometry.iloc[0].bounds[3] - gp.geometry.iloc[0].bounds[1]   # cell height\n",
        "transform = from_origin(minx, maxy, dx, dy)\n",
        "width  = int(np.round((maxx - minx) / dx))\n",
        "height = int(np.round((maxy - miny) / dy))\n",
        "\n",
        "# ------------------------------------------------------------------\n",
        "# 3.  Rasterise one 2-D array per year -----------------------------\n",
        "nodata = -9999.0     # pick any nodata flag that will never occur in your data\n",
        "arrays = {}\n",
        "for yr in years:\n",
        "    shapes = ((geom, value) for geom, value in zip(gp.geometry, gp[yr]))\n",
        "    arr = rasterize(\n",
        "        shapes=shapes,\n",
        "        out_shape=(height, width),\n",
        "        transform=transform,\n",
        "        fill=nodata,\n",
        "        dtype='float32')\n",
        "    arrays[yr] = arr\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "15fb2bda",
      "metadata": {
        "id": "15fb2bda"
      },
      "outputs": [],
      "source": [
        "# --- Preview the first 4 layers with boundary & transparency --------------\n",
        "preview_years = years[:4]                       # 1998-2001\n",
        "fig, axes = plt.subplots(2, 2, figsize=(10, 10))\n",
        "\n",
        "for ax, yr in zip(axes.flat, preview_years):\n",
        "    # (a) mask nodata so it draws with alpha = 0\n",
        "    masked = np.ma.masked_equal(arrays[yr], nodata)\n",
        "\n",
        "    # (b) plot the raster\n",
        "    show(masked,\n",
        "         transform=transform,\n",
        "         ax=ax,\n",
        "         cmap='viridis_r',\n",
        "         title=str(yr),\n",
        "         vmin=0,\n",
        "         vmax=max(masked.max(), 1))             # keep common colour scale\n",
        "\n",
        "    # (c) overlay the boundary (no fill, black outline)\n",
        "    boundary_gdf.boundary.plot(\n",
        "        ax=ax,\n",
        "        color='black',\n",
        "        linewidth=1.2,\n",
        "        label='Boundary'\n",
        "    )\n",
        "\n",
        "    ax.set_axis_off()                           # optional: remove tick marks\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "40a0840a",
      "metadata": {
        "id": "40a0840a"
      },
      "outputs": [],
      "source": [
        "# # ------------------------------------------------------------------\n",
        "# # 4.  Write multi-band GeoTIFF -------------------------------------\n",
        "# out_tif = 'data/species_richness_1998_2023.tif'\n",
        "# with rasterio.open(\n",
        "#         out_tif,\n",
        "#         'w',\n",
        "#         driver='GTiff',\n",
        "#         width=width,\n",
        "#         height=height,\n",
        "#         count=len(years),\n",
        "#         crs=gp.crs,\n",
        "#         transform=transform,\n",
        "#         dtype='float32',\n",
        "#         nodata=nodata,\n",
        "#         compress='lzw') as dst:\n",
        "#     for i, yr in enumerate(years, start=1):\n",
        "#         dst.write(arrays[yr], i)\n",
        "#         dst.set_band_description(i, str(yr))\n",
        "\n",
        "# print(f\"✓ Wrote {out_tif} with {len(years)} bands.\")\n",
        "\n",
        "# ------------------------------------------------------------------\n",
        "# 3·bis  Build a geometry mask from boundary_gdf -------------------\n",
        "#        (run after Step 3, before Step 4)\n",
        "\n",
        "# 3.1  Ensure both layers share a CRS\n",
        "boundary_aligned = boundary_gdf.to_crs(gp.crs)\n",
        "\n",
        "# 3.2  True inside the boundary, False outside\n",
        "mask = geometry_mask(\n",
        "    boundary_aligned.geometry,\n",
        "    out_shape=(height, width),\n",
        "    transform=transform,\n",
        "    invert=True)          # invert=True ⇒ mask is True *inside*\n",
        "\n",
        "# ------------------------------------------------------------------\n",
        "# 4.  Write multi-band GeoTIFF, masking outside the boundary -------\n",
        "out_tif = 'data/species_richness_1998_2023_masked.tif'\n",
        "with rasterio.open(\n",
        "        out_tif,\n",
        "        'w',\n",
        "        driver='GTiff',\n",
        "        width=width,\n",
        "        height=height,\n",
        "        count=len(years),\n",
        "        crs=gp.crs,\n",
        "        transform=transform,\n",
        "        dtype='float32',\n",
        "        nodata=nodata,\n",
        "        compress='lzw') as dst:\n",
        "\n",
        "    for i, yr in enumerate(years, start=1):\n",
        "        # keep values >0 inside boundary; everything else → nodata\n",
        "        valid = (mask) & (arrays[yr] > 0)\n",
        "        clipped = np.where(valid, arrays[yr], nodata).astype('float32')\n",
        "\n",
        "        dst.write(clipped, i)\n",
        "        dst.set_band_description(i, str(yr))\n",
        "\n",
        "print(f\"✓ Wrote masked GeoTIFF {out_tif} (values ≤0 set to nodata)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "87cb51c5",
      "metadata": {
        "id": "87cb51c5"
      },
      "source": [
        "---\n",
        "#### 6.\tGET ENVIRONMENTAL COVARIATES\n",
        "> Environmental covariates—such as temperature, precipitation, elevation, vegetation productivity, and human population density—are critical for understanding the patterns and processes underlying biodiversity. These variables influence species distributions, community composition, and ecological dynamics across spatial and temporal scales. Capturing them as Data Cubes enables the integration of environmental context into biodiversity analyses, providing consistent, multi-layered inputs for modelling and monitoring. Each variable takes a different structural form depending on their dimensions. Elevation is a simple 3D matrix with two spatial dimensions (latitude and longitude) and one variable (elevation), but no temporal component. In contrast, WorldClim’s bioclimatic variables form a multi-dimensional matrix or spatial data cube, consisting of spatial dimensions and multiple variables (e.g. BIO1 to BIO19), but still static in time. Meanwhile, variables like NDVI, rainfall, and surface water are true spatio-temporal data cubes—they include latitude, longitude, and time dimensions, with one variable per cube (e.g. NDVI index, precipitation in mm, or water presence/absence).\n",
        "> Each cube encodes changes in an environmental variable across geographic space and time, allowing researchers to assess ecological responses to climate variability, habitat change, and anthropogenic pressures. These cubes are essential for linking biodiversity observations to their environmental drivers.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ab18f5ad",
      "metadata": {
        "id": "ab18f5ad"
      },
      "source": [
        "##### 6.0.\tCountry Boundaries\n",
        "[LSIB 2017: Large Scale International Boundary Polygons, Simplified](https://developers.google.com/earth-engine/datasets/catalog/USDOS_LSIB_SIMPLE_2017) is a simplified version of the detailed LSIB (2013) derived from two other datasets: a LSIB line vector file and the World Vector Shorelines (WVS) from the National Geospatial-Intelligence Agency (NGA). Compared with the detailed LSIB, in this simplified dataset some disjointed regions of each country have been reduced to a single feature. Furthermore, it excludes medium and smaller islands. The resulting simplified boundary lines are rarely shifted by more than 100 meters from the detailed LSIB lines. Each of the 312 features is a part of the geometry of one of the 284 countries described in this dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "da51d6f4",
      "metadata": {
        "id": "da51d6f4"
      },
      "outputs": [],
      "source": [
        "# Define the region of interest (ROI) as South Africa\n",
        "# south_africa = ee.FeatureCollection('USDOS/LSIB_SIMPLE/2017') \\\n",
        "#                 .filter(ee.Filter.inList('country_na', ['South Africa','Lesotho','Swaziland']))\n",
        "# https://code.earthengine.google.com/?asset=projects/ee-nithecs/assets/rsa_boundary_shp\n",
        "south_africa = ee.FeatureCollection(\"projects/ee-nithecs/assets/rsa_boundary_shp\")\n",
        "aoi = south_africa.bounds()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2571634d",
      "metadata": {
        "id": "2571634d"
      },
      "source": [
        "##### 6.1.\tElevation\n",
        "[NASADEM: NASA 30m Digital Elevation Model](https://developers.google.com/earth-engine/datasets/catalog/NASA_NASADEM_HGT_001) is an enhanced 30-meter digital elevation model based on reprocessed Shuttle Radar Topography Mission (SRTM) data. Improvements include void-filling and error correction using auxiliary datasets such as ASTER GDEM, ICESat GLAS, and PRISM. The product represents elevation surfaces collected around 2000, offering accurate topographic information globally. Elevation data are essential for modeling terrain-driven processes such as hydrology, erosion, and species distributions. Derived products include slope, aspect, and hillshade, which are valuable inputs in environmental modeling, remote sensing, and biodiversity research in topographically complex landscapes.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "41c4705e",
      "metadata": {
        "id": "41c4705e"
      },
      "outputs": [],
      "source": [
        "# -- Elevation image: NASADEM\n",
        "elev = ee.Image('NASA/NASADEM_HGT/001').select('elevation').clip(south_africa)\n",
        "\n",
        "elev_vis = {'min': 0, 'max': 2000,\n",
        "            'palette': ['#081d58', '#253494', '#225ea8',\n",
        "                        '#1d91c0', '#41b6c4', '#7fcdbb',\n",
        "                        '#c7e9b4', '#edf8b1', '#ffffd9']}\n",
        "\n",
        "# -- Build interactive map\n",
        "# Display results using geemap\n",
        "m = geemap.Map()\n",
        "m.centerObject(south_africa, 5)\n",
        "\n",
        "# -- Basemap gallery (a few common ones are added as convenience methods)\n",
        "m.add_basemap(\"ROADMAP\")    # default\n",
        "# m.add_basemap(\"HYBRID\", False)\n",
        "# m.add_basemap(\"TERRAIN\", False)\n",
        "\n",
        "# -- Elevation layer\n",
        "m.addLayer(elev, elev_vis, \"Elevation (m)\")\n",
        "\n",
        "# -- Boundary outline (no fill)\n",
        "# m.add_gdf(boundary_gdf, layer_name=\"Boundary\",\n",
        "#           style={'fillOpacity': 0, 'color': 'black', 'weight': 2})\n",
        "m.addLayer(south_africa, {}, \"South Africa (Lesotho & eSwatini)\", False)\n",
        "m"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0d07fb66",
      "metadata": {
        "id": "0d07fb66"
      },
      "outputs": [],
      "source": [
        "# --- Export to a local GeoTIFF via geemap --------------------------\n",
        "out_tif = 'data/elevation_nasadem.tif'  # choose your path\n",
        "geemap.ee_export_image(          # ← note: no `image=` keyword here\n",
        "    elev,                        # 1st arg = ee.Image to export\n",
        "    filename     = out_tif,\n",
        "    region       = south_africa.geometry(),\n",
        "    scale        = 5000,           # NASADEM native resolution is 30m\n",
        "    crs          = \"EPSG:4326\",\n",
        "    file_per_band=False\n",
        ")\n",
        "\n",
        "print(\"✓ Elevation GeoTIFF saved →\", out_tif)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5472923d",
      "metadata": {
        "id": "5472923d"
      },
      "source": [
        "##### 6.2.\tHuman Population\n",
        "[LandScan Population Data Global 1km]( https://developers.google.com/earth-engine/datasets/catalog/projects_sat-io_open-datasets_ORNL_LANDSCAN_GLOBAL) provides a high-resolution global population dataset developed by Oak Ridge National Laboratory. It uses census data, satellite imagery, land use, and infrastructure to estimate population distribution at ~1 km resolution (30 arc-seconds). LandScan models ambient population, representing the average presence over 24 hours rather than residential counts. It is widely used in disaster risk reduction, epidemiology, and conservation planning, particularly in assessing human pressures on ecosystems. The spatial granularity supports integration with environmental and socio-economic models, making it useful for global sustainability and spatial decision-making studies.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1b3b8247",
      "metadata": {
        "id": "1b3b8247"
      },
      "outputs": [],
      "source": [
        "# -- LandScan Population Data Global 1km\n",
        "landscan_global = ee.ImageCollection('projects/sat-io/open-datasets/ORNL/LANDSCAN_GLOBAL')\n",
        "\n",
        "pop_vis = {'min': 0, 'max': 4200,\n",
        "            'palette': ['#CCCCCC', '#FFFFBE', '#FEFF73', '#FEFF2C',\n",
        "                         '#FFAA27', '#FF6625', '#FF0023', '#CC001A', '#730009']}\n",
        "\n",
        "# -- Add to interactive map\n",
        "m.addLayer(landscan_global.mean().clip(south_africa), pop_vis, \"Population\")\n",
        "m"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a40714ce",
      "metadata": {
        "id": "a40714ce"
      },
      "source": [
        "##### 6.3.\tRoads\n",
        "[Global Roads Inventory Project (GRIP) global roads database](https://gee-community-catalog.org/projects/grip/) was developed to provide a more recent and consistent global roads dataset for use in global environmental and biodiversity assessment models like GLOBIO. The GRIP dataset consists of global and regional vector datasets in ESRI filegeodatabase and shapefile format, and global raster datasets of road density at a 5 arcminutes resolution (~8km x 8km). The GRIP dataset is mainly aimed at providing a roads dataset that is easily usable for scientific global environmental and biodiversity modelling projects. The dataset is not suitable for navigation. GRIP4 is based on many different sources (including OpenStreetMap) and to the best of our ability we have verified their public availability, as a criteria in our research. The UNSDI-Transportation datamodel was applied for harmonization of the individual source datasets.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c6fccee6",
      "metadata": {
        "id": "c6fccee6"
      },
      "outputs": [],
      "source": [
        "# -- Global Roads Inventory Project global roads database\n",
        "grip4_africa = ee.FeatureCollection(\"projects/sat-io/open-datasets/GRIP4/Africa\")\n",
        "# print('Grip4 Africa size',grip4_africa.size())\n",
        "\n",
        "# Clip each feature in the FeatureCollection to the geometry\n",
        "grip4_rsa = grip4_africa.filterBounds(south_africa.bounds())\n",
        "\n",
        "# -- Add to interactive map\n",
        "m.addLayer(grip4_rsa,{},'RSA Roads')\n",
        "m"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "44a70787",
      "metadata": {
        "id": "44a70787"
      },
      "source": [
        "##### 6.4.\tBioclimatic Variables\n",
        "[WorldClim V1 Bioclim](https://developers.google.com/earth-engine/datasets/catalog/WORLDCLIM_V1_BIO) provides 19 biologically meaningful variables derived from monthly temperature and precipitation data, capturing seasonality, variability, and limiting climatic factors. These variables are frequently used in species distribution modeling, ecological niche modeling, and biodiversity assessments. Covering the period 1960–1991, this dataset offers a spatial resolution of approximately 927.67 meters. The dataset supports ecological and climate-related analyses across various scales and has become a standard reference for correlating climate with biological processes. The variables include annual means, seasonality indices, and extreme temperature/precipitation metrics relevant for understanding climatic constraints on species and ecosystems.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cb9ca32c",
      "metadata": {
        "id": "cb9ca32c"
      },
      "outputs": [],
      "source": [
        "# -- WorldClim BIO Variables V1\n",
        "bioclim = ee.Image('WORLDCLIM/V1/BIO').clip(south_africa)\n",
        "\n",
        "bio_vis = {'min': -23, 'max': 30,\n",
        "            'palette': ['blue', 'purple', 'cyan', 'green', 'yellow', 'red']}\n",
        "\n",
        "# -- Add to interactive map\n",
        "m.addLayer(bioclim.select('bio01').multiply(0.1), bio_vis, \"bio01: Annual Mean Temperature\")\n",
        "m"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "75428918",
      "metadata": {
        "id": "75428918"
      },
      "source": [
        "##### 6.5.\tVegetation Greeness\n",
        "[MOD13A3.061 Vegetation Indices Monthly L3 Global 1 km SIN Grid]( https://developers.google.com/earth-engine/datasets/catalog/MODIS_061_MOD13A3) provides monthly global vegetation indices at 1 km resolution. This product includes NDVI and EVI, derived from daily MODIS observations (MOD13A2) using a weighted temporal average. These indices reflect vegetation condition, biomass, and photosynthetic activity. NDVI is sensitive to chlorophyll content, while EVI minimizes atmospheric and canopy background effects. MOD13A3 is widely used in land cover monitoring, phenology, productivity modeling, and change detection. Applications range from local to global scales in climate-ecological studies, agricultural monitoring, and habitat suitability analyses. The dataset enables consistent long-term vegetation assessment across varied biomes.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "db6d7b32",
      "metadata": {
        "id": "db6d7b32"
      },
      "outputs": [],
      "source": [
        "# -- MOD13A3.061 Vegetation Indices Monthly L3 Global 1 km\n",
        "ndvi = ee.ImageCollection('MODIS/061/MOD13A3').filter(ee.Filter.date('2020-01-01', '2023-05-01')).select('NDVI')\n",
        "\n",
        "ndvi_vis = {'min': 0, 'max': 9000,\n",
        "            'palette': ['ffffff', 'ce7e45', 'df923d', 'f1b555', 'fcd163', '99b718', '74a901',\n",
        "    '66a000', '529400', '3e8601', '207401', '056201', '004c00', '023b01',\n",
        "    '012e01', '011d01', '011301']}\n",
        "\n",
        "# -- Add to interactive map\n",
        "m.addLayer(ndvi.mean().clip(south_africa), ndvi_vis, \"Mean NDVI\")\n",
        "m"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5aa92884",
      "metadata": {
        "id": "5aa92884"
      },
      "source": [
        "##### 6.7.\tRainfall\n",
        "[CHIRPS Pentad: Climate Hazards Center InfraRed Precipitation With Station Data v2.0](https://developers.google.com/earth-engine/datasets/catalog/UCSB-CHG_CHIRPS_PENTAD) offers a 30+ year quasi-global rainfall dataset combining satellite-derived precipitation estimates with station data. Available at 0.05° (~5 km) resolution, CHIRPS provides temporally consistent data suitable for monitoring rainfall patterns and detecting anomalies. It is especially useful in regions with sparse gauge coverage, offering reliable inputs for drought monitoring, agricultural planning, hydrological modeling, and climate trend analysis. The pentad format aggregates rainfall over 5-day intervals, balancing temporal resolution with data stability. CHIRPS has been widely adopted in food security and climate vulnerability applications, particularly across the tropics and subtropics.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "eab5a864",
      "metadata": {
        "id": "eab5a864"
      },
      "outputs": [],
      "source": [
        "# -- CHIRPS Pentad Rainfall\n",
        "rain = ee.ImageCollection('UCSB-CHG/CHIRPS/PENTAD').filter(ee.Filter.date('2020-01-01', '2023-05-01')).select('precipitation')\n",
        "\n",
        "rain_vis = {'min': 0, 'max': 22,\n",
        "            'palette': ['001137', '0aab1e', 'e7eb05', 'ff4a2d', 'e90000']}\n",
        "\n",
        "# -- Add to interactive map\n",
        "m.addLayer(rain.mean().clip(south_africa), rain_vis, \"Mean Rainfall\")\n",
        "m"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "acb4cf32",
      "metadata": {
        "id": "acb4cf32"
      },
      "source": [
        "##### 6.7.\tSurface Water\n",
        "[JRC Yearly Water Classification History, v1.4 ]( https://developers.google.com/earth-engine/datasets/catalog/JRC_GSW1_4_YearlyHistory) provides annual classifications of surface water presence from 1984 to 2021. Based on over 4.7 million Landsat scenes (5, 7, and 8), this dataset classifies each pixel by water occurrence, seasonality, and change dynamics. The product enables high-resolution analysis of long-term trends in surface water extent, supporting studies in hydrology, climate change, wetland loss, and freshwater conservation. It includes year-by-year records and two reference epochs (1984–1999, 2000–2021), facilitating change detection over time. Water pixels are identified using a rule-based classification system, providing reliable global coverage for water monitoring applications."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2db03664",
      "metadata": {
        "id": "2db03664"
      },
      "outputs": [],
      "source": [
        "# -- JRC Yearly Water Classification History\n",
        "water = ee.ImageCollection('JRC/GSW1_4/YearlyHistory')\n",
        "\n",
        "# Crop data to boundary\n",
        "def clipToCol(image):\n",
        "  return image.clip(south_africa)\n",
        "\n",
        "water_class = water.map(clipToCol)\n",
        "\n",
        "water_vis = {'bands': ['waterClass'],\n",
        "  'min': 0.0,\n",
        "  'max': 3.0,\n",
        "  'palette': ['cccccc', 'ffffff', '99d9ea', '0000ff']}\n",
        "\n",
        "# -- Add to interactive map\n",
        "m.addLayer(water_class, water_vis, \"Water Class\")\n",
        "m\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4d618fc9",
      "metadata": {
        "id": "4d618fc9"
      },
      "source": [
        "---\n",
        "#### 7.\tINTERACTIVE MAPPING\n",
        "> Interactive mapping is a key component of biodiversity analysis, enabling users to explore spatial patterns in species distributions, environmental variables, and ecological indicators. Python libraries such as geemap, folium, and leaflet provide user-friendly interfaces for building dynamic web-based maps.\n",
        ">\n",
        "> -   `geemap` integrates seamlessly with Google Earth Engine, allowing users to overlay large-scale remote sensing and environmental datasets alongside species occurrence data.\n",
        "> -   `folium`, built on `Leaflet.js`, supports lightweight, customizable maps with features like pop-ups, layer controls, and time sliders.\n",
        "> -   `leaflet` (accessed via `R` or `JavaScript`) offers flexible, high-performance map rendering ideal for dashboard applications and spatial storytelling.\n",
        ">\n",
        "> These tools make it easy to visualise biodiversity data cubes, track temporal changes, and communicate results interactively.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f776867c",
      "metadata": {
        "id": "f776867c"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "fdc3e7be",
      "metadata": {
        "id": "fdc3e7be"
      },
      "source": [
        "---\n",
        "#### 8.\tLINK OCCURRENCE RECORDS TO ENVIRONMENTAL VARIABLES\n",
        "> This section focuses on enriching species occurrence data by linking it to relevant environmental variables at corresponding spatial locations. By sampling raster-based environmental layers (e.g. rainfall, vegetation indices) at the centroids of grid cells or observation points, we create a unified dataset that combines biodiversity and environmental information. This spatial join enables the analysis of how species patterns vary with underlying environmental gradients.\n",
        "> The process involves converting aggregated occurrence records into a spatial format (e.g. a `FeatureCollection`), and then extracting values from remote sensing-derived rasters (such as CHIRPS rainfall or MODIS NDVI) at each point location. The result is a structured table where each row represents a spatial unit, annotated with both the number of species observations and associated environmental values. This dataset forms the basis for subsequent modelling and interpretation of ecological processes.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "315826ea",
      "metadata": {
        "id": "315826ea"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "b3bd2795",
      "metadata": {
        "id": "b3bd2795"
      },
      "source": [
        "---\n",
        "#### 9.\tMODELLING RELATIONSHIPS\n",
        "> With our biodiversity and environmental data cubes in place, we now turn to explaining how and why patterns emerge. Four case studies illustrate a spectrum of modelling strategies matched to specific ecological questions and data structures:\n",
        ">\n",
        "> -   **Climate–vegetation coupling (9.1)**: A Random-Forest regression in Google Earth Engine captures the highly non-linear, lagged response of NDVI to rainfall, producing per-pixel error estimates and scalable, cloud-native predictions.\n",
        "> -   **Accessibility bias (9.2)**: A Generalised Additive Model with a negative-binomial family flexibly models over-dispersed sampling-effort counts as a smooth function of distance-to-roads, revealing the shape and strength of accessibility effects.\n",
        "> -   **Drivers of species richness (9.3)**: Gradient-Boosted Decision Trees (XGBoost) handle complex, interacting predictors. Feature importance scores and SHAP values rank climatic, topographic, and anthropogenic drivers, providing transparent insight into richness hotspots.\n",
        "> -   **Observer bias correction (9.4)**: A hierarchical occupancy-detection model separates true species occurrence from imperfect detection by incorporating effort and accessibility covariates in the detection sub-model, yielding bias-adjusted distribution estimates.\n",
        ">\n",
        "> Across all case studies, participants will learn to export predictor stacks from Earth Engine, fit and validate models in Python, quantify uncertainty through cross-validation or Bayesian posterior summaries, and return spatially explicit predictions to Earth Engine for interactive visualisation. By the end of this module, you will be able to select appropriate modelling frameworks, interpret key ecological drivers, and communicate results through reproducible, data-cube-based workflows.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9b4f6962",
      "metadata": {
        "id": "9b4f6962"
      },
      "source": [
        "##### 9.1.\tPredicting NDVI from rainfall\n",
        "Random-Forest regression ` smileRandomForest` handles non-linear, lagged, and threshold responses between rainfall and vegetation greenness without parametric assumptions; runs natively in GEE, scales to multi-year image collections, and provides per-pixel error estimates for map-based validation.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "36101151",
      "metadata": {
        "id": "36101151"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "a47db98d",
      "metadata": {
        "id": "a47db98d"
      },
      "source": [
        "##### 9.2.\tEffect of distance-to-roads on sampling effort\n",
        "Generalised Additive Model (GAM) model with a negative-binomial family because our sampling effort is over-dispersed count data that often shows a smooth, non-monotonic response to accessibility. A GAM fitted in Python (`pyGAM` or `statsmodels.gam`) uses spline terms to flexibly model the distance-to-road curve while the negative-binomial distribution accounts for extra-Poisson variance. Distance rasters exported from GEE serve as predictors.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "437618fd",
      "metadata": {
        "id": "437618fd"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "51ca803d",
      "metadata": {
        "id": "51ca803d"
      },
      "source": [
        "##### 9.3.\tKey drivers of species diversity\n",
        "Gradient-Boosted Decision Trees (XGBoost) handles complex, non-additive effects of multiple environmental covariates on species richness; built-in feature importance and SHAP values support transparent driver ranking. Predictor stacks are sampled from GEE and modelled in Python (`xgboost`, `scikit-learn`).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "af84576d",
      "metadata": {
        "id": "af84576d"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "aeccf262",
      "metadata": {
        "id": "aeccf262"
      },
      "source": [
        "##### 9.4.\tCompensating for observer bias\n",
        "Single-season occupancy-detection model (hierarchical Bayesian, e.g. `PyMC` or `pystan`) because it’s able to separate true occurrence from detection probability by including effort, observer identity, or accessibility covariates in the detection sub-model. Bias surfaces created in GEE feed into the model, providing effort-corrected distribution estimates."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a6c6a570",
      "metadata": {
        "id": "a6c6a570"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "c2676ee9",
      "metadata": {
        "id": "c2676ee9"
      },
      "source": [
        "---\n",
        "#### 10.\tINTERPRET RESULTS AND MAP PREDICTIONS\n",
        "> The final step translates model outputs into ecological insight and actionable spatial products. Begin by examining goodness-of-fit diagnostics—cross-validated RMSE, deviance, or posterior predictive checks—to judge model credibility. Next, interrogate driver importance and response curves: feature importance scores, SHAP values, or partial-dependence plots reveal how each environmental covariate shapes biodiversity patterns.\n",
        "> Predicted surfaces and their uncertainties are then exported back to Google Earth Engine and rendered with geemap or folium. Interactive layers allow users to toggle between baseline predictions, scenario maps, and confidence intervals, facilitating rapid exploration of hotspots, refugia, or data-poor areas. Complex temporal outputs (e.g. yearly richness trends) can be animated to show change through time, while static high-resolution maps are generated for reports and policy briefs.\n",
        "> Finally, link these visualisations to conservation objectives—such as identifying priority sampling gaps, forecasting climate-driven shifts, or evaluating management interventions—ensuring that data-cube analyses inform real-world biodiversity decisions.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8bf4d346",
      "metadata": {
        "id": "8bf4d346"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "781433bb",
      "metadata": {
        "id": "781433bb"
      },
      "source": [
        "# XXXX - WORKING CODE - XXXX"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "99a29ac8",
      "metadata": {
        "id": "99a29ac8"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    },
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "bd853b11",
        "bb520c84",
        "KkjSft4_h9dc",
        "TdEOplXziMt1"
      ],
      "toc_visible": true,
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}