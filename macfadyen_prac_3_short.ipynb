{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "540fa351",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nithecs-biomath/mini-schools/blob/main/macfadyen_prac_3_v3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fa473b21",
      "metadata": {
        "id": "fa473b21"
      },
      "source": [
        "<style>\n",
        "/* tighten heading–paragraph gap */\n",
        "h1, h2, h3, h4, h5, h6 { margin-bottom: 0.25rem; }\n",
        "\n",
        "/* tighten gap above any paragraph that follows a heading */\n",
        "h1 + p, h2 + p, h3 + p, h4 + p, h5 + p, h6 + p { margin-top: 0.25rem; }\n",
        "\n",
        "/* justify body text */\n",
        "p { text-align: justify; }\n",
        "</style>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e7c75b46",
      "metadata": {
        "id": "e7c75b46"
      },
      "source": [
        "# Monitoring Biodiversity with Data Cubes: Techniques and Applications for Open Science\n",
        "\n",
        "**Sandra MacFadyen** (macfadyen@sun.ac.za)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2300ec4b",
      "metadata": {
        "id": "2300ec4b"
      },
      "source": [
        "### Ecological Complexity and Biodiversity Mini-School\n",
        "Hosted by the [National Institute for Theoretical and Computational Sciences](https://nithecs.ac.za/) (NITheCS) through the Complexity in Biological Systems (E5) research programme, this four-part Mini-school—under Work Package 1: Ecological Complexity and Biodiversity—equips participants with practical and conceptual skills to harness the power of environmental data cubes for biodiversity monitoring and ecological modelling.  \n",
        "\n",
        "The course is designed for postgraduate students, early-career researchers, conservation professionals, and environmental data scientists interested in open, scalable, and reproducible workflows. It introduces participants to the use of multi-dimensional spatiotemporal data structures—known as \"Data Cubes\"—to monitor ecological patterns and processes across space and time. By combining open-access species-occurrence records from the [Global Biodiversity Information Facility](https://www.gbif.org/) (GBIF), environmental, and trait data with cloud-based tools like [Google Earth Engine](https://earthengine.google.com/) through [Google Colab](https://colab.google/), participants will learn how to build and apply biodiversity data cubes for analytical and decision-support purposes.\n",
        "Rooted in the principles of Open Science, this Mini-school emphasises transparency, accessibility, and collaborative research. Each session aligns with one or more of the UN [Sustainable Development Goals](https://sdgs.un.org/goals) (SDGs), highlighting the societal relevance of biodiversity informatics.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2d81cf7f",
      "metadata": {
        "id": "2d81cf7f"
      },
      "source": [
        "### Work Package 1: Ecological Complexity and Biodiversity\n",
        "This Mini-school is presented as part of Work Package 1 (WP1): Ecological Complexity and Biodiversity. WP1 explores how biodiversity patterns and ecological processes emerge, interact, and change across spatial, temporal, and biological scales. It aims to develop quantitative frameworks and open tools to better understand and monitor biodiversity in a changing world.\n",
        "WP1 is structured around five core objectives:\n",
        "\n",
        "1.  **Biodiversity Entropy and Symmetry Across Scales**: Develop a unified theoretical framework to harmonise biodiversity metrics across scales, reducing bias and exploring eco-evolutionary processes through the lens of entropy and symmetry.\n",
        "2.\t**Structural Emergence in Complex Adaptive Networks**: Investigate the stability, resilience, and emergent properties of adaptive ecological networks, from microbiomes to food webs, in response to both internal dynamics and external environmental shifts.\n",
        "3.\t**Biodiversity Dynamics in Protected Landscapes**: Quantify, model, and map biodiversity—from genes to ecosystems—across protected areas to understand ecological functioning and inform conservation policy and planning.\n",
        "4.\t**Spatiotemporal Dynamics and Adaptive Interactions in Ecosystems**: Model species distributions, biome transitions, and ecological disequilibria under climate change, using biodiversity metrics, movement data, and evolutionary game theory to explore adaptive interactions.\n",
        "5.\t**Innovative Computational Tools for Biodiversity Monitoring**: Develop open, scalable software tools—including a Biodiversity Informatics Hub and smart sensor systems—to support long-term biodiversity monitoring in African landscapes. This also includes building a research network across African protected areas to co-design training and capacity-building tailored to regional conservation needs.\n",
        "\n",
        "This Mini-school contributes directly to objectives 3, 4, and 5.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3ab0a0bc",
      "metadata": {
        "id": "3ab0a0bc"
      },
      "source": [
        "### Biodiversity\n",
        "Biodiversity—the variety of life on Earth—is the foundation of ecosystem health and resilience. It encompasses the diversity of genes, species, and ecosystems, and includes the complex interactions among organisms and their environments. This variety supports essential ecosystem functions such as nutrient cycling, pollination, and climate regulation, making biodiversity critical not only for nature but also for human well-being.\n",
        "Biodiversity operates across multiple dimensions and scales:\n",
        "\n",
        "-   **Composition**, **structure**, and **function** are key facets of biodiversity. Composition includes the identity and variety of species and genes present. Structure refers to the spatial organisation of biodiversity—species abundance, distribution patterns, and habitat arrangement. Function encompasses ecological processes and interactions such as energy flow and food web dynamics.\n",
        "-   **Genetic diversity** refers to the variation of genes within species, enabling populations to adapt to changing conditions.\n",
        "-   **Species diversity** reflects the number and variety of species in a given ecosystem or region.\n",
        "-   **Ecosystem diversity** captures the range of different habitats—such as forests, wetlands, savannas, and coral reefs—that sustain life and ecosystem services.\n",
        "\n",
        "\n",
        "Global threats like habitat loss, climate change, invasive species, and land degradation are causing rapid biodiversity declines. Monitoring biodiversity change across appropriate spatial and temporal scales has therefore become an urgent global priority. However, to respond effectively, we need globally relevant, standardised and scalable solutions that can track biodiversity patterns over time, inform conservation policy, and support ecosystem management.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "83e1e754",
      "metadata": {
        "id": "83e1e754"
      },
      "source": [
        "### **Practical Session 3: Ecological Modelling with Data Cubes**\n",
        "> In this hands-on session, participants will explore analytical approaches for modelling species distributions and environmental associations using biodiversity data cubes. Focusing on SDG 13 (Climate Action), the session will guide participants in investigating climate-related biodiversity responses and potential adaptation strategies.\n",
        "By the end of the session, participants will be able to:\n",
        ">\n",
        "> 1.\tConstruct a biodiversity data cube using open-access species occurrence data from GBIF, processed via Google Colab and Earth Engine.\n",
        "> 2.\tVisualise ecological patterns interactively using spatial dashboards and maps.\n",
        "> 3.\tAnalyse species diversity in relation to climatic and environmental variables, such as rainfall, temperature, and elevation, using summary statistics and visualisations.\n",
        ">\n",
        "> This session bridges computational ecology with applied conservation science, demonstrating how data-driven tools can inform real-world biodiversity management.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "475163a0",
      "metadata": {
        "id": "475163a0"
      },
      "source": [
        "---\n",
        "#### 1.\tSETUP\n",
        "> This section prepares the working environment for the data cube analysis. It ensures that all necessary packages are installed and available, initializes access to external services (e.g. Google Colab and Earth Engine), and connects to storage locations where datasets are kept or generated.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "caa34377",
      "metadata": {
        "id": "caa34377"
      },
      "source": [
        "##### 1.2.\tInstall packages\n",
        "Begin by installing the required Python libraries used throughout the workflow. These include geospatial, data science, and biodiversity informatics packages essential for data processing, analysis, and visualization.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a02fc324",
      "metadata": {
        "id": "a02fc324"
      },
      "outputs": [],
      "source": [
        "# -- Install only missing packages in Colab (or any Python env)\n",
        "import importlib.util, subprocess, sys\n",
        "\n",
        "# Colab (and most local installs) already include folium, shapely, numpy, pandas, etc.\n",
        "_required_pkgs = [\n",
        "    \"geopandas\",        # vector data processing\n",
        "    \"earthengine-api\",  # Google Earth Engine Python client\n",
        "    \"geemap\",           # interactive mapping for GEE\n",
        "    \"rasterio\",         # raster I/O & processing\n",
        "]\n",
        "\n",
        "def _install(pkg):\n",
        "    print(f\"Installing {pkg}…\")\n",
        "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"--quiet\", pkg])\n",
        "\n",
        "for pkg in _required_pkgs:\n",
        "    if importlib.util.find_spec(pkg) is None:\n",
        "        _install(pkg)\n",
        "    else:\n",
        "        print(f\"{pkg} already installed, skipping.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "88b1e8c4",
      "metadata": {},
      "outputs": [],
      "source": [
        "# -- Quick install/update other libraries\n",
        "# !pip install --upgrade earthengine-api\n",
        "# !pip install --upgrade pandas\n",
        "# !pip install xgboost shap"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8a6e717b",
      "metadata": {
        "id": "8a6e717b"
      },
      "source": [
        "##### 1.3.\tLoad packages\n",
        "Once installed, the relevant packages are imported into the working session. This step makes their functions available for use in the script and ensures compatibility between modules.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "671b9979",
      "metadata": {
        "id": "671b9979"
      },
      "outputs": [],
      "source": [
        "# ──────────────────────────────────────────────────────────────────────────────\n",
        "# IMPORTS: core utilities & OS interaction\n",
        "import os                              # file & directory operations\n",
        "from io import BytesIO                 # core I/O operations and in-memory streams\n",
        "# import time                          # polite pauses between API calls (unused)\n",
        "import zipfile                         # handle ZIP archives\n",
        "import requests                        # HTTP requests for downloading\n",
        "import importlib.metadata              # inspect package metadata/version\n",
        "\n",
        "# ──────────────────────────────────────────────────────────────────────────────\n",
        "# DATA STRUCTURES & GEOSPATIAL\n",
        "import pandas as pd                    # tabular data handling\n",
        "import geopandas as gpd                # GeoDataFrame and vector data operations\n",
        "import numpy as np                     # numerical computing, arrays\n",
        "import csv                             # CSV reading/writing\n",
        "# import xarray as xr                 # labeled N-dimensional arrays (unused)\n",
        "import folium                          # interactive web maps\n",
        "from folium.raster_layers import ImageOverlay\n",
        "# import branca                       # interactive web maps (unused)\n",
        "import ee                              # Google Earth Engine Python API\n",
        "import geemap                          # mapping extension for GEE Python API\n",
        "import shapely                         # geometry objects and operations\n",
        "from shapely.geometry import box       # create rectangular geometries\n",
        "# from shapely.geometry import Polygon  # polygon geometries (unused)\n",
        "\n",
        "# ──────────────────────────────────────────────────────────────────────────────\n",
        "# REMOTE DATA DOWNLOAD\n",
        "# import gdown                        # download from Google Drive (unused)\n",
        "\n",
        "# ──────────────────────────────────────────────────────────────────────────────\n",
        "# RASTER & TILE HANDLING\n",
        "import rasterio                       # raster I/O and processing\n",
        "from rasterio.transform import from_origin   # build affine transforms\n",
        "from rasterio.features import rasterize      # rasterize vector data\n",
        "# from rasterio.features import geometry_mask # vector-to-mask (unused)\n",
        "from rasterio.plot import show        # quick plotting of rasters\n",
        "\n",
        "# ──────────────────────────────────────────────────────────────────────────────\n",
        "# MAPPING & VISUALIZATION\n",
        "import matplotlib.pyplot as plt       # plotting library\n",
        "import matplotlib.colors as mcolors   # color maps and normalization\n",
        "from matplotlib.colors import BoundaryNorm\n",
        "import matplotlib as mpl             # colormap reference\n",
        "# from matplotlib_scalebar.scalebar import ScaleBar  # scale bars (unused)\n",
        "from matplotlib.colors import (\n",
        "    LinearSegmentedColormap,          # custom colormap construction\n",
        "    Normalize,                        # normalize data to colormap\n",
        "    BoundaryNorm,                     # discrete colormap intervals\n",
        "    ListedColormap                    # colormap from list of colors\n",
        ")\n",
        "\n",
        "# Optional basemap support:\n",
        "# import contextily as cx             # add web‐basemap tiles under GeoPandas plots\n",
        "# import localtileserver              # serve local raster tiles\n",
        "\n",
        "# ──────────────────────────────────────────────────────────────────────────────\n",
        "# MACHINE LEARNING & STATISTICS\n",
        "import statsmodels.api as sm          # statistical models and inference\n",
        "# from statsmodels.nonparametric.smoothers_lowess import lowess  # LOWESS smoothing (unused)\n",
        "\n",
        "import xgboost as xgb                 # gradient‐boosted trees\n",
        "# import shap                          # SHAP values for interpretability (unused)\n",
        "\n",
        "from sklearn.model_selection import train_test_split  # data splitting\n",
        "from sklearn.metrics import r2_score, mean_squared_error  # regression metrics\n",
        "from sklearn.linear_model import LinearRegression    # simple linear model\n",
        "from sklearn.inspection import permutation_importance  # model-agnostic importance\n",
        "\n",
        "# ──────────────────────────────────────────────────────────────────────────────\n",
        "# SPECIALIZED APIs\n",
        "# from pygbif import occurrences        # GBIF biodiversity occurrence API (unused)\n",
        "\n",
        "# ──────────────────────────────────────────────────────────────────────────────\n",
        "# AFFINE GEOMETRY\n",
        "from affine import Affine               # affine transformations (raster-to-world)\n",
        "\n",
        "# ──────────────────────────────────────────────────────────────────────────────\n",
        "# NOTE: This import block brings together data I/O, geospatial processing,\n",
        "# statistical modeling, and machine‐learning libraries needed for full\n",
        "# end-to-end analysis: from downloading raw data, through raster/vector\n",
        "# manipulation, to predictive modeling and visualization."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d8a3d379",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Check package version\n",
        "# print(importlib.metadata.version(\"earthengine-api\"))   # ≥ 0.1.390 is fine"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bc9855a6",
      "metadata": {
        "id": "bc9855a6"
      },
      "source": [
        "##### 1.4.\tInitialise packages\n",
        "Some packages require additional setup or authentication steps—such as logging in to Google services or initializing Earth Engine—to enable access to cloud-based resources and tools. Use the [Google Cloud Console](https://console.cloud.google.com/) for this.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2c7ca9e1",
      "metadata": {
        "id": "2c7ca9e1"
      },
      "outputs": [],
      "source": [
        "# -- Initialise Google Earth Engine (requires one‑off authentication)\n",
        "# Trigger the authentication flow\n",
        "# ee.Authenticate()\n",
        "\n",
        "# -- Initialize the Project (see https://console.cloud.google.com/)\n",
        "# ee.Initialize(project='ee-biomath')\n",
        "ee.Initialize(project='ee-nithecs')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2eccb33d",
      "metadata": {
        "id": "2eccb33d"
      },
      "source": [
        "##### 1.5.\tMount drive when in [Google Colab](https://colab.research.google.com/) environment\n",
        "This step links a cloud storage account (e.g. Google Drive) to the session. It allows read/write access to local folders in the cloud, which are used to store input data, intermediate outputs, and final results.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fdc816d2",
      "metadata": {},
      "source": [
        "**CODE BLOCK BELOW ONLY WORKS IN COLAB ENVIRONMENT**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2fe0a992",
      "metadata": {
        "id": "2fe0a992"
      },
      "outputs": [],
      "source": [
        "# # Import ``google.colab' library\n",
        "# from google.colab import files\n",
        "\n",
        "# # Mount Google Drive\n",
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "400b0aab",
      "metadata": {
        "id": "400b0aab"
      },
      "source": [
        "---\n",
        "#### 2.\tLOAD OCCURRENCE DATA\n",
        "> Species occurrence data form the foundation of biodiversity data cubes because they provide direct, spatially explicit records of where and when organisms have been observed. These data underpin our ability to quantify species distributions, assess patterns of species richness, and monitor changes in biodiversity over time. When structured appropriately, occurrence records can be aggregated into grid-based data cubes that support scalable, repeatable analyses across large regions and multiple time steps.\n",
        "> This section introduces several ways to access or import species occurrence data from both local and remote sources. Local data may include curated datasets collected from field surveys, research networks, or institutional archives. Online sources—such as the Global Biodiversity Information Facility (GBIF) or data repositories hosted on GitHub—offer access to millions of open-access records contributed by museums, monitoring programmes, and citizen science initiatives. Whether working with structured CSV files, APIs, or live database queries, users will learn how to load, clean, and format occurrence data for integration into biodiversity data cubes.\n",
        "> By combining these occurrence records with environmental and spatial metadata, we can begin to explore the composition, structure, and function of biodiversity across different ecological contexts—key elements discussed earlier in understanding biodiversity patterns and processes.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bd853b11",
      "metadata": {
        "id": "bd853b11"
      },
      "source": [
        "##### 2.1.\tRead from local drive\n",
        "Use pre-downloaded datasets stored in a local or mounted drive. This approach is useful for working offline or when using curated data from previous projects.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "faa93baf",
      "metadata": {},
      "source": [
        "**CODE BLOCK BELOW ONLY WORKS IN COLAB ENVIRONMENT**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "00179f2e",
      "metadata": {
        "id": "00179f2e"
      },
      "outputs": [],
      "source": [
        "# # Pick a local CSV/XLSX for upload\n",
        "# uploaded = files.upload()\n",
        "# # Expecting: table.csv\n",
        "# local_fn = next(iter(uploaded))\n",
        "# df_local = pd.read_csv(local_fn)\n",
        "# print(df_local.head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "410a5410",
      "metadata": {},
      "outputs": [],
      "source": [
        "# ──────────────────────────────────────────────────────────────────────────────\n",
        "# Specify the path to your local data file; can be CSV or Excel\n",
        "local_fn = \"data/sample_data_SA.csv\"      # e.g., \"data/sample_data_SA.csv\" or \"table.xlsx\"\n",
        "\n",
        "# ──────────────────────────────────────────────────────────────────────────────\n",
        "# Determine file type by its extension and read accordingly\n",
        "if local_fn.lower().endswith(\".csv\"):\n",
        "    # For CSVs, we may have commas or tabs—let’s sniff the delimiter\n",
        "    with open(local_fn, newline=\"\") as f:\n",
        "        # Read the first line and let the Sniffer detect comma vs. tab\n",
        "        first_line = f.readline()\n",
        "        dialect    = csv.Sniffer().sniff(first_line, delimiters=\",\\t\")\n",
        "    \n",
        "    # Now read with pandas using the detected delimiter\n",
        "    df_local = pd.read_csv(local_fn, sep=dialect.delimiter)\n",
        "    \n",
        "elif local_fn.lower().endswith((\".xls\", \".xlsx\")):\n",
        "    # For Excel files, use pandas' built-in reader\n",
        "    df_local = pd.read_excel(local_fn)\n",
        "    \n",
        "else:\n",
        "    # If it's neither CSV nor Excel, raise an informative error\n",
        "    raise ValueError(f\"Unsupported file type: {local_fn!r}\")\n",
        "\n",
        "# ──────────────────────────────────────────────────────────────────────────────\n",
        "# Show the first few rows to confirm successful import\n",
        "print(df_local.head())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bb520c84",
      "metadata": {
        "id": "bb520c84"
      },
      "source": [
        "##### 2.2.\tImport from Github\n",
        "Access datasets that have been shared via a public [GitHub](https://github.com/) repository. This promotes reproducibility and ensures that everyone uses the same standard input files.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fdb2abc5",
      "metadata": {
        "id": "fdb2abc5"
      },
      "outputs": [],
      "source": [
        "# # ──────────────────────────────────────────────────────────────────────────────\n",
        "# # Define the URL of the raw CSV file hosted on GitHub\n",
        "# url = \"https://raw.githubusercontent.com/nithecs-biomath/mini-schools/main/data/sample_data_SA.csv\"\n",
        "\n",
        "# # ──────────────────────────────────────────────────────────────────────────────\n",
        "# # Option 1: Quick import assuming comma delimiter\n",
        "# # df_git = pd.read_csv(url)\n",
        "\n",
        "# # Option 2: Shorter alias for tab‐delimited data\n",
        "# # df_git = pd.read_table(url)\n",
        "\n",
        "# # Option 3: Explicitly specify tab delimiter for clarity and robustness\n",
        "# df_git = pd.read_csv(url, sep='\\t')\n",
        "\n",
        "# # ──────────────────────────────────────────────────────────────────────────────\n",
        "# # Print the total number of rows loaded (with thousands separator)\n",
        "# print(f\"Rows: {len(df_git):,}\")\n",
        "\n",
        "# # Display the first few rows to verify contents\n",
        "# df_git.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3742f35c",
      "metadata": {
        "id": "3742f35c"
      },
      "source": [
        "##### 2.3.\tAccess GBIF data\n",
        "Query the [Global Biodiversity Information Facility](https://www.gbif.org/) (GBIF) directly from within the session. This allows users to search, filter, and download up-to-date species occurrence records for a target taxon and region. For example [DOI: 15468/dl.jh6maj](https://www.gbif.org/occurrence/download/0006880-241024112534372) containing all *Lepidoptera* species recorded in South Africa from 1998 until 2023."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3iIE5vpyhQr-",
      "metadata": {
        "id": "3iIE5vpyhQr-"
      },
      "source": [
        "###### Function `load_from_url_zip()`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "31483e3d",
      "metadata": {
        "id": "31483e3d"
      },
      "outputs": [],
      "source": [
        "# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
        "# Function: load_from_url_zip\n",
        "# --------------------------------------------------------------------\n",
        "# Downloads a ZIP archive from a given URL, finds the first CSV inside,\n",
        "# and reads it as a tab-delimited pandas DataFrame. Skips any bad lines\n",
        "# and sets low_memory=False to suppress mixed‐type column warnings.\n",
        "#\n",
        "# Parameters:\n",
        "#   zip_url : str\n",
        "#     URL pointing to a .zip file containing at least one .csv\n",
        "#\n",
        "# Returns:\n",
        "#   pd.DataFrame\n",
        "#     The contents of the first .csv in the archive, parsed with sep='\\t'\n",
        "# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
        "def load_from_url_zip(zip_url):\n",
        "    \"\"\"\n",
        "    Downloads a ZIP from `zip_url`, finds the first .csv inside,\n",
        "    reads it as a tab-delimited DataFrame (skipping bad lines),\n",
        "    and returns the DataFrame with low_memory=False to avoid DtypeWarning.\n",
        "    \"\"\"\n",
        "    # 1. Fetch the ZIP file from the web\n",
        "    resp = requests.get(zip_url)\n",
        "    # 2. Raise an error if the download did not succeed (e.g., 404)\n",
        "    resp.raise_for_status()\n",
        "\n",
        "    # 3. Open the downloaded bytes as a ZIP archive\n",
        "    zf = zipfile.ZipFile(BytesIO(resp.content)) # type: ignore\n",
        "\n",
        "    # 4. Iterate through each file in the archive\n",
        "    for fname in zf.namelist():\n",
        "        # 5. Look for the first .csv (case-insensitive)\n",
        "        if fname.lower().endswith('.csv'):\n",
        "            # 6. Open the CSV file within the ZIP\n",
        "            with zf.open(fname) as f:\n",
        "                # 7. Read into pandas, using tab delimiter and skip bad lines\n",
        "                df = pd.read_csv(\n",
        "                    f,\n",
        "                    sep='\\t',             # tab-separated values\n",
        "                    on_bad_lines='skip',  # drop malformatted rows\n",
        "                    low_memory=False      # avoid mixed‐dtype warnings\n",
        "                )\n",
        "            # 8. Return the loaded DataFrame and exit the loop\n",
        "            return df\n",
        "\n",
        "    # 9. If no CSV was found, raise an error\n",
        "    raise FileNotFoundError(\"No CSV file found in archive\")\n",
        "# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "GwLRc7llhjQ4",
      "metadata": {
        "id": "GwLRc7llhjQ4"
      },
      "source": [
        "###### Run `load_from_url_zip()`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e6cca454",
      "metadata": {
        "id": "e6cca454"
      },
      "outputs": [],
      "source": [
        "# ──────────────────────────────────────────────────────────────────────────────\n",
        "# 1. Download & load GBIF occurrence data from a ZIP URL\n",
        "#    Using the pre-defined `load_from_url_zip()` function, which:\n",
        "#      • Downloads the ZIP\n",
        "#      • Extracts the first .csv inside\n",
        "#      • Reads it as a tab-delimited pandas DataFrame\n",
        "#      • Skips any bad lines and avoids dtype warnings\n",
        "# ------------------------------------------------------------------------------\n",
        "\n",
        "# Choose one of the GBIF download URLs:\n",
        "url_zip = 'https://api.gbif.org/v1/occurrence/download/request/0006880-241024112534372.zip'  \n",
        "# (81,825 Lepidoptera occurrences)\n",
        "# url_zip = 'https://api.gbif.org/v1/occurrence/download/request/0021941-250525065834625.zip'\n",
        "# (2,337 Ceratotherium simum occurrences)\n",
        "\n",
        "# Run the helper function to fetch and parse the ZIP\n",
        "df_url = load_from_url_zip(url_zip)\n",
        "\n",
        "# ──────────────────────────────────────────────────────────────────────────────\n",
        "# 2. Subset to the columns we actually need for downstream analysis\n",
        "#    Keep only temporal (year, month), taxonomic, and spatial fields\n",
        "# ------------------------------------------------------------------------------\n",
        "df_url = df_url[\n",
        "    [\n",
        "        'year', \n",
        "        'month', \n",
        "        'family', \n",
        "        'speciesKey', \n",
        "        'species', \n",
        "        'decimalLatitude', \n",
        "        'decimalLongitude'\n",
        "    ]\n",
        "]\n",
        "\n",
        "# 3. Inspect the result\n",
        "print(f\"Rows loaded: {len(df_url):,}\")\n",
        "df_url.head(4)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "33e730aa",
      "metadata": {
        "id": "33e730aa"
      },
      "source": [
        "---\n",
        "#### 3.\tSET AREA OF INTEREST AND GRID SIZE\n",
        "> This section defines the geographic scope of the analysis and sets the resolution for spatial aggregation. It forms the spatial structure for data cube construction.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1f1272b5",
      "metadata": {
        "id": "1f1272b5"
      },
      "source": [
        "##### 3.1.\tRead boundary shapefile\n",
        "Import a shapefile that outlines the study area. This serves as the spatial boundary within which all analyses will be performed.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "94eb3d95",
      "metadata": {
        "id": "94eb3d95"
      },
      "outputs": [],
      "source": [
        "# SETUP & INPUTS\n",
        "# ──────────────────────────────────────────────────────────────────────────────\n",
        "# Assume these objects are already prepared:\n",
        "# • grid_polys   : GeoDataFrame of quarter-degree grid cells \n",
        "#                  (must include a numeric 'spp_rich' column for species richness)\n",
        "# • boundary_gdf : GeoDataFrame of South African (and Lesotho/Eswatini) boundary \n",
        "#                  in EPSG:4326\n",
        "# • pts_gdf      : GeoDataFrame of occurrence points (optional; for scatter overlay)\n",
        "# ---------------------------------------------------------------------\n",
        "\n",
        "# If you need to upload boundary shapefiles (.shp, .shx, .dbf, .prj), you can use:\n",
        "# files.upload()\n",
        "\n",
        "# Optionally dissolve internal borders so only the outer outline is drawn:\n",
        "# boundary_gdf = boundary_gdf.dissolve().reset_index(drop=True)\n",
        "\n",
        "# ──────────────────────────────────────────────────────────────────────────────\n",
        "# 1. Read the boundary GeoJSON/Shapefile directly from GitHub into EPSG:4326\n",
        "# ---------------------------------------------------------------------\n",
        "boundary_gdf = (\n",
        "    gpd.read_file(\n",
        "        \"https://raw.githubusercontent.com/\"\n",
        "        \"nithecs-biomath/mini-schools/main/\"\n",
        "        \"data/south_africa_provinces_lesotho_eswatini.shp\"\n",
        "    )\n",
        "    .to_crs(\"EPSG:4326\")  # ensure lat/lon CRS for plotting\n",
        ")\n",
        "\n",
        "# ──────────────────────────────────────────────────────────────────────────────\n",
        "# 2. (Optional) If you instead have local files, uncomment and use:\n",
        "# ---------------------------------------------------------------------\n",
        "# boundary_gdf = (\n",
        "#     gpd.read_file(\"south_africa_provinces_lesotho_eswatini.shp\")\n",
        "#     .to_crs(\"EPSG:4326\")\n",
        "# )\n",
        "\n",
        "# Now `boundary_gdf` is ready to plot under your `grid_polys` layer.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "de1e79fa",
      "metadata": {
        "id": "de1e79fa"
      },
      "source": [
        "##### 3.2.\tGenerate grid\n",
        "Divide the study area into regularly spaced square or rectangular grid cells. These cells act as analytical units for summarising biodiversity metrics.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6ffdd697",
      "metadata": {},
      "source": [
        "Below is a single, self-contained function that *(a) builds a regular grid over your observation extent*, *(b) assigns each record to a cell*, and *(c) returns both the per-cell species–time counts and a GeoDataFrame of grid cells with richness/effort*:"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "TdEOplXziMt1",
      "metadata": {
        "id": "TdEOplXziMt1"
      },
      "source": [
        "###### Function `species_grid_summary()`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c1253bd8",
      "metadata": {},
      "outputs": [],
      "source": [
        "# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
        "# Function: species_grid_summary\n",
        "# --------------------------------------------------------------------\n",
        "# Builds a regular grid over occurrence data, assigns records to cells,\n",
        "# and returns both a tally of observations per cell and a grid GeoDataFrame\n",
        "#\n",
        "# Parameters:\n",
        "#   df_raw   : pandas.DataFrame with occurrence records and coordinate columns\n",
        "#   res       : float – grid resolution (in degrees if unit='deg', metres if 'm')\n",
        "#   unit      : str   – \"deg\" or \"m\" (determines CRS and reprojection)\n",
        "#   lat, lon  : str   – column names for latitude/longitude in df_raw\n",
        "#   year, month, species : str – column names for temporal & taxon grouping\n",
        "#   crs_deg   : str   – geographic CRS of input coords (default \"EPSG:4326\")\n",
        "#\n",
        "# Returns:\n",
        "#   grid_counts : DataFrame with columns\n",
        "#       [grid_id, year, month, species, count, centroid_lon, centroid_lat]\n",
        "#   grid_gdf    : GeoDataFrame of grid cells with\n",
        "#       [grid_id, spp_rich, obs_sum, geometry]\n",
        "# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
        "def species_grid_summary(\n",
        "    df_raw,\n",
        "    res=0.25,\n",
        "    unit=\"deg\",\n",
        "    lat=\"decimalLatitude\",\n",
        "    lon=\"decimalLongitude\",\n",
        "    year=\"year\",\n",
        "    month=\"month\",\n",
        "    species=\"species\",\n",
        "    crs_deg=\"EPSG:4326\"\n",
        "):\n",
        "    # 0. Drop records missing coordinates\n",
        "    df = df_raw[[year, month, species, lat, lon]].dropna(subset=[lat, lon]).copy()\n",
        "    \n",
        "    # 1. Create a GeoDataFrame of points in WGS84\n",
        "    gdf_pts = gpd.GeoDataFrame(\n",
        "        df,\n",
        "        geometry=gpd.points_from_xy(df[lon], df[lat]),\n",
        "        crs=crs_deg\n",
        "    )\n",
        "    \n",
        "    # 2. If grid in metres, reproject points to a metric CRS\n",
        "    if unit == \"m\":\n",
        "        gdf_pts = gdf_pts.to_crs(\"EPSG:3857\")\n",
        "    \n",
        "    # 3. Align the bounding box to the grid resolution\n",
        "    xmin, ymin, xmax, ymax = gdf_pts.total_bounds\n",
        "    xmin = np.floor(xmin / res) * res\n",
        "    ymin = np.floor(ymin / res) * res\n",
        "    xmax = np.ceil (xmax / res) * res\n",
        "    ymax = np.ceil (ymax / res) * res\n",
        "    \n",
        "    # 4. Build square grid cells and track cell-centroids (in grid CRS)\n",
        "    xs, ys = np.arange(xmin, xmax, res), np.arange(ymin, ymax, res)\n",
        "    polys, ids, cx, cy = [], [], [], []\n",
        "    for i, x0 in enumerate(xs):\n",
        "        for j, y0 in enumerate(ys):\n",
        "            polys.append(box(x0, y0, x0 + res, y0 + res))\n",
        "            gid = f\"g_{i}_{j}\"\n",
        "            ids.append(gid)\n",
        "            cx.append(x0 + res/2)\n",
        "            cy.append(y0 + res/2)\n",
        "    \n",
        "    grid_gdf = gpd.GeoDataFrame(\n",
        "        {\"grid_id\": ids},\n",
        "        geometry=polys,\n",
        "        crs=gdf_pts.crs\n",
        "    )\n",
        "    \n",
        "    # 5. Compute true centroids in WGS84 if metric grid, else use precalculated cx/cy\n",
        "    if unit == \"m\":\n",
        "        centroids = grid_gdf.geometry.centroid.to_crs(\"EPSG:4326\")\n",
        "        cent_lon, cent_lat = centroids.x, centroids.y\n",
        "    else:\n",
        "        cent_lon, cent_lat = cx, cy\n",
        "    \n",
        "    cent_df = pd.DataFrame({\n",
        "        \"grid_id\":      ids,\n",
        "        \"centroid_lon\": cent_lon,\n",
        "        \"centroid_lat\": cent_lat\n",
        "    })\n",
        "    \n",
        "    # 6. Spatially join points to grid cells\n",
        "    joined = gpd.sjoin(\n",
        "        gdf_pts,\n",
        "        grid_gdf[[\"grid_id\", \"geometry\"]],\n",
        "        how=\"inner\",\n",
        "        predicate=\"within\"\n",
        "    )\n",
        "    \n",
        "    # 7. Count observations per grid-year-month-species\n",
        "    grid_counts = (\n",
        "        joined\n",
        "        .groupby([\"grid_id\", year, month, species])\n",
        "        .size()\n",
        "        .reset_index(name=\"count\")\n",
        "        .merge(cent_df, on=\"grid_id\", how=\"left\")\n",
        "    )\n",
        "    \n",
        "    # 8. Compute species richness & total effort per cell\n",
        "    richness = grid_counts.groupby(\"grid_id\")[species].nunique().rename(\"spp_rich\")\n",
        "    effort   = grid_counts.groupby(\"grid_id\")[\"count\"].sum().rename(\"obs_sum\")\n",
        "    stats    = pd.concat([richness, effort], axis=1).reset_index()\n",
        "    \n",
        "    # 9. Merge these stats back into the grid GeoDataFrame (fill missing with zero)\n",
        "    grid_gdf = (\n",
        "        grid_gdf\n",
        "        .merge(stats, on=\"grid_id\", how=\"left\")\n",
        "        .fillna({\"spp_rich\": 0, \"obs_sum\": 0})\n",
        "    )\n",
        "    \n",
        "    return grid_counts, grid_gdf\n",
        "# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "m3fQiExJiR7z",
      "metadata": {
        "id": "m3fQiExJiR7z"
      },
      "source": [
        "###### Run `species_grid_summary()`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6e2ac983",
      "metadata": {
        "id": "6e2ac983"
      },
      "outputs": [],
      "source": [
        "# ──────────────────────────────────────────────────────────────────────────────\n",
        "# Example use of `species_grid_summary()` with a GBIF‐loaded DataFrame `df_url`\n",
        "# ---------------------------------------------------------------------\n",
        "# (If you had a local TSV, you might first load it like this:)\n",
        "# df_raw = pd.read_csv(\"occurrence_table.tsv\", sep=\"\\t\")\n",
        "\n",
        "# ──────────────────────────────────────────────────────────────────────────────\n",
        "# Call the function to build a 0.5° grid and summarise occurrences\n",
        "# ------------------------------------------------------------------------------\n",
        "grid_counts, grid_polys = species_grid_summary(\n",
        "    df_url,       # pandas.DataFrame with columns: year, month, species, decimalLatitude, decimalLongitude\n",
        "    res=0.5,      # grid resolution = 0.5 degrees (~56 km at the equator)\n",
        "    unit=\"deg\"    # grid in geographic degrees; use \"m\" to build in projected metres\n",
        ")\n",
        "\n",
        "# ──────────────────────────────────────────────────────────────────────────────\n",
        "# Inspect the per‐cell, per‐time, per‐species count table:\n",
        "# ------------------------------------------------------------------------------\n",
        "# columns: [grid_id, year, month, species, count, centroid_lon, centroid_lat]\n",
        "print(grid_counts.head())\n",
        "\n",
        "# Inspect the GeoDataFrame of grid‐cells:\n",
        "# columns: [grid_id, spp_rich, obs_sum, geometry]\n",
        "# where:\n",
        "#  • spp_rich = number of unique species in that cell\n",
        "#  • obs_sum  = total observation count in that cell\n",
        "print(grid_polys.head())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "757681c3",
      "metadata": {
        "id": "757681c3"
      },
      "source": [
        "\n",
        "##### 3.3.\tVisualise static map\n",
        "Display the grid overlay on top of the study region using a map viewer. This step ensures the grid is correctly aligned and helps users explore the spatial configuration."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bfe29fe7",
      "metadata": {},
      "outputs": [],
      "source": [
        "# ──────────────────────────────────────────────────────────────────────────────\n",
        "# Aggregate total counts at each grid‐cell centroid\n",
        "# --------------------------------------------------------------------\n",
        "# We group by the centroid coordinates (lat, lon) to sum all 'count' values\n",
        "# for each unique cell centroid, producing the total observations per cell.\n",
        "cent_df = (\n",
        "    grid_counts\n",
        "      .groupby(\n",
        "          ['centroid_lat', 'centroid_lon'],  # group by each cell’s centroid\n",
        "          as_index=False                     # keep these as columns, not index\n",
        "      )\n",
        "      .agg(\n",
        "          sum_cnt=('count', 'sum')           # sum of all counts in that cell\n",
        "      )\n",
        ")\n",
        "\n",
        "# ──────────────────────────────────────────────────────────────────────────────\n",
        "# Transform the summed counts to stabilize variance\n",
        "# --------------------------------------------------------------------\n",
        "# We take the square root of the total counts to reduce skew and heteroskedasticity.\n",
        "cent_df[\"sqrt_sum_cnt\"] = np.sqrt(cent_df[\"sum_cnt\"])\n",
        "\n",
        "# Display the first few rows: columns are centroid_lat, centroid_lon, sum_cnt, sqrt_sum_cnt\n",
        "print(cent_df.head(4))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "15e58b6b",
      "metadata": {
        "id": "15e58b6b"
      },
      "outputs": [],
      "source": [
        "# ──────────────────────────────────────────────────────────────────────────────\n",
        "# Custom colour ramp: yellow → green → blue → red → black\n",
        "# --------------------------------------------------------------------\n",
        "cmap = LinearSegmentedColormap.from_list(\n",
        "    \"YGBRB\",                        # name of the colormap\n",
        "    [\"yellow\", \"green\", \"blue\", \"red\", \"black\"],  # color sequence\n",
        "    N=256                           # number of discrete steps\n",
        ")\n",
        "# Normalize colours to the range of √count values\n",
        "norm = Normalize(\n",
        "    vmin=cent_df[\"sqrt_sum_cnt\"].min(),  # lower bound for colormap\n",
        "    vmax=cent_df[\"sqrt_sum_cnt\"].max()   # upper bound\n",
        ")\n",
        "\n",
        "# ──────────────────────────────────────────────────────────────────────────────\n",
        "# Scale point sizes so that area ∝ √(obs_count)\n",
        "# --------------------------------------------------------------------\n",
        "sizes = np.sqrt(cent_df[\"sqrt_sum_cnt\"])      # base sizes from √count\n",
        "sizes = sizes * 200 / sizes.max()             # scale so max size ~200\n",
        "\n",
        "# ──────────────────────────────────────────────────────────────────────────────\n",
        "# Create figure & axes before plotting any layers\n",
        "# --------------------------------------------------------------------\n",
        "fig, ax = plt.subplots(figsize=(8, 7))\n",
        "\n",
        "# 1. Plot grid cell outlines in light grey (zorder=1)\n",
        "grid_polys.to_crs(\"EPSG:4326\").boundary.plot(\n",
        "    ax=ax,\n",
        "    color=\"lightgrey\",\n",
        "    linewidth=0.4,\n",
        "    zorder=1\n",
        ")\n",
        "\n",
        "# 2. Plot national boundary in green (zorder=2)\n",
        "boundary_gdf.to_crs(\"EPSG:4326\").boundary.plot(\n",
        "    ax=ax,\n",
        "    color=\"green\",\n",
        "    linewidth=1.0,\n",
        "    zorder=2\n",
        ")\n",
        "\n",
        "# ──────────────────────────────────────────────────────────────────────────────\n",
        "# Scatter centroids sized by sampling effort, coloured by √(sum_cnt)\n",
        "# --------------------------------------------------------------------\n",
        "sc = ax.scatter(\n",
        "    cent_df[\"centroid_lon\"],            # x-coordinate (longitude)\n",
        "    cent_df[\"centroid_lat\"],            # y-coordinate (latitude)\n",
        "    s=sizes,                            # marker sizes\n",
        "    c=cent_df[\"sqrt_sum_cnt\"],          # marker colors\n",
        "    cmap=cmap,                          # custom colormap\n",
        "    norm=norm,                          # color normalization\n",
        "    edgecolor=\"k\",                      # black outline for each point\n",
        "    linewidth=0.3,                      # outline thickness\n",
        "    alpha=0.85,                         # marker transparency\n",
        "    zorder=3                            # draw on top\n",
        ")\n",
        "\n",
        "# ──────────────────────────────────────────────────────────────────────────────\n",
        "# Final touches: titles, labels, colorbar\n",
        "# --------------------------------------------------------------------\n",
        "ax.set_title(\"Sampling Effort per Grid Cell (√sum_cnt)\")\n",
        "ax.set_xlabel(\"Longitude\")\n",
        "ax.set_ylabel(\"Latitude\")\n",
        "ax.set_aspect(\"equal\")                # equal scaling on x/y axes\n",
        "\n",
        "# Add a colorbar showing the mapping of √count to color\n",
        "cbar = fig.colorbar(\n",
        "    sc,\n",
        "    ax=ax,\n",
        "    shrink=0.75,                      # scale colorbar length\n",
        "    label=\"√(Total observations)\"    # colorbar label\n",
        ")\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2c087c51",
      "metadata": {
        "id": "2c087c51"
      },
      "source": [
        "---\n",
        "#### 4.\tSUMMARISE OCCURRENCES BY GRID ID\n",
        "> Once the spatial structure is set, species records are aggregated to grid cells. This section describes the derivation of key biodiversity indicators at the grid-cell level.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f05771de",
      "metadata": {
        "id": "f05771de"
      },
      "source": [
        "##### 4.1.\tCalculate Sampling Effort (count total observations) and Species Richness (count unique species)\n",
        "-  Calculate the number of records per grid cell. This metric approximates sampling effort and is useful for identifying areas with data biases or coverage gaps.\n",
        "-  Count the number of unique species observed in each grid cell. Species richness is a basic but widely used indicator of biodiversity.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "917be5b6",
      "metadata": {
        "id": "917be5b6"
      },
      "outputs": [],
      "source": [
        "# ──────────────────────────────────────────────────────────────────────────────\n",
        "# Prompt:\n",
        "#   • Transpose the data.frame and sum species richness across years,\n",
        "#     i.e. find how many unique species were recorded each year.\n",
        "#   • Also compute the total number of observations per year.\n",
        "#   • Then plot both as a time series (line chart).\n",
        "# ──────────────────────────────────────────────────────────────────────────────\n",
        "\n",
        "# 1. Compute species richness per year:\n",
        "#    - group `grid_counts` by 'year'\n",
        "#    - count unique 'species' values each year\n",
        "species_richness_per_year = (\n",
        "    grid_counts\n",
        "      .groupby('year')['species']\n",
        "      .nunique()\n",
        ")\n",
        "species_richness_per_year.name = 'spp_rich'\n",
        "\n",
        "# 2. Compute total observations per year:\n",
        "#    - group by 'year' and sum all 'count' values\n",
        "total_observations_per_year = (\n",
        "    grid_counts\n",
        "      .groupby('year')['count']\n",
        "      .sum()\n",
        ")\n",
        "total_observations_per_year.name = 'obs_cnt'\n",
        "\n",
        "# 3. Combine into a single DataFrame for plotting\n",
        "annual_summary = pd.concat(\n",
        "    [species_richness_per_year, total_observations_per_year],\n",
        "    axis=1\n",
        ")\n",
        "print(annual_summary.head(4))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1538b8c6",
      "metadata": {
        "id": "1538b8c6"
      },
      "source": [
        "##### 4.2.\tResults - Line charts\n",
        "Plot time series or summary statistics (e.g. number of observations or richness) using line charts to explore trends across time or spatial resolution.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ad33af0f",
      "metadata": {
        "id": "ad33af0f"
      },
      "outputs": [],
      "source": [
        "# ──────────────────────────────────────────────────────────────────────────────\n",
        "# 1. Plot species richness and observation counts over time\n",
        "fig, ax1 = plt.subplots(figsize=(8, 5))\n",
        "\n",
        "years = annual_summary.index.astype(int)\n",
        "\n",
        "# 1a. Plot species richness on primary y-axis\n",
        "ax1.plot(\n",
        "    years,\n",
        "    annual_summary['spp_rich'],\n",
        "    color='green',\n",
        "    marker='o',\n",
        "    label='Species richness'\n",
        ")\n",
        "ax1.set_xlabel('Year')\n",
        "ax1.set_ylabel('Unique species', color='green')\n",
        "ax1.tick_params(axis='y', labelcolor='green')\n",
        "\n",
        "# 1b. Create a second y-axis sharing the same x-axis\n",
        "ax2 = ax1.twinx()\n",
        "ax2.plot(\n",
        "    years,\n",
        "    annual_summary['obs_cnt'],\n",
        "    color='blue',\n",
        "    marker='s',\n",
        "    label='Total observations'\n",
        ")\n",
        "ax2.set_ylabel('Total observations', color='blue')\n",
        "ax2.tick_params(axis='y', labelcolor='blue')\n",
        "\n",
        "# 2. Decorations\n",
        "plt.title('Annual Species Richness and Sampling Effort')\n",
        "fig.legend(loc='upper left', bbox_to_anchor=(0.1, 0.9))\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a4f49c0b",
      "metadata": {
        "id": "a4f49c0b"
      },
      "source": [
        "##### 4.3.\tResults – Static Map\n",
        "Visualise spatial patterns of sampling effort and species richness using map outputs. These maps can reveal biodiversity hotspots and under-sampled areas."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "514e0157",
      "metadata": {
        "id": "514e0157"
      },
      "outputs": [],
      "source": [
        "# ──────────────────────────────────────────────────────────────────────────────\n",
        "# Ensure all spatial layers share the same geographic CRS (WGS84)\n",
        "# ──────────────────────────────────────────────────────────────────────────────\n",
        "crs_wgs84   = \"EPSG:4326\"\n",
        "grid_polys  = grid_polys.to_crs(crs_wgs84)\n",
        "boundary_gdf = boundary_gdf.to_crs(crs_wgs84)\n",
        "\n",
        "# ──────────────────────────────────────────────────────────────────────────────\n",
        "# Filter grid cells to those that touch the study boundary (no clipping)\n",
        "# • Build a single (unioned) boundary geometry for fast intersection tests\n",
        "# ──────────────────────────────────────────────────────────────────────────────\n",
        "boundary_union = boundary_gdf.unary_union\n",
        "grid_filtered  = grid_polys[grid_polys.geometry.intersects(boundary_union)].copy()\n",
        "\n",
        "# ──────────────────────────────────────────────────────────────────────────────\n",
        "# Square-root transform the species‐richness column\n",
        "# • Stabilizes variance and reduces skew for visualization\n",
        "# ──────────────────────────────────────────────────────────────────────────────\n",
        "grid_filtered[\"spp_rich_sqrt\"] = np.sqrt(grid_filtered[\"spp_rich\"])\n",
        "\n",
        "# ──────────────────────────────────────────────────────────────────────────────\n",
        "# Define a custom colour ramp: white → yellow → green → blue → red → black\n",
        "# • Use 256 discrete steps for smooth gradients\n",
        "# ──────────────────────────────────────────────────────────────────────────────\n",
        "my_colors = [\"white\", \"yellow\", \"green\", \"blue\", \"red\", \"black\"]\n",
        "my_cmap   = LinearSegmentedColormap.from_list(\"wygbr\", my_colors, N=256)\n",
        "\n",
        "# ──────────────────────────────────────────────────────────────────────────────\n",
        "# Plotting\n",
        "# ──────────────────────────────────────────────────────────────────────────────\n",
        "fig, ax = plt.subplots(figsize=(9, 7))\n",
        "\n",
        "# 1. Plot the grid cells, coloured by √(species richness)\n",
        "grid_filtered.plot(\n",
        "    column=\"spp_rich_sqrt\",            # which attribute to colour by\n",
        "    ax=ax,\n",
        "    cmap=my_cmap,                      # custom colormap\n",
        "    linewidth=0,                       # no border lines between cells\n",
        "    legend=True,                       # show colourbar\n",
        "    legend_kwds={                      # colourbar settings\n",
        "        \"label\": \"√ Species Richness\",\n",
        "        \"shrink\": 0.7\n",
        "    },\n",
        "    vmin=0,                            # minimum colour scale\n",
        "    vmax=grid_filtered[\"spp_rich_sqrt\"].max()  # maximum colour scale\n",
        ")\n",
        "\n",
        "# 2. Overlay the national boundary outline\n",
        "boundary_gdf.boundary.plot(\n",
        "    ax=ax,\n",
        "    color=\"black\",\n",
        "    linewidth=1.2,\n",
        "    label=\"National Boundary\"\n",
        ")\n",
        "\n",
        "# ──────────────────────────────────────────────────────────────────────────────\n",
        "# Cosmetic tweaks\n",
        "# ──────────────────────────────────────────────────────────────────────────────\n",
        "ax.set_title(\n",
        "    \"Square-Rooted Species Richness\\n(Quarter-Degree Grid)\",\n",
        "    fontsize=16,\n",
        "    pad=12\n",
        ")\n",
        "ax.set_xticks([])  # remove x-axis tick marks\n",
        "ax.set_yticks([])  # remove y-axis tick marks\n",
        "ax.legend(loc=\"upper right\")\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2ab771a7",
      "metadata": {},
      "source": [
        "##### 4.4.\tResults - Interactive Folium Map\n",
        "This routine begins by applying a square-root transformation to the species richness attribute on each grid polygon, then computes the spatial extent and aspect ratio of the study area to define the dimensions and affine transform of a new raster at a user-specified resolution. It pairs each polygon geometry with its transformed richness value and rasterizes these pairs into a 2D NumPy array. After ensuring the output directory exists, the script opens a new GeoTIFF with the correct CRS, dimensions, data type, and transform, writes the array as a single band, and closes the file."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "76cdb0e4",
      "metadata": {
        "id": "76cdb0e4"
      },
      "outputs": [],
      "source": [
        "# ---------------------------------------------------------------------\n",
        "# Rasterize square-rooted species richness from a GeoDataFrame\n",
        "# ---------------------------------------------------------------------\n",
        "\n",
        "# 1. Extract the bounding box of your filtered grid cells\n",
        "xmin, ymin, xmax, ymax = grid_filtered.total_bounds\n",
        "\n",
        "# 2. Infer the grid resolution from the first cell’s bounds\n",
        "minx, miny, maxx, maxy = grid_filtered.geometry.bounds.iloc[0]\n",
        "res_x = maxx - minx    # pixel width\n",
        "res_y = maxy - miny    # pixel height\n",
        "\n",
        "# 3. Compute the number of columns and rows for the raster\n",
        "n_cols = int((xmax - xmin) / res_x)\n",
        "n_rows = int((ymax - ymin) / res_y)\n",
        "\n",
        "# 4. Build the affine transform\n",
        "#    Affine(scale_x, 0,      translate_x,\n",
        "#           0,      -scale_y, translate_y)\n",
        "transform = Affine(\n",
        "    res_x,  0,    xmin,    # scale in x, no shear, x origin\n",
        "    0,     -res_y, ymax    # no shear, negative scale in y, y origin\n",
        ")\n",
        "\n",
        "# 5. Prepare a generator of (geometry, value) pairs\n",
        "#    feeding each cell polygon and its √species richness\n",
        "shapes = (\n",
        "    (geom, val)\n",
        "    for geom, val in zip(\n",
        "        grid_filtered.geometry,\n",
        "        grid_filtered[\"spp_rich_sqrt\"]\n",
        "    )\n",
        ")\n",
        "\n",
        "# 6. Rasterize: burn values into a NumPy array\n",
        "arr = rasterize(\n",
        "    shapes,\n",
        "    out_shape=(n_rows, n_cols),\n",
        "    transform=transform,\n",
        "    fill=0,            # background value for areas without cells\n",
        "    dtype=\"float32\"    # data type of output array\n",
        ")\n",
        "\n",
        "# 7. Ensure the output directory exists\n",
        "output_dir = \"data\"\n",
        "os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "# 8. Write the array to a single-band GeoTIFF\n",
        "outfile = os.path.join(output_dir, \"spp_rich_sqrt.tif\")\n",
        "with rasterio.open(\n",
        "    outfile, \"w\",\n",
        "    driver=\"GTiff\",\n",
        "    height=n_rows,\n",
        "    width=n_cols,\n",
        "    count=1,               # number of bands\n",
        "    dtype=arr.dtype,\n",
        "    crs=\"EPSG:4326\",       # geographic CRS\n",
        "    transform=transform\n",
        ") as dst:\n",
        "    dst.write(arr, 1)     # write array to band 1\n",
        "\n",
        "print(f\"✓ Wrote GeoTIFF: {outfile}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a00e1149",
      "metadata": {},
      "source": [
        "This code block reads a GeoTIFF containing square‐root‐transformed species richness values, extracts the raster array and its geographic bounds, and builds a custom six‐step colormap normalised to the data range. The normalised values are mapped to an RGBA image array for display, and—if necessary—flipped to ensure north‐up orientation. A Folium map is then initialised at the raster’s centroid, the colored overlay is added with adjustable opacity, and a layer control widget is enabled to toggle the \"√ Species Richness\" layer on and off."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "18a64330",
      "metadata": {
        "id": "18a64330"
      },
      "outputs": [],
      "source": [
        "# ──────────────────────────────────────────────────────────────────────────────\n",
        "# 1. Read your GeoTIFF of square‐rooted species richness\n",
        "# --------------------------------------------------------------------\n",
        "with rasterio.open(\"data/spp_rich_sqrt.tif\") as src:\n",
        "    # Read only the first band (our single-layer raster)\n",
        "    data = src.read(1)\n",
        "    # Grab the geographic bounds (left, bottom, right, top)\n",
        "    bounds = src.bounds\n",
        "\n",
        "# ──────────────────────────────────────────────────────────────────────────────\n",
        "# 2. Build a discrete Matplotlib colormap & normalization\n",
        "# --------------------------------------------------------------------\n",
        "# Define the color sequence from low to high values\n",
        "my_colors = [\"white\", \"yellow\", \"green\", \"blue\", \"red\", \"black\"]\n",
        "# Create a ListedColormap from that list\n",
        "cmap = mcolors.ListedColormap(my_colors)\n",
        "# Normalize data values to the [0,1] interval for the colormap\n",
        "norm = mcolors.Normalize(vmin=data.min(), vmax=data.max())\n",
        "\n",
        "# ──────────────────────────────────────────────────────────────────────────────\n",
        "# 3. Map the data array to RGBA values\n",
        "#    This produces an (height, width, 4) array with RGBA in [0,1]\n",
        "# --------------------------------------------------------------------\n",
        "rgba_img = cmap(norm(data))\n",
        "\n",
        "# ──────────────────────────────────────────────────────────────────────────────\n",
        "# 4. (Optional) Flip the image vertically if needed\n",
        "#    If your raster’s first row is the top edge, you can skip this.\n",
        "# --------------------------------------------------------------------\n",
        "# rgba_img = np.flipud(rgba_img)\n",
        "\n",
        "# ──────────────────────────────────────────────────────────────────────────────\n",
        "# 5. Create a Folium map and overlay the RGBA image\n",
        "# --------------------------------------------------------------------\n",
        "# Center map on the raster’s midpoint\n",
        "center_lat = (bounds.top + bounds.bottom) / 2\n",
        "center_lon = (bounds.left + bounds.right) / 2\n",
        "\n",
        "m = folium.Map(location=[center_lat, center_lon], zoom_start=6)\n",
        "\n",
        "# Overlay the RGBA image with the proper geographic bounds\n",
        "ImageOverlay( # type: ignore\n",
        "    image=rgba_img,\n",
        "    bounds=[[bounds.bottom, bounds.left],\n",
        "            [bounds.top,    bounds.right]],\n",
        "    opacity=0.7,\n",
        "    name=\"√ Species Richness\"\n",
        ").add_to(m)\n",
        "\n",
        "# ──────────────────────────────────────────────────────────────────────────────\n",
        "# 6. Add the national boundary outline as a GeoJSON layer\n",
        "# --------------------------------------------------------------------\n",
        "folium.GeoJson(\n",
        "    boundary_gdf,  # your boundary GeoDataFrame\n",
        "    name=\"Boundary\",\n",
        "    style_function=lambda feature: {\n",
        "        \"fill\": False,\n",
        "        \"color\": \"black\",\n",
        "        \"weight\": 2,\n",
        "        \"opacity\": 1\n",
        "    }\n",
        ").add_to(m)\n",
        "\n",
        "# ──────────────────────────────────────────────────────────────────────────────\n",
        "# 7. Add layer control and display the map\n",
        "# --------------------------------------------------------------------\n",
        "folium.LayerControl().add_to(m)\n",
        "m  # In a notebook or folium‐enabled VS Code this will render the interactive map\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b14dd44b",
      "metadata": {
        "id": "b14dd44b"
      },
      "source": [
        "---\n",
        "#### 5.\tCREATE YOUR FIRST DATA CUBES\n",
        "> In this section, participants will construct biodiversity data cubes—structured as three-dimensional arrays that integrate space, time, and ecological variables. Each cube captures how a specific biodiversity metric (e.g. sampling effort or species richness) varies across geographic locations (latitude × longitude) over multiple time steps (e.g. years or months). This structure allows for efficient storage, querying, and analysis of large spatiotemporal datasets.\n",
        "> As illustrated in the image, the data cube is organised with spatial layers (rows and columns representing grid cells) stacked across a time axis (e.g. Year 1 to Year 5). Each “slice” of the cube represents a complete spatial layer for one time step, and the full cube enables users to trace changes through time at any given location or across the entire landscape.\n",
        "Visualisation plays a key role in understanding and communicating these dynamics. Once the cube is built, maps and animated plots can be used to display how biodiversity indicators change over time and space. This can reveal trends such as increasing sampling effort in certain areas, emerging biodiversity hotspots, or declining richness in response to environmental pressures.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a68437de",
      "metadata": {
        "id": "a68437de"
      },
      "source": [
        "##### 5.1.\tSpatio-Temporal Data Cube >> Sampling Effort\n",
        "Here, participants will build a data cube that records the number of occurrence records (i.e. sampling effort) for each grid cell in each time period. Each layer in the cube corresponds to a different year or time step, and each cell holds the count of records observed in that space-time unit. This cube helps identify spatial sampling biases and temporal coverage gaps.\n",
        "Visualization of this cube can include animated heatmaps or time-series plots that show changes in sampling intensity across the landscape and over time. These outputs are useful for diagnosing data quality and guiding future sampling campaigns.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cfbc1edb",
      "metadata": {
        "id": "cfbc1edb"
      },
      "outputs": [],
      "source": [
        "# ──────────────────────────────────────────────────────────────────────────────\n",
        "# 1. Aggregate total observations per grid cell per year\n",
        "#    We group by grid_id, centroid coordinates, and year, then sum the 'count' values\n",
        "# --------------------------------------------------------------------\n",
        "obs_cnt = (\n",
        "    grid_counts\n",
        "      .groupby(['grid_id', 'centroid_lat', 'centroid_lon', 'year'])['count']\n",
        "      .sum()\n",
        "      .unstack(level='year')    # pivot years into separate columns\n",
        ")\n",
        "\n",
        "# 2. Replace any missing year–cell combinations with zero observations\n",
        "# --------------------------------------------------------------------\n",
        "obs_cnt = obs_cnt.fillna(0)\n",
        "\n",
        "# 3. Inspect the table structure and summary statistics\n",
        "# --------------------------------------------------------------------\n",
        "# Print the first few rows to see the layout\n",
        "# print(obs_cnt.head(4))\n",
        "\n",
        "# Get quick descriptive stats (count, mean, std, min, max, etc.) for each year\n",
        "# print(obs_cnt.describe())\n",
        "\n",
        "# (Optional) Compute specific summary stats across all years\n",
        "# stats = obs_cnt.agg(['min','mean','max'])\n",
        "# print(stats)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "aa6e146b",
      "metadata": {
        "id": "aa6e146b"
      },
      "outputs": [],
      "source": [
        "# ──────────────────────────────────────────────────────────────────────────────\n",
        "# 1. Join per‐year observation counts to the grid polygons\n",
        "#    • `obs_cnt` has grid_id as index and one column per year\n",
        "#    • Joining drops the temporary centroid columns used earlier\n",
        "# ------------------------------------------------------------------------------\n",
        "years = list(range(1998, 2024))  # years from 1998 through 2023 inclusive\n",
        "gpObs = (\n",
        "    grid_polys\n",
        "      .set_index('grid_id')       # use grid_id to align with obs_cnt\n",
        "      .join(obs_cnt)              # attach one column per year of counts\n",
        ")\n",
        "\n",
        "# ──────────────────────────────────────────────────────────────────────────────\n",
        "# 2. Derive raster geometry parameters from the polygon grid\n",
        "#    • Bounds of the full grid → origin\n",
        "#    • Cell width/height from any single polygon’s bounds\n",
        "#    • Compute number of columns & rows to cover the extent\n",
        "# ------------------------------------------------------------------------------\n",
        "minx, miny, maxx, maxy = gpObs.total_bounds\n",
        "\n",
        "# Infer cell size from the first polygon\n",
        "dx = gpObs.geometry.iloc[0].bounds[2] - gpObs.geometry.iloc[0].bounds[0] # type: ignore  # cell width\n",
        "dy = gpObs.geometry.iloc[0].bounds[3] - gpObs.geometry.iloc[0].bounds[1] # type: ignore  # cell height\n",
        "\n",
        "# Build an Affine transform for the raster (origin at top-left)\n",
        "transform = from_origin(minx, maxy, dx, dy)\n",
        "\n",
        "# Calculate raster dimensions\n",
        "width  = int(np.round((maxx - minx) / dx))\n",
        "height = int(np.round((maxy - miny) / dy))\n",
        "\n",
        "# ──────────────────────────────────────────────────────────────────────────────\n",
        "# 3. Rasterize observation counts for each year into a NumPy array\n",
        "#    • Use a consistent NoData value that does not overlap real data\n",
        "#    • Store each year’s 2D array in a dictionary for downstream use\n",
        "# ------------------------------------------------------------------------------\n",
        "nodata = -9999.0\n",
        "arraysObs = {}\n",
        "\n",
        "for yr in years:\n",
        "    # Generator of (geometry, value) pairs\n",
        "    shapes = ((geom, value) for geom, value in zip(gpObs.geometry, gpObs[yr]))\n",
        "    # Rasterize into a float32 array\n",
        "    arrObs = rasterize(\n",
        "        shapes=shapes,\n",
        "        out_shape=(height, width),\n",
        "        transform=transform,\n",
        "        fill=nodata, # type: ignore\n",
        "        dtype='float32')\n",
        "    arraysObs[yr] = arrObs\n",
        "\n",
        "# Now `arraysObs[1998]`, `arraysObs[1999]`, …, `arraysObs[2023]`\n",
        "# contain the rasterized observation counts per year."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "abd4f935",
      "metadata": {},
      "outputs": [],
      "source": [
        "# ──────────────────────────────────────────────────────────────────────────────\n",
        "# 1. Define which years to preview (e.g., 2018–2021)\n",
        "# ------------------------------------------------------------------------------\n",
        "preview_years = [yr for yr in years if 2018 <= yr <= 2021]\n",
        "\n",
        "# ──────────────────────────────────────────────────────────────────────────────\n",
        "# 2. Collect all non‐zero, non‐nodata values across those years\n",
        "#    This builds a 1D array for computing a common color scale\n",
        "# ------------------------------------------------------------------------------\n",
        "all_nonzero = np.hstack([\n",
        "    arraysObs[yr][\n",
        "        (arraysObs[yr] != nodata) &\n",
        "        (arraysObs[yr] > 0)\n",
        "    ].ravel()\n",
        "    for yr in preview_years\n",
        "])\n",
        "\n",
        "# ──────────────────────────────────────────────────────────────────────────────\n",
        "# 3. Compute quantiles for a discrete 5‐interval color scale\n",
        "# ------------------------------------------------------------------------------\n",
        "obs_quant = np.quantile(all_nonzero, np.linspace(0, 1, 6))\n",
        "# print(\"Quantile breakpoints:\", obs_quant)\n",
        "\n",
        "# ──────────────────────────────────────────────────────────────────────────────\n",
        "# 4. Build a colormap and normalization based on those quantiles\n",
        "# ------------------------------------------------------------------------------\n",
        "# Use the resampled API\n",
        "base = mpl.colormaps['viridis_r']\n",
        "cmap = base.resampled(len(obs_quant) - 1)\n",
        "norm = BoundaryNorm(obs_quant, ncolors=cmap.N, clip=True)\n",
        "\n",
        "# ──────────────────────────────────────────────────────────────────────────────\n",
        "# 5. Plot a 2×2 panel of raster previews with a consistent scale\n",
        "# ------------------------------------------------------------------------------\n",
        "fig, axes = plt.subplots(2, 2, figsize=(10, 10))\n",
        "\n",
        "for ax, yr in zip(axes.flat, preview_years):\n",
        "    # Mask out nodata pixels for plotting\n",
        "    masked = np.ma.masked_equal(arraysObs[yr], nodata)\n",
        "    # Show the raster\n",
        "    show(\n",
        "        masked,\n",
        "        transform=transform,\n",
        "        ax=ax,\n",
        "        cmap=cmap,\n",
        "        norm=norm,\n",
        "        title=str(yr)\n",
        "    )\n",
        "    # Overlay the boundary outline\n",
        "    boundary_gdf.boundary.plot(\n",
        "        ax=ax,\n",
        "        color='black',\n",
        "        linewidth=1.2\n",
        "    )\n",
        "    ax.set_axis_off()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "654790f1",
      "metadata": {},
      "outputs": [],
      "source": [
        "# ──────────────────────────────────────────────────────────────────────────────\n",
        "# Define output path and years\n",
        "# ------------------------------------------------------------------------------\n",
        "obs_tif = 'data/sampling_effort_1998_2023.tif'\n",
        "years   = list(range(1998, 2024))  # inclusive list of years\n",
        "nodata  = -9999.0                   # NoData flag used in arraysObs\n",
        "\n",
        "# ──────────────────────────────────────────────────────────────────────────────\n",
        "# Pre‐compute a mask for pixels inside the study boundary\n",
        "# ------------------------------------------------------------------------------\n",
        "# Create a single geometry covering all boundary features\n",
        "boundary_union = boundary_gdf.unary_union\n",
        "\n",
        "# Rasterize: 1 inside boundary, 0 outside → boolean mask\n",
        "mask = rasterize(\n",
        "    [(boundary_union, 1)],\n",
        "    out_shape=(height, width),\n",
        "    transform=transform,\n",
        "    fill=0,\n",
        "    dtype='uint8'\n",
        ").astype(bool)\n",
        "\n",
        "# ──────────────────────────────────────────────────────────────────────────────\n",
        "# Write a multi‐band GeoTIFF where each band is one year’s effort raster\n",
        "# and values outside the boundary or ≤0 are set to NoData\n",
        "# ------------------------------------------------------------------------------\n",
        "with rasterio.open(\n",
        "    obs_tif,\n",
        "    'w',\n",
        "    driver='GTiff',\n",
        "    width=width,\n",
        "    height=height,\n",
        "    count=len(years),       # one band per year\n",
        "    crs=gpObs.crs,          # same CRS as the polygon grid\n",
        "    transform=transform,\n",
        "    dtype='float32',\n",
        "    nodata=nodata,\n",
        "    compress='lzw'          # lossless compression\n",
        ") as dstObs:\n",
        "\n",
        "    for i, yr in enumerate(years, start=1):\n",
        "        # 1) Load the precomputed 2D numpy array for this year\n",
        "        arr = arraysObs[yr]   # float32, with nodata flags already present\n",
        "\n",
        "        # 2) Build a valid-data mask: must be inside boundary AND >0\n",
        "        valid = mask & (arr > 0)\n",
        "\n",
        "        # 3) Clip values outside boundary or non-positive to nodata\n",
        "        clipped = np.where(valid, arr, nodata).astype('float32')\n",
        "\n",
        "        # 4) Write the clipped array to band i, and set the band’s description\n",
        "        dstObs.write(clipped, i)\n",
        "        dstObs.set_band_description(i, str(yr))\n",
        "\n",
        "print(f\"✓ Wrote masked GeoTIFF '{obs_tif}' with {len(years)} annual bands.\")    "
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ac7da0b0",
      "metadata": {
        "id": "ac7da0b0"
      },
      "source": [
        "##### 5.2.\tSpatio-Temporal Data Cube >> Species Richness\n",
        "This cube stores species richness values (i.e. the number of unique species recorded) for each grid cell over multiple time steps. It enables users to explore biodiversity patterns across both spatial and temporal dimensions, facilitating the detection of shifts in species composition, local extinctions, or emerging biodiversity hotspots.\n",
        "Visual outputs from this cube may include interactive maps, animated sequences, or 3D plots showing how richness evolves over time. These tools support interpretation of complex spatiotemporal patterns and are valuable for ecological forecasting, conservation prioritisation, and communicating results to stakeholders."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9b957dee",
      "metadata": {
        "id": "9b957dee"
      },
      "outputs": [],
      "source": [
        "# ──────────────────────────────────────────────────────────────────────────────\n",
        "# 1. Aggregate unique species per grid cell per year\n",
        "#    • Group by grid_id and year, count distinct 'species', then pivot years to columns\n",
        "# ------------------------------------------------------------------------------\n",
        "spp_rich = (\n",
        "    grid_counts\n",
        "      .groupby(['grid_id', 'year'])['species']\n",
        "      .nunique()\n",
        "      .unstack(fill_value=0)  # one column per year, fill missing with 0\n",
        ")\n",
        "\n",
        "# ──────────────────────────────────────────────────────────────────────────────\n",
        "# 2. Join richness table to grid polygons\n",
        "#    • Drops centroid metadata since geometry already in grid_polys\n",
        "# ------------------------------------------------------------------------------\n",
        "years = list(range(1998, 2024))\n",
        "gp = grid_polys.set_index('grid_id').join(spp_rich)\n",
        "\n",
        "# ──────────────────────────────────────────────────────────────────────────────\n",
        "# 3. Derive raster geometry parameters from the joined grid\n",
        "#    • total_bounds → raster extent\n",
        "#    • first polygon’s bounds → cell size (dx, dy)\n",
        "#    • compute width, height, and affine transform\n",
        "# ------------------------------------------------------------------------------\n",
        "minx, miny, maxx, maxy = gp.total_bounds\n",
        "dx = gp.geometry.iloc[0].bounds[2] - gp.geometry.iloc[0].bounds[0] # type: ignore   # cell width\n",
        "dy = gp.geometry.iloc[0].bounds[3] - gp.geometry.iloc[0].bounds[1] # type: ignore  # cell height\n",
        "\n",
        "transform = from_origin(minx, maxy, dx, dy)\n",
        "width  = int(np.round((maxx - minx) / dx))\n",
        "height = int(np.round((maxy - miny) / dy))\n",
        "\n",
        "# ──────────────────────────────────────────────────────────────────────────────\n",
        "# 4. Rasterize species richness for each year into a dict of arrays\n",
        "#    • Use a consistent NoData flag\n",
        "# ------------------------------------------------------------------------------\n",
        "nodata = -9999.0     # pick any nodata flag that will never occur in your data\n",
        "arrays = {}\n",
        "for yr in years:\n",
        "    shapes = ((geom, value) for geom, value in zip(gp.geometry, gp[yr]))\n",
        "    arr = rasterize(\n",
        "        shapes=shapes,\n",
        "        out_shape=(height, width),\n",
        "        transform=transform,\n",
        "        fill=nodata, # type: ignore\n",
        "        dtype='float32')\n",
        "    arrays[yr] = arr\n",
        "\n",
        "# ──────────────────────────────────────────────────────────────────────────────\n",
        "# 5. Prepare a 2×2 preview of selected years (2018–2021) with quantile‐based scale\n",
        "# ------------------------------------------------------------------------------\n",
        "preview_years = [yr for yr in years if 2018 <= yr <= 2021]\n",
        "\n",
        "# ──────────────────────────────────────────────────────────────────────────────\n",
        "# 5a. Stack all non‐zero, non‐nodata values to compute common quantiles\n",
        "# ------------------------------------------------------------------------------\n",
        "all_vals = np.hstack([\n",
        "    arrays[yr][(arrays[yr] != nodata) & (arrays[yr] > 0)].ravel()\n",
        "    for yr in preview_years\n",
        "])\n",
        "rich_quant = np.quantile(all_vals, np.linspace(0, 1, 6))\n",
        "# print(\"Quantile breakpoints:\", rich_quant)\n",
        "\n",
        "# ──────────────────────────────────────────────────────────────────────────────\n",
        "# 5b. Build discrete colormap + normalizer\n",
        "# ------------------------------------------------------------------------------\n",
        "# Use the resampled API\n",
        "base = mpl.colormaps['viridis_r']\n",
        "cmap = base.resampled(len(rich_quant) - 1)\n",
        "norm = BoundaryNorm(rich_quant, ncolors=cmap.N, clip=True)\n",
        "\n",
        "# ──────────────────────────────────────────────────────────────────────────────\n",
        "# 5c. Plot panel of rasters with shared scale\n",
        "# ------------------------------------------------------------------------------\n",
        "fig, axes = plt.subplots(2, 2, figsize=(10, 10))\n",
        "for ax, yr in zip(axes.flat, preview_years):\n",
        "    masked = np.ma.masked_equal(arrays[yr], nodata)\n",
        "    show(\n",
        "        masked,\n",
        "        transform=transform,\n",
        "        ax=ax,\n",
        "        cmap=cmap,\n",
        "        norm=norm,\n",
        "        title=str(yr)\n",
        "    )\n",
        "    boundary_gdf.boundary.plot(ax=ax, color='black', linewidth=0.5)\n",
        "    ax.set_axis_off()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "40a0840a",
      "metadata": {
        "id": "40a0840a"
      },
      "outputs": [],
      "source": [
        "# ──────────────────────────────────────────────────────────────────────────────\n",
        "# 0. Preliminaries: years list, nodata flag, CRS, transform, dimensions\n",
        "# ------------------------------------------------------------------------------\n",
        "years     = list(range(1998, 2024))  # inclusive list of annual bands\n",
        "nodata    = -9999.0                  # flag for NoData values\n",
        "crs_wgs84 = \"EPSG:4326\"              # target CRS for all layers\n",
        "\n",
        "# 'transform', 'width', 'height', and 'gp' (grid polygons GeoDataFrame)\n",
        "# are assumed defined as in prior steps.\n",
        "\n",
        "# ──────────────────────────────────────────────────────────────────────────────\n",
        "# 1. Build a geometry mask for the study boundary\n",
        "#    • True inside the boundary, False outside\n",
        "# ------------------------------------------------------------------------------\n",
        "boundary_aligned = boundary_gdf.to_crs(crs_wgs84)\n",
        "\n",
        "# Rasterize: 1 inside boundary, 0 outside → boolean mask\n",
        "mask = rasterize(\n",
        "    [(boundary_union, 1)],\n",
        "    out_shape=(height, width),\n",
        "    transform=transform,\n",
        "    fill=0,\n",
        "    dtype='uint8'\n",
        ").astype(bool)\n",
        "\n",
        "# ──────────────────────────────────────────────────────────────────────────────\n",
        "# 2. Write a multi-band GeoTIFF of species richness per year\n",
        "#    • Values ≤ 0 or outside boundary are set to NoData\n",
        "# ------------------------------------------------------------------------------\n",
        "rich_tif = 'data/species_richness_1998_2023.tif'\n",
        "\n",
        "with rasterio.open(\n",
        "    rich_tif,\n",
        "    'w',\n",
        "    driver     = 'GTiff',\n",
        "    width      = width,\n",
        "    height     = height,\n",
        "    count      = len(years),    # one band per year\n",
        "    crs        = crs_wgs84,\n",
        "    transform  = transform,\n",
        "    dtype      = 'float32',\n",
        "    nodata     = nodata,\n",
        "    compress   = 'lzw'\n",
        ") as dst:\n",
        "\n",
        "    for i, yr in enumerate(years, start=1):\n",
        "        # 2a. Fetch the pre-rasterized array for this year\n",
        "        arr = arrays[yr]\n",
        "\n",
        "        # 2b. Build valid-data mask: inside boundary AND arr > 0\n",
        "        valid = mask & (arr > 0)\n",
        "\n",
        "        # 2c. Clip values: keep valid, else set to nodata\n",
        "        clipped = np.where(valid, arr, nodata).astype('float32')\n",
        "\n",
        "        # 2d. Write clipped array to band 'i' and label it with the year\n",
        "        dst.write(clipped, i)\n",
        "        dst.set_band_description(i, str(yr))\n",
        "\n",
        "print(f\"✓ Wrote masked GeoTIFF: {rich_tif} ({len(years)} annual bands, values ≤0 set to NoData)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "87cb51c5",
      "metadata": {
        "id": "87cb51c5"
      },
      "source": [
        "---\n",
        "#### 6.\tGET ENVIRONMENTAL COVARIATES\n",
        "> Environmental covariates—such as temperature, precipitation, elevation, vegetation productivity, and human population density—are critical for understanding the patterns and processes underlying biodiversity. These variables influence species distributions, community composition, and ecological dynamics across spatial and temporal scales. Capturing them as Data Cubes enables the integration of environmental context into biodiversity analyses, providing consistent, multi-layered inputs for modelling and monitoring. Each variable takes a different structural form depending on their dimensions. Elevation is a simple 3D matrix with two spatial dimensions (latitude and longitude) and one variable (elevation), but no temporal component. In contrast, WorldClim’s bioclimatic variables form a multi-dimensional matrix or spatial data cube, consisting of spatial dimensions and multiple variables (e.g. BIO1 to BIO19), but still static in time. Meanwhile, variables like NDVI, rainfall, and surface water are true spatio-temporal data cubes—they include latitude, longitude, and time dimensions, with one variable per cube (e.g. NDVI index, precipitation in mm, or water presence/absence).\n",
        "> Each cube encodes changes in an environmental variable across geographic space and time, allowing researchers to assess ecological responses to climate variability, habitat change, and anthropogenic pressures. These cubes are essential for linking biodiversity observations to their environmental drivers.   \n",
        "> Browse available dataset on [Earth Engine Data Catalog](https://developers.google.com/earth-engine/datasets/) as well as the [awesome-gee-community-catalog](https://gee-community-catalog.org/projects/).   \n",
        ">  \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "743b5655",
      "metadata": {},
      "source": [
        "##### 6.0.\tUser inputs\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "415baa54",
      "metadata": {},
      "outputs": [],
      "source": [
        "# ──────────────────────────────────────────────────────────────────────────────\n",
        "# 1. USER INPUTS: Define the analysis period\n",
        "#    • start_date: first day to include (inclusive)\n",
        "#    • end_date:   last day to include (inclusive)\n",
        "# ------------------------------------------------------------------------------\n",
        "start_date = ee.Date('2000-01-01')  # type: ignore\n",
        "end_date   = ee.Date('2020-12-31')  # type: ignore\n",
        "\n",
        "# ──────────────────────────────────────────────────────────────────────────────\n",
        "# 2. Compute how many full months lie between start_date and end_date\n",
        "#    • difference(..., 'month') returns a server-side Number\n",
        "#    • .toInt() ensures we have an integer count\n",
        "# ------------------------------------------------------------------------------\n",
        "n_months = end_date.difference(start_date, 'month').toInt()  # type: ignore\n",
        "\n",
        "# ──────────────────────────────────────────────────────────────────────────────\n",
        "# 3. Build an Earth Engine List of month-offset indices\n",
        "#    • sequence from 0 up to (n_months - 1)\n",
        "#    • each element corresponds to months after start_date\n",
        "# ------------------------------------------------------------------------------\n",
        "month_ids = ee.List.sequence(0, n_months.subtract(1))  # type: ignore\n",
        "\n",
        "# Now you can map over `month_ids` to create one image per month:\n",
        "# month_images = ee.ImageCollection(month_ids.map(lambda m: make_periodic(m, 'month', 'sum', rain, south_africa, start_date)))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ab18f5ad",
      "metadata": {
        "id": "ab18f5ad"
      },
      "source": [
        "##### 6.1.\tCountry Boundaries\n",
        "[LSIB 2017: Large Scale International Boundary Polygons, Simplified](https://developers.google.com/earth-engine/datasets/catalog/USDOS_LSIB_SIMPLE_2017) is a simplified version of the detailed LSIB (2013) derived from two other datasets: a LSIB line vector file and the World Vector Shorelines (WVS) from the National Geospatial-Intelligence Agency (NGA). Compared with the detailed LSIB, in this simplified dataset some disjointed regions of each country have been reduced to a single feature. Furthermore, it excludes medium and smaller islands. The resulting simplified boundary lines are rarely shifted by more than 100 meters from the detailed LSIB lines. Each of the 312 features is a part of the geometry of one of the 284 countries described in this dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "da51d6f4",
      "metadata": {
        "id": "da51d6f4"
      },
      "outputs": [],
      "source": [
        "# ──────────────────────────────────────────────────────────────────────────────\n",
        "# Define the Region of Interest (ROI) as South Africa (including Lesotho/Eswatini)\n",
        "# ------------------------------------------------------------------------------\n",
        "# Option A: Load from a public LSIB dataset and filter by country name\n",
        "# south_africa = (\n",
        "#     ee.FeatureCollection('USDOS/LSIB_SIMPLE/2017')\n",
        "#       .filter(ee.Filter.inList('country_na', ['South Africa', 'Lesotho', 'Swaziland']))\n",
        "# )\n",
        "\n",
        "# Option B: Load from a custom Earth Engine asset (pre-uploaded boundary)\n",
        "south_africa = ee.FeatureCollection( # type: ignore\n",
        "    \"projects/ee-nithecs/assets/rsa_boundary_shp\"  # custom shapefile asset\n",
        ")  # type: ignore\n",
        "\n",
        "# Derive a rectangle covering the entire ROI (useful for exports or tiling)\n",
        "aoi = south_africa.bounds()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4d618fc9",
      "metadata": {
        "id": "4d618fc9"
      },
      "source": [
        "##### 6.2.\tInteractive mapping with `geemap`\n",
        "This snippet initializes a Geemap interactive map centered on South Africa at zoom level 5, then populates the basemap gallery with three common layers—Roadmap (as the default), Hybrid, and Terrain—so users can switch seamlessly between vector, satellite, and topographic views. By leveraging `geemap`’s built-in Earth Engine integration, it provides a lightweight web map environment for overlaying remote-sensing imagery, vector boundaries, and other geospatial datasets, enabling real-time pan, zoom, and layer toggling without any manual HTML or JavaScript setup.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "44a937e6",
      "metadata": {},
      "outputs": [],
      "source": [
        "# ──────────────────────────────────────────────────────────────────────────────\n",
        "# 1. Initialize an interactive map centered on South Africa\n",
        "# ------------------------------------------------------------------------------\n",
        "m = geemap.Map()\n",
        "m.centerObject(south_africa, zoom=5)  # smaller zoom ⇒ more zoomed out\n",
        "\n",
        "# ──────────────────────────────────────────────────────────────────────────────\n",
        "# 2. Add common basemap layers for user toggling\n",
        "#    • \"ROADMAP\": default Google roadmap\n",
        "#    • \"HYBRID\": satellite + labels\n",
        "#    • \"TERRAIN\": terrain with relief shading\n",
        "# ------------------------------------------------------------------------------\n",
        "m.add_basemap(\"ROADMAP\",    show=True)   # show by default\n",
        "m.add_basemap(\"HYBRID\",     show=False)\n",
        "m.add_basemap(\"TERRAIN\",    show=False)\n",
        "\n",
        "# ──────────────────────────────────────────────────────────────────────────────\n",
        "# 3. Overlay the South Africa boundary (no fill, semi-transparent outline)\n",
        "# ------------------------------------------------------------------------------\n",
        "m.addLayer(\n",
        "    south_africa,           # EE FeatureCollection of RSA + Lesotho + Eswatini\n",
        "    {\n",
        "        \"color\": \"black\",    # boundary line color\n",
        "        \"fillOpacity\": 0     # no interior fill\n",
        "    },\n",
        "    name=\"RSA + Lesotho + eSwatini\",\n",
        "    shown=True,\n",
        "    opacity=0.5\n",
        ")\n",
        "\n",
        "# ──────────────────────────────────────────────────────────────────────────────\n",
        "# 4. Finalize and display the map with layer controls\n",
        "# ------------------------------------------------------------------------------\n",
        "m.addLayerControl()  # enable the basemap and overlay toggles\n",
        "m  # renders in Jupyter or VS Code notebook"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2571634d",
      "metadata": {
        "id": "2571634d"
      },
      "source": [
        "##### 6.3.\tElevation\n",
        "[NASADEM: NASA 30m Digital Elevation Model](https://developers.google.com/earth-engine/datasets/catalog/NASA_NASADEM_HGT_001) is an enhanced 30-meter digital elevation model based on reprocessed Shuttle Radar Topography Mission (SRTM) data. Improvements include void-filling and error correction using auxiliary datasets such as ASTER GDEM, ICESat GLAS, and PRISM. The product represents elevation surfaces collected around 2000, offering accurate topographic information globally. Elevation data are essential for modeling terrain-driven processes such as hydrology, erosion, and species distributions. Derived products include slope, aspect, and hillshade, which are valuable inputs in environmental modeling, remote sensing, and biodiversity research in topographically complex landscapes.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "41c4705e",
      "metadata": {
        "id": "41c4705e"
      },
      "outputs": [],
      "source": [
        "# ──────────────────────────────────────────────────────────────────────────────\n",
        "# Add NASADEM elevation layer to the interactive map `m`\n",
        "# ------------------------------------------------------------------------------\n",
        "\n",
        "# 1. Load NASADEM elevation band and clip to South Africa boundary\n",
        "elev = (\n",
        "    ee.Image('NASA/NASADEM_HGT/001')   # NASADEM global height # type: ignore\n",
        "      .select('elevation')             # pick the elevation band\n",
        "      .clip(south_africa)              # restrict to our ROI\n",
        ")\n",
        "\n",
        "# 2. Define visualization parameters:\n",
        "#    • min/max elevation (m)\n",
        "#    • color palette from deep green to white for high peaks\n",
        "elev_vis = {\n",
        "    'min': 0,\n",
        "    'max': 2000,\n",
        "    'palette': [\n",
        "        '#004400',  # deep forest green\n",
        "        '#228B22',  # medium green\n",
        "        '#7FFF00',  # light green\n",
        "        '#FFFF00',  # yellow\n",
        "        '#F4A460',  # sandy tan\n",
        "        '#8B4513',  # dark brown\n",
        "        '#D9D9D9',  # light grey (rocky)\n",
        "        '#FFFFFF'   # snow/ice\n",
        "    ]\n",
        "}\n",
        "\n",
        "# 3. Add the elevation layer to the map\n",
        "m.addLayer(\n",
        "    elev,\n",
        "    elev_vis,\n",
        "    name=\"Elevation (m)\"\n",
        ")\n",
        "\n",
        "# 4. (Optional) Re-add the boundary outline for context\n",
        "#    Since we already added it earlier, set show=False to keep it available but hidden by default\n",
        "m.addLayer(south_africa, {'color': 'black', 'fillOpacity': 0}, \n",
        "           \"South Africa (Lesotho & eSwatini)\", \n",
        "           False)\n",
        "\n",
        "# 5. Display the map (in a notebook or VS Code)\n",
        "m"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0d07fb66",
      "metadata": {
        "id": "0d07fb66"
      },
      "outputs": [],
      "source": [
        "# ──────────────────────────────────────────────────────────────────────────────\n",
        "# Export the clipped NASADEM elevation image to a local GeoTIFF\n",
        "# ------------------------------------------------------------------------------\n",
        "elev_tif = 'data/elevation_nasadem.tif'  # local file path for export\n",
        "\n",
        "geemap.ee_export_image(\n",
        "    elev,                             # EE Image to export (no 'image=' keyword)\n",
        "    filename     = elev_tif,          # local output path\n",
        "    region       = south_africa.geometry(),  # clip/export region\n",
        "    scale        = 5000,              # output resolution in meters (NASADEM ≈30m native)\n",
        "    crs          = \"EPSG:4326\",       # coordinate reference system\n",
        "    file_per_band=False              # combine all bands (just one here) into one file\n",
        ")\n",
        "\n",
        "print(\"✓ Elevation GeoTIFF saved →\", elev_tif)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5472923d",
      "metadata": {
        "id": "5472923d"
      },
      "source": [
        "##### 6.4.\tHuman Population\n",
        "[LandScan Population Data Global 1km]( https://developers.google.com/earth-engine/datasets/catalog/projects_sat-io_open-datasets_ORNL_LANDSCAN_GLOBAL) provides a high-resolution global population dataset developed by Oak Ridge National Laboratory. It uses census data, satellite imagery, land use, and infrastructure to estimate population distribution at ~1 km resolution (30 arc-seconds). LandScan models ambient population, representing the average presence over 24 hours rather than residential counts. It is widely used in disaster risk reduction, epidemiology, and conservation planning, particularly in assessing human pressures on ecosystems. The spatial granularity supports integration with environmental and socio-economic models, making it useful for global sustainability and spatial decision-making studies.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1b3b8247",
      "metadata": {
        "id": "1b3b8247"
      },
      "outputs": [],
      "source": [
        "# ──────────────────────────────────────────────────────────────────────────────\n",
        "# Add LandScan global population (~1 km resolution) to the map\n",
        "# ------------------------------------------------------------------------------\n",
        "\n",
        "# 1. Load the LandScan Global Population ImageCollection\n",
        "#    This contains annual population density rasters at ~1 km resolution.\n",
        "landscan_global = ee.ImageCollection( # type: ignore\n",
        "    'projects/sat-io/open-datasets/ORNL/LANDSCAN_GLOBAL'  # type: ignore\n",
        ")\n",
        "\n",
        "# 2. Define visualization parameters:\n",
        "#    • min/max density (people per km²)\n",
        "#    • a color palette from light greys to deep reds\n",
        "pop_vis = {\n",
        "    'min': 0,\n",
        "    'max': 4200,\n",
        "    'palette': [\n",
        "        '#CCCCCC', '#FFFFBE', '#FEFF73', '#FEFF2C',\n",
        "        '#FFAA27', '#FF6625', '#FF0023', '#CC001A', '#730009'\n",
        "    ]\n",
        "}\n",
        "\n",
        "# 3. Compute the mean population density over all years, clip to RSA boundary\n",
        "pop_mean = landscan_global.mean().clip(south_africa)\n",
        "\n",
        "# 4. Add the population layer to the interactive map `m`\n",
        "m.addLayer(\n",
        "    pop_mean,\n",
        "    pop_vis,\n",
        "    name=\"Mean Population Density\"\n",
        ")\n",
        "\n",
        "# 5. Display the updated map\n",
        "m"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a40714ce",
      "metadata": {
        "id": "a40714ce"
      },
      "source": [
        "##### 6.5.\tRoads\n",
        "[Global Roads Inventory Project (GRIP) global roads database](https://gee-community-catalog.org/projects/grip/) was developed to provide a more recent and consistent global roads dataset for use in global environmental and biodiversity assessment models like GLOBIO. The GRIP dataset consists of global and regional vector datasets in ESRI filegeodatabase and shapefile format, and global raster datasets of road density at a 5 arcminutes resolution (~8km x 8km). The GRIP dataset is mainly aimed at providing a roads dataset that is easily usable for scientific global environmental and biodiversity modelling projects. The dataset is not suitable for navigation. GRIP4 is based on many different sources (including OpenStreetMap) and to the best of our ability we have verified their public availability, as a criteria in our research. The UNSDI-Transportation datamodel was applied for harmonization of the individual source datasets.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c6fccee6",
      "metadata": {
        "id": "c6fccee6"
      },
      "outputs": [],
      "source": [
        "# ──────────────────────────────────────────────────────────────────────────────\n",
        "# 1. Load & filter the Global Roads Inventory Project (GRIP4) for South Africa\n",
        "# ------------------------------------------------------------------------------\n",
        "grip4_africa = ee.FeatureCollection( # type: ignore\n",
        "    \"projects/sat-io/open-datasets/GRIP4/Africa\"  # all African roads\n",
        ")\n",
        "\n",
        "# Clip to our ROI (South Africa + Lesotho + Eswatini boundary)\n",
        "grip4_rsa = grip4_africa.filterBounds(south_africa.bounds())\n",
        "# print(grip4_rsa.first().propertyNames().getInfo())\n",
        "# print(grip4_rsa.first().getInfo()['properties'])\n",
        "\n",
        "# ──────────────────────────────────────────────────────────────────────────────\n",
        "# 2. Inspect available road-type codes (GP_RTP property)\n",
        "# ------------------------------------------------------------------------------\n",
        "unique_types = (\n",
        "    grip4_rsa\n",
        "      .aggregate_array('GP_RTP')  # collect all GP_RTP values\n",
        "      .distinct()                 # remove duplicates\n",
        "      .sort()                     # sort ascending\n",
        "      .getInfo()                  # bring to client\n",
        ")\n",
        "print('Unique GP_RTP values:', unique_types)\n",
        "# e.g. [1, 2, 3, 4, 5]\n",
        "\n",
        "# ──────────────────────────────────────────────────────────────────────────────\n",
        "# 3. Define styling for road types 1–3 only (others omitted/grouped)\n",
        "#    • Type 1: Highways (red, thick)\n",
        "#    • Type 2: Primary roads (blue, medium)\n",
        "#    • Type 3: Secondary roads (green, thin)\n",
        "# ------------------------------------------------------------------------------\n",
        "styles = {\n",
        "    1: {'color': '#e41a1c', 'width': 2.5}, # Highways\n",
        "    2: {'color': '#377eb8', 'width': 2.0}, # Primary roads\n",
        "    3: {'color': '#4daf4a', 'width': 1.0}, # Secondary roads\n",
        "    # 4: {'color':\"#d0d2cf\", 'width':0.5},     # Tertiary roads\n",
        "    # 5: {'color':\"#e8eae7\", 'width':0.5},     # Local roads\n",
        "}\n",
        "\n",
        "# ──────────────────────────────────────────────────────────────────────────────\n",
        "# 4. Add each styled road layer to the interactive map\n",
        "#    • Plot in reverse order so Type 1 is on top\n",
        "#    • Off by default; users can toggle via layer control\n",
        "# ------------------------------------------------------------------------------\n",
        "for t in sorted(styles.keys(), reverse=True):  # 5,4,3,2,1\n",
        "    fc = grip4_rsa.filter(ee.Filter.eq('GP_RTP', t)) # type: ignore\n",
        "    m.addLayer(\n",
        "        fc.style(color=styles[t]['color'], width=styles[t]['width']),\n",
        "        {},\n",
        "        f'Road Type {t}', False\n",
        "    )\n",
        "\n",
        "# ──────────────────────────────────────────────────────────────────────────────\n",
        "# 5. Display the map with layer control (in Jupyter or VS Code)\n",
        "# ------------------------------------------------------------------------------\n",
        "# m.addLayerControl()\n",
        "m"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e59da09d",
      "metadata": {},
      "source": [
        "**Compute distance to nearest major road (meters)**   \n",
        "This section extracts the primary transport network from the GRIP4 dataset (road categories 1–3), computes a continuous distance‐to‐nearest‐roads, and renders it interactively. First, we filter for highways, primary and secondary roads; next, we rasterize these features into a binary mask and calculate Euclidean distance (in meters, then converted to kilometers) within a 100 km radius; finally, we overlay this distance surface on the map to highlight zones of increasing remoteness from major roads."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ec496681",
      "metadata": {},
      "outputs": [],
      "source": [
        "# ──────────────────────────────────────────────────────────────────────────────\n",
        "# 1. Filter GRIP4 roads to types 1–3 (highways, primary, secondary)\n",
        "# ------------------------------------------------------------------------------\n",
        "roads_1to3 = grip4_rsa.filter(ee.Filter.inList('GP_RTP', [1, 2, 3]))  # type: ignore\n",
        "\n",
        "# ──────────────────────────────────────────────────────────────────────────────\n",
        "# 2. Compute distance-to-nearest-road surface (meters)\n",
        "#    • Paint all roads → binary mask\n",
        "#    • distance() with a large kernel → Euclidean distance per pixel\n",
        "#    • Clip to the study area\n",
        "# ------------------------------------------------------------------------------\n",
        "road_mask = ee.Image().byte().paint(roads_1to3, 1)  # 1 inside road, 0 elsewhere # type: ignore\n",
        "dist_to_roads = (\n",
        "    road_mask\n",
        "      .distance(\n",
        "          ee.Kernel.euclidean(radius=100000, units='meters'), # type: ignore\n",
        "          False # type: ignore\n",
        "      )\n",
        "      .clip(south_africa.geometry()).divide(1000)\n",
        ")\n",
        "\n",
        "# 2a. Visualize distance surface (white = near, red = far up to 20 km)\n",
        "m.addLayer(\n",
        "    dist_to_roads,\n",
        "    {'min': 0, 'max': 55, 'palette': ['ffffff','ff0000']},\n",
        "    name='Distance to nearest main road'\n",
        ")\n",
        "\n",
        "# ──────────────────────────────────────────────────────────────────────────────\n",
        "# 3. Display the interactive map (in Jupyter/VS Code)\n",
        "# ------------------------------------------------------------------------------\n",
        "# m.addLayerControl()\n",
        "m  # renders the map in notebook environments"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "75530e25",
      "metadata": {},
      "source": [
        "**Compute distance to nearest road by type (meters)**   \n",
        "In this section, we define a 5 km neighborhood radius and generate separate distance‐to‐roads for each road category (highway, primary, secondary). We encapsulate the workflow in a `make_dist_band()` function that:\n",
        "1. Filters and rasterizes roads of a given type into a binary mask.\n",
        "2. Computes Euclidean distance (m) to the nearest road pixel and clips to our study area.\n",
        "3. Converts distances into kilometres and labels the band as `dist_type{t}`.\n",
        "   \n",
        "We then stack the three distance bands into a multi‐band image and add each layer to the interactive map using consistent styling (white at 0 km, red at 100 km) for direct comparison of remoteness across road types."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3514171d",
      "metadata": {},
      "outputs": [],
      "source": [
        "# ──────────────────────────────────────────────────────────────────────────────\n",
        "# 1. Define the analysis parameters\n",
        "#    • radius_m = 5 000 m, used for neighborhood operations if needed\n",
        "# ------------------------------------------------------------------------------\n",
        "radius_m = 5000  # neighborhood radius in metres\n",
        "\n",
        "# ──────────────────────────────────────────────────────────────────────────────\n",
        "# 2. Build a distance band for each road type (1–3)\n",
        "#    • Paint each road type to a binary mask (1 on roads, 0 elsewhere)\n",
        "#    • Compute the Euclidean distance (in metres) from every pixel to the nearest road\n",
        "#    • Clip to the study area and convert to kilometres\n",
        "#    • Rename each band as \"dist_type{t}\" for clarity\n",
        "# ------------------------------------------------------------------------------\n",
        "def make_dist_band(road_type):\n",
        "    # a) Select and paint roads of this type into a binary image\n",
        "    fc_roads = grip4_rsa.filter(ee.Filter.eq('GP_RTP', road_type))  # type: ignore\n",
        "    mask      = ee.Image().byte().paint(fc_roads, 1) # type: ignore\n",
        "    \n",
        "    # b) Compute Euclidean distance (metres) to the nearest road pixel\n",
        "    dist_m = (\n",
        "        mask\n",
        "          .distance(\n",
        "              ee.Kernel.euclidean(radius=100000, units='meters'), # type: ignore\n",
        "              False # type: ignore\n",
        "          )\n",
        "          .clip(south_africa.geometry())\n",
        "    )\n",
        "    \n",
        "    # c) Convert metres → kilometres for easier interpretation\n",
        "    dist_km = dist_m.divide(1000).rename(f\"dist_type{road_type}\")\n",
        "    \n",
        "    return dist_km\n",
        "\n",
        "# 3. Create one distance image per road type and concatenate into a multi‐band image\n",
        "dist_bands = [make_dist_band(t) for t in [1, 2, 3]]\n",
        "dist_stack = ee.Image.cat(dist_bands)  # type: ignore\n",
        "print(\"Bands in `dist_stack`:\", dist_stack.bandNames().getInfo())\n",
        "\n",
        "# ──────────────────────────────────────────────────────────────────────────────\n",
        "# 4. Visualize each band on the interactive map `m`\n",
        "#    • Use a common style: white = 0 km (on road), red = farthest (up to 100 km)\n",
        "# ------------------------------------------------------------------------------\n",
        "viz_params = {\n",
        "    'min':     0,\n",
        "    'max':   100,               # maximum distance in km for color scaling\n",
        "    'palette': ['white', 'blue', 'red']\n",
        "}\n",
        "\n",
        "m.addLayer(dist_stack.select('dist_type1'), viz_params, 'Dist to Highways')\n",
        "m.addLayer(dist_stack.select('dist_type2'), viz_params, 'Dist to Primary roads')\n",
        "m.addLayer(dist_stack.select('dist_type3'), viz_params, 'Dist to Secondary roads')\n",
        "\n",
        "# 5. Finally, render the map (in a notebook or geemap-enabled VS Code)\n",
        "m"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4b116587",
      "metadata": {},
      "outputs": [],
      "source": [
        "# ──────────────────────────────────────────────────────────────────────────────\n",
        "# Export the multi-band distance‐to‐roads image to a local GeoTIFF\n",
        "# ------------------------------------------------------------------------------\n",
        "dist_tif = 'data/dist_to_roads.tif'  # local filepath for the exported raster\n",
        "\n",
        "geemap.ee_export_image(\n",
        "    dist_stack,                     # EE Image containing stacked distance bands\n",
        "    filename     = dist_tif,        # output filename\n",
        "    region       = south_africa.geometry(),  # clip/export region\n",
        "    scale        = 5000,            # output pixel size in metres\n",
        "    crs          = \"EPSG:4326\",     # coordinate reference system\n",
        "    file_per_band=False            # combine all bands into a single file\n",
        ")\n",
        "\n",
        "# Confirm export\n",
        "n_bands = len(dist_stack.bandNames().getInfo())  # type: ignore\n",
        "print(f\"✓ Saved {n_bands}-band distance GeoTIFF → {dist_tif}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "44a70787",
      "metadata": {
        "id": "44a70787"
      },
      "source": [
        "##### 6.6.\tBioclimatic Variables\n",
        "[WorldClim V1 Bioclim](https://developers.google.com/earth-engine/datasets/catalog/WORLDCLIM_V1_BIO) provides 19 biologically meaningful variables derived from monthly temperature and precipitation data, capturing seasonality, variability, and limiting climatic factors. These variables are frequently used in species distribution modeling, ecological niche modeling, and biodiversity assessments. Covering the period 1960–1991, this dataset offers a spatial resolution of approximately 927.67 meters. The dataset supports ecological and climate-related analyses across various scales and has become a standard reference for correlating climate with biological processes. The variables include annual means, seasonality indices, and extreme temperature/precipitation metrics relevant for understanding climatic constraints on species and ecosystems.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cb9ca32c",
      "metadata": {
        "id": "cb9ca32c"
      },
      "outputs": [],
      "source": [
        "# ──────────────────────────────────────────────────────────────────────────────\n",
        "# Add WorldClim BIO variables (v1) to the interactive map\n",
        "# ------------------------------------------------------------------------------\n",
        "\n",
        "# 1. Load the WorldClim Version 1 Bioclimatic variables and clip to RSA\n",
        "bioclim = (\n",
        "    ee.Image('WORLDCLIM/V1/BIO')     # 19 bioclimatic bands, unit-scaled # type: ignore\n",
        "      .clip(south_africa)            # restrict to South Africa + neighbors\n",
        ")\n",
        "\n",
        "# 2. Define visualization parameters for Annual Mean Temperature (bio01)\n",
        "#    • Multiply by 0.1 to convert to °C (per WorldClim scaling)\n",
        "#    • Color palette from cold (blue) to hot (red)\n",
        "bio_vis = {\n",
        "    'min': -23,                      # lower bound in °C\n",
        "    'max': 30,                       # upper bound in °C\n",
        "    'palette': [\n",
        "        'blue',    # cold\n",
        "        'purple', \n",
        "        'cyan', \n",
        "        'green',  \n",
        "        'yellow', \n",
        "        'red'      # warm\n",
        "    ]\n",
        "}\n",
        "\n",
        "# 3. Add the bio01 layer (Annual Mean Temperature) to the map\n",
        "m.addLayer(\n",
        "    bioclim.select('bio01').multiply(0.1),  # convert scale to °C\n",
        "    bio_vis,\n",
        "    name=\"BIO01: Annual Mean Temperature (°C)\"\n",
        ")\n",
        "\n",
        "# 4. Display the map with the new layer\n",
        "m"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "898c329c",
      "metadata": {},
      "outputs": [],
      "source": [
        "# ──────────────────────────────────────────────────────────────────────────────\n",
        "# Export the multi-band BioClim image to a local GeoTIFF\n",
        "# ------------------------------------------------------------------------------\n",
        "bioclim_tif = 'data/bioclim.tif'  # local filepath for the exported raster\n",
        "\n",
        "geemap.ee_export_image(\n",
        "    bioclim,                     # EE Image containing stacked distance bands\n",
        "    filename     = bioclim_tif,        # output filename\n",
        "    region       = south_africa.geometry(),  # clip/export region\n",
        "    scale        = 5000,            # output pixel size in metres\n",
        "    crs          = \"EPSG:4326\",     # coordinate reference system\n",
        "    file_per_band=False            # combine all bands into a single file\n",
        ")\n",
        "\n",
        "# Confirm export\n",
        "print(f\"✓ Saved {len(bioclim.bandNames().getInfo())}-band distance GeoTIFF → {bioclim_tif}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "75428918",
      "metadata": {
        "id": "75428918"
      },
      "source": [
        "##### 6.7.\tVegetation Greeness\n",
        "[MOD13A3.061 Vegetation Indices Monthly L3 Global 1 km SIN Grid]( https://developers.google.com/earth-engine/datasets/catalog/MODIS_061_MOD13A3) provides monthly global vegetation indices at 1 km resolution. This product includes NDVI and EVI, derived from daily MODIS observations (MOD13A2) using a weighted temporal average. These indices reflect vegetation condition, biomass, and photosynthetic activity. NDVI is sensitive to chlorophyll content, while EVI minimizes atmospheric and canopy background effects. MOD13A3 is widely used in land cover monitoring, phenology, productivity modeling, and change detection. Applications range from local to global scales in climate-ecological studies, agricultural monitoring, and habitat suitability analyses. The dataset enables consistent long-term vegetation assessment across varied biomes.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "db6d7b32",
      "metadata": {
        "id": "db6d7b32"
      },
      "outputs": [],
      "source": [
        "# ──────────────────────────────────────────────────────────────────────────────\n",
        "# 1. Load MODIS MOD13A3 Version 061 monthly NDVI (1 km) for the analysis period\n",
        "# ------------------------------------------------------------------------------\n",
        "ndvi = (\n",
        "    ee.ImageCollection('MODIS/061/MOD13A3') # type: ignore\n",
        "      .filter(ee.Filter.date(start_date, end_date))  # restrict by date range # type: ignore\n",
        "      .select('NDVI')                                 # keep only the NDVI band\n",
        ")\n",
        "\n",
        "# ──────────────────────────────────────────────────────────────────────────────\n",
        "# 2. Define visualization parameters for NDVI\n",
        "#    • min/max values are scaled as in the MODIS product (0–9000)\n",
        "#    • a sequential green‐to‐brown palette for vegetation vigor\n",
        "# ------------------------------------------------------------------------------\n",
        "ndvi_vis = {\n",
        "    'min': 0,\n",
        "    'max': 9000,\n",
        "    'palette': [\n",
        "        'ffffff', 'ce7e45', 'df923d', 'f1b555', 'fcd163',\n",
        "        '99b718', '74a901', '66a000', '529400', '3e8601',\n",
        "        '207401', '056201', '004c00', '023b01', '012e01',\n",
        "        '011d01', '011301'\n",
        "    ]\n",
        "}\n",
        "\n",
        "# ──────────────────────────────────────────────────────────────────────────────\n",
        "# 3. Compute the mean NDVI across the time series and clip to the study area\n",
        "# ------------------------------------------------------------------------------\n",
        "ndvi_mean = ndvi.mean().clip(south_africa)\n",
        "\n",
        "# ──────────────────────────────────────────────────────────────────────────────\n",
        "# 4. Add the mean NDVI layer to the interactive map\n",
        "# ------------------------------------------------------------------------------\n",
        "m.addLayer(\n",
        "    ndvi_mean,\n",
        "    ndvi_vis,\n",
        "    name=\"Mean NDVI (MOD13A3)\"\n",
        ")\n",
        "\n",
        "# ──────────────────────────────────────────────────────────────────────────────\n",
        "# 5. Display the map (in notebook or geemap‐enabled VS Code)\n",
        "# ------------------------------------------------------------------------------\n",
        "m"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5aa92884",
      "metadata": {
        "id": "5aa92884"
      },
      "source": [
        "##### 6.8.\tRainfall\n",
        "[CHIRPS Pentad: Climate Hazards Center InfraRed Precipitation With Station Data v2.0](https://developers.google.com/earth-engine/datasets/catalog/UCSB-CHG_CHIRPS_PENTAD) offers a 30+ year quasi-global rainfall dataset combining satellite-derived precipitation estimates with station data. Available at 0.05° (~5 km) resolution, CHIRPS provides temporally consistent data suitable for monitoring rainfall patterns and detecting anomalies. It is especially useful in regions with sparse gauge coverage, offering reliable inputs for drought monitoring, agricultural planning, hydrological modeling, and climate trend analysis. The pentad format aggregates rainfall over 5-day intervals, balancing temporal resolution with data stability. CHIRPS has been widely adopted in food security and climate vulnerability applications, particularly across the tropics and subtropics.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "eab5a864",
      "metadata": {
        "id": "eab5a864"
      },
      "outputs": [],
      "source": [
        "# ──────────────────────────────────────────────────────────────────────────────\n",
        "# 1. Load CHIRPS Pentad rainfall (UCSB‐CHG) for the analysis period\n",
        "#    • Pentad = 5‐day composites\n",
        "#    • Filter by start_date and end_date defined earlier\n",
        "#    • Select only the 'precipitation' band\n",
        "# ------------------------------------------------------------------------------\n",
        "rain = (\n",
        "    ee.ImageCollection('UCSB-CHG/CHIRPS/PENTAD') # type: ignore\n",
        "      .filter(ee.Filter.date(start_date, end_date)) # type: ignore\n",
        "      .select('precipitation')\n",
        ")\n",
        "\n",
        "# ──────────────────────────────────────────────────────────────────────────────\n",
        "# 2. Define visualization parameters for mean rainfall (mm per pentad)\n",
        "#    • min/max chosen to cover most realistic pentad totals\n",
        "#    • palette from dry (white) to extreme rain (black)\n",
        "# ------------------------------------------------------------------------------\n",
        "rain_vis = {\n",
        "    'min': 0,\n",
        "    'max': 22,\n",
        "    'palette': ['white', 'lightblue', 'blue', 'darkblue', 'black']\n",
        "}\n",
        "\n",
        "# ──────────────────────────────────────────────────────────────────────────────\n",
        "# 3. Compute mean rainfall over the time series and clip to ROI\n",
        "# ------------------------------------------------------------------------------\n",
        "rain_mean = rain.mean().clip(south_africa)\n",
        "\n",
        "# ──────────────────────────────────────────────────────────────────────────────\n",
        "# 4. Add the mean rainfall layer to the interactive map\n",
        "# ------------------------------------------------------------------------------\n",
        "m.addLayer(\n",
        "    rain_mean,\n",
        "    rain_vis,\n",
        "    name=\"Mean CHIRPS Pentad Rainfall\"\n",
        ")\n",
        "\n",
        "# ──────────────────────────────────────────────────────────────────────────────\n",
        "# 5. Render the map (in a notebook or VS Code)\n",
        "# ------------------------------------------------------------------------------\n",
        "m"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b89d5931",
      "metadata": {},
      "source": [
        "##### 6.9.\tCalculate monthly or annual  summaries from `imageCollection`"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "007b5eb9",
      "metadata": {},
      "source": [
        "Below is a flexible function that can produce either **monthly** or **annual** summaries (*sum or mean*) of `imageCollections` e.g. rain collection, clipped to `boundary_fc`:"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e12d8691",
      "metadata": {},
      "source": [
        "###### Function `make_periodic()`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5d3f5bc4",
      "metadata": {},
      "outputs": [],
      "source": [
        "# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
        "# Function: make_periodic\n",
        "# --------------------------------------------------------------------\n",
        "# Generate a periodic (monthly or yearly) aggregate image from an ImageCollection.\n",
        "#\n",
        "# Parameters:\n",
        "#   offset      : int or ee.Number\n",
        "#                 Number of periods (months or years) to offset from start_date.\n",
        "#   period      : str, 'month' or 'year'\n",
        "#                 Unit of aggregation.\n",
        "#   agg         : str, 'sum' or 'mean'\n",
        "#                 Type of aggregation to perform.\n",
        "#   collection  : ee.ImageCollection\n",
        "#                 The source collection (e.g., rainfall, NDVI).\n",
        "#   boundary    : ee.Geometry or ee.FeatureCollection\n",
        "#                 Area to which the result image will be clipped.\n",
        "#   start_date  : ee.Date\n",
        "#                 Reference date for offset calculations.\n",
        "#\n",
        "# Returns:\n",
        "#   ee.Image\n",
        "#     • Pixel values = sum or mean over the period\n",
        "#     • Band name = 'YYYY_MM' for months or 'YYYY' for years\n",
        "#     • system:index property = same as band name\n",
        "# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
        "def make_periodic(\n",
        "    offset,\n",
        "    period='month',\n",
        "    agg='sum',\n",
        "    collection=rain,\n",
        "    boundary=south_africa,\n",
        "    start_date=start_date\n",
        "):\n",
        "    # 1. Cast offset to server-side Number\n",
        "    off = ee.Number(offset)  # type: ignore\n",
        "    \n",
        "    # 2. Compute the start and end dates for this period\n",
        "    period = period.lower()\n",
        "    start  = start_date.advance(off, period)\n",
        "    end    = start.advance(1, period)\n",
        "    \n",
        "    # 3. Define band name format based on period type\n",
        "    fmt = ee.String('YYYY_MM') if period == 'month' else ee.String('YYYY')  # type: ignore\n",
        "    band_name = start.format(fmt)\n",
        "    \n",
        "    # 4. Filter the collection to the period and aggregate\n",
        "    coll = collection.filterDate(start, end)\n",
        "    if   agg == 'sum':\n",
        "        img = coll.sum()\n",
        "    elif agg == 'mean':\n",
        "        img = coll.mean()\n",
        "    else:\n",
        "        # Raise a client-side error for unsupported agg types\n",
        "        raise ValueError(f\"Unsupported aggregation: {agg!r}\")\n",
        "    \n",
        "    # 5. Clip to boundary, rename the band, and set the index property\n",
        "    return (\n",
        "        img\n",
        "          .clip(boundary)\n",
        "          .rename(band_name)\n",
        "          .set('system:index', band_name)\n",
        "    )\n",
        "# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
        "# Example usage:\n",
        "#   # Monthly sums of CHIRPS rainfall:\n",
        "#   month_list = ee.List.sequence(0, end_date.difference(start_date, 'month').subtract(1))\n",
        "#   rain_monthly = ee.ImageCollection(month_list.map(lambda m: make_periodic(m, 'month', 'sum', rain)))\n",
        "#\n",
        "#   # Yearly means of MODIS NDVI:\n",
        "#   year_list = ee.List.sequence(start_date.get('year'), end_date.get('year'))\n",
        "#   ndvi_yearly = ee.ImageCollection(year_list.map(lambda y: make_periodic(y, 'year', 'mean', ndvi)))\n",
        "# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b290e4d8",
      "metadata": {},
      "source": [
        "###### Run `make_periodic()`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "64861dbf",
      "metadata": {},
      "outputs": [],
      "source": [
        "# ──────────────────────────────────────────────────────────────────────────────\n",
        "# 1. Extract the start/end years as server‐side Numbers\n",
        "# ------------------------------------------------------------------------------\n",
        "start_year = ee.Number(start_date.get('year'))  # e.g. 2000 # type: ignore\n",
        "end_year   = ee.Number(end_date.get('year'))    # e.g. 2020 # type: ignore\n",
        "\n",
        "# ──────────────────────────────────────────────────────────────────────────────\n",
        "# 2. Compute the number of years (inclusive) and build a zero‐based offset list\n",
        "#    • Offsets 0…(end_year–start_year) correspond to each successive year\n",
        "# ------------------------------------------------------------------------------\n",
        "n_years     = end_year.subtract(start_year)     # e.g. 20\n",
        "year_offsets = ee.List.sequence(0, n_years)     # [0, 1, …, 20] # type: ignore\n",
        "\n",
        "# ──────────────────────────────────────────────────────────────────────────────\n",
        "# 3. Build a fixed list of month offsets 1–12 for within‐year aggregation\n",
        "# ------------------------------------------------------------------------------\n",
        "month_offsets = ee.List.sequence(1, 12)         # [1, 2, …, 12] # type: ignore\n",
        "\n",
        "# ──────────────────────────────────────────────────────────────────────────────\n",
        "# 4. Generate ImageCollections by mapping over those offset lists\n",
        "#    • rain_monthly: monthly sums of CHIRPS rainfall\n",
        "#    • rain_yearly : annual sums of CHIRPS rainfall\n",
        "#    • ndvi_monthly: monthly means of MODIS NDVI\n",
        "#    • ndvi_yearly : annual means of MODIS NDVI\n",
        "# ------------------------------------------------------------------------------\n",
        "rain_monthly = ee.ImageCollection( # type: ignore\n",
        "    month_offsets.map(\n",
        "        lambda m: make_periodic(\n",
        "            offset=m,\n",
        "            period='month',\n",
        "            agg='sum',\n",
        "            collection=rain,\n",
        "            boundary=south_africa,\n",
        "            start_date=start_date\n",
        "        )\n",
        "    )\n",
        ")\n",
        "\n",
        "rain_yearly = ee.ImageCollection( # type: ignore\n",
        "    year_offsets.map(\n",
        "        lambda y: make_periodic(\n",
        "            offset=y,\n",
        "            period='year',\n",
        "            agg='sum',\n",
        "            collection=rain,\n",
        "            boundary=south_africa,\n",
        "            start_date=start_date\n",
        "        )\n",
        "    )\n",
        ")\n",
        "\n",
        "ndvi_monthly = ee.ImageCollection( # type: ignore\n",
        "    month_offsets.map(\n",
        "        lambda m: make_periodic(\n",
        "            offset=m,\n",
        "            period='month',\n",
        "            agg='mean',\n",
        "            collection=ndvi,\n",
        "            boundary=south_africa,\n",
        "            start_date=start_date\n",
        "        )\n",
        "    )\n",
        ")\n",
        "\n",
        "ndvi_yearly = ee.ImageCollection( # type: ignore\n",
        "    year_offsets.map(\n",
        "        lambda y: make_periodic(\n",
        "            offset=y,\n",
        "            period='year',\n",
        "            agg='mean',\n",
        "            collection=ndvi,\n",
        "            boundary=south_africa,\n",
        "            start_date=start_date\n",
        "        )\n",
        "    )\n",
        ")\n",
        "\n",
        "# ──────────────────────────────────────────────────────────────────────────────\n",
        "# 5. Optionally, inspect the first few band names to confirm\n",
        "# ------------------------------------------------------------------------------\n",
        "print(\"Rain monthly bands:\", rain_monthly.first().bandNames().getInfo())\n",
        "print(\"Rain yearly bands :\", rain_yearly.first().bandNames().getInfo())\n",
        "print(\"NDVI monthly bands:\", ndvi_monthly.first().bandNames().getInfo())\n",
        "print(\"NDVI yearly bands :\", ndvi_yearly.first().bandNames().getInfo())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "edb46652",
      "metadata": {},
      "outputs": [],
      "source": [
        "# STACK & RENAME BANDS for Rainfall & NDVI (monthly & yearly)\n",
        "# ──────────────────────────────────────────────────────────────────────────────\n",
        "# 1. Convert the monthly ImageCollections to multi-band Images\n",
        "rain_monthly_stack = rain_monthly.toBands()\n",
        "ndvi_monthly_stack = ndvi_monthly.toBands()\n",
        "# ------------------------------------------------------------------------------\n",
        "\n",
        "# ──────────────────────────────────────────────────────────────────────────────\n",
        "# 2. Rename monthly bands with clear prefixes (\"rain_YYYY_MM\", \"ndvi_YYYY_MM\")\n",
        "#    We take the first 7 characters of each original band name (e.g. \"2000_01\")\n",
        "# ------------------------------------------------------------------------------\n",
        "rain_mon_names = rain_monthly_stack.bandNames().getInfo()  # list of strings\n",
        "ndvi_mon_names = ndvi_monthly_stack.bandNames().getInfo()\n",
        "\n",
        "rain_monthly_stack = rain_monthly_stack.rename(\n",
        "    [f\"rain_{nm[:7]}\" for nm in rain_mon_names] # type: ignore\n",
        ")\n",
        "ndvi_monthly_stack = ndvi_monthly_stack.rename(\n",
        "    [f\"ndvi_{nm[:7]}\" for nm in ndvi_mon_names] # type: ignore\n",
        ")\n",
        "\n",
        "print(\"Monthly stacks:\", \n",
        "      rain_monthly_stack.bandNames().getInfo(),\n",
        "      ndvi_monthly_stack.bandNames().getInfo())\n",
        "\n",
        "# ──────────────────────────────────────────────────────────────────────────────\n",
        "# 3. Convert the yearly ImageCollections to multi-band Images\n",
        "rain_yearly_stack = rain_yearly.toBands()\n",
        "ndvi_yearly_stack = ndvi_yearly.toBands()\n",
        "# ------------------------------------------------------------------------------\n",
        "\n",
        "# ──────────────────────────────────────────────────────────────────────────────\n",
        "# 4. Rename yearly bands (\"rain_YYYY\", \"ndvi_YYYY\")\n",
        "# ------------------------------------------------------------------------------\n",
        "rain_yr_names = rain_yearly_stack.bandNames().getInfo()\n",
        "ndvi_yr_names = ndvi_yearly_stack.bandNames().getInfo()\n",
        "\n",
        "rain_yearly_stack = rain_yearly_stack.rename(\n",
        "    [f\"rain_{nm.split('_',1)[1]}\" for nm in rain_yr_names] # type: ignore\n",
        ")\n",
        "ndvi_yearly_stack = ndvi_yearly_stack.rename(\n",
        "    [f\"ndvi_{nm.split('_',1)[1]}\" for nm in ndvi_yr_names] # type: ignore\n",
        ")\n",
        "\n",
        "print(\"Yearly stacks:\", \n",
        "      rain_yearly_stack.bandNames().getInfo(),\n",
        "      ndvi_yearly_stack.bandNames().getInfo())\n",
        "\n",
        "# ──────────────────────────────────────────────────────────────────────────────\n",
        "# 5. Combine the yearly stacks into one Image with all rain & NDVI bands\n",
        "# ------------------------------------------------------------------------------\n",
        "ndvi_rain_yearly_stack = rain_yearly_stack.addBands(ndvi_yearly_stack)\n",
        "print(\"Combined yearly stack bands:\", \n",
        "      ndvi_rain_yearly_stack.bandNames().getInfo())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2c355780",
      "metadata": {},
      "outputs": [],
      "source": [
        "# EXPORT: Combined Annual NDVI & Rainfall Stack to GeoTIFF via geemap\n",
        "# ──────────────────────────────────────────────────────────────────────────────\n",
        "# 1. Define local file path for the output GeoTIFF\n",
        "ndvi_rain_out = 'data/ndvi_rain_yearly_2000_2020.tif'  # adjust filename/year range as needed\n",
        "# ------------------------------------------------------------------------------\n",
        "\n",
        "# ──────────────────────────────────────────────────────────────────────────────\n",
        "# 2. Use geemap’s ee_export_image to pull the EE Image to disk\n",
        "#    • First arg = the ee.Image to export (no `image=` keyword)\n",
        "#    • filename   = local path\n",
        "#    • region     = EE geometry to clip/export (our RSA boundary)\n",
        "#    • scale      = output pixel size in metres (here ~50 km for CHIRPS)\n",
        "#    • crs        = coordinate reference system of output\n",
        "#    • file_per_band=False ⇒ pack all bands into one multi‐band TIFF\n",
        "# ------------------------------------------------------------------------------\n",
        "geemap.ee_export_image(\n",
        "    ndvi_rain_yearly_stack,\n",
        "    filename      = ndvi_rain_out,\n",
        "    region        = south_africa.geometry(),\n",
        "    scale         = 50000,\n",
        "    crs           = \"EPSG:4326\",\n",
        "    file_per_band = False\n",
        ")\n",
        "\n",
        "# ──────────────────────────────────────────────────────────────────────────────\n",
        "# 3. Confirm successful export and report number of bands\n",
        "# ------------------------------------------------------------------------------\n",
        "print(f\"✓ Saved multi‐band GeoTIFF with {len(ndvi_rain_yearly_stack.bandNames().getInfo())} layers → {ndvi_rain_out}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fdc3e7be",
      "metadata": {
        "id": "fdc3e7be"
      },
      "source": [
        "---\n",
        "#### 7.\tLINK OCCURRENCE RECORDS TO ENVIRONMENTAL VARIABLES\n",
        "> This section focuses on enriching species occurrence data by linking it to relevant environmental variables at corresponding spatial locations. By sampling raster-based environmental layers (e.g. rainfall, vegetation indices) at the centroids of grid cells or observation points, we create a unified dataset that combines biodiversity and environmental information. This spatial join enables the analysis of how species patterns vary with underlying environmental gradients.\n",
        "> The process involves converting aggregated occurrence records into a spatial format (e.g. a `FeatureCollection`), and then extracting values from remote sensing-derived rasters (such as CHIRPS rainfall or MODIS NDVI) at each point location. The result is a structured table where each row represents a spatial unit, annotated with both the number of species observations and associated environmental values. This dataset forms the basis for subsequent modelling and interpretation of ecological processes.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "315826ea",
      "metadata": {
        "id": "315826ea"
      },
      "outputs": [],
      "source": [
        "# ──────────────────────────────────────────────────────────────────────────────\n",
        "# 1. DEFINE YOUR AOI & GRID POINTS\n",
        "#    • Convert study-area boundary & grid centroids to Earth Engine FeatureCollections\n",
        "# ------------------------------------------------------------------------------\n",
        "# 1a. Boundary as EE FeatureCollection\n",
        "boundary_fc = geemap.geopandas_to_ee(boundary_gdf)\n",
        "\n",
        "# 1b. Compute centroid coords for each grid cell (if not already points)\n",
        "grid_polys['lon'] = grid_polys.geometry.centroid.x\n",
        "grid_polys['lat'] = grid_polys.geometry.centroid.y\n",
        "\n",
        "# 1c. Convert grid GeoDataFrame to EE FeatureCollection, retaining key properties\n",
        "grid_fc = geemap.geopandas_to_ee(\n",
        "    grid_polys[['grid_id','lon','lat','spp_rich','obs_sum','geometry']],\n",
        "    geodesic=False\n",
        ")\n",
        "\n",
        "# ──────────────────────────────────────────────────────────────────────────────\n",
        "# 2. SAMPLE IMAGE COLLECTIONS AT GRID POINTS\n",
        "#    • rain_monthly_stack, ndvi_monthly_stack, ndvi_rain_yearly_stack\n",
        "#    • scale & tileScale tuned for ~5.6 km pixels\n",
        "# ------------------------------------------------------------------------------\n",
        "sample_rain = rain_monthly_stack.sampleRegions(\n",
        "    collection=grid_fc,\n",
        "    scale=5566,\n",
        "    tileScale=4,\n",
        "    geometries=False\n",
        ")\n",
        "\n",
        "sample_ndvi = ndvi_monthly_stack.sampleRegions(\n",
        "    collection=grid_fc,\n",
        "    scale=5566,\n",
        "    tileScale=4,\n",
        "    geometries=False\n",
        ")\n",
        "\n",
        "sample_ndvi_rain = ndvi_rain_yearly_stack.sampleRegions(\n",
        "    collection=grid_fc,\n",
        "    scale=5566,\n",
        "    tileScale=4,\n",
        "    geometries=False\n",
        ")\n",
        "\n",
        "# ──────────────────────────────────────────────────────────────────────────────\n",
        "# 3. INSPECT SAMPLED RESULTS\n",
        "#    • Print EE object types & first feature property names\n",
        "# ------------------------------------------------------------------------------\n",
        "print('sample_rain type:', sample_rain.name())\n",
        "print('rain properties:', sample_rain.first().propertyNames().getInfo())\n",
        "\n",
        "print('sample_ndvi type:', sample_ndvi.name())\n",
        "print('ndvi properties:', sample_ndvi.first().propertyNames().getInfo())\n",
        "\n",
        "print('sample_ndvi_rain type:', sample_ndvi_rain.name())\n",
        "print('ndvi_rain properties:', sample_ndvi_rain.first().propertyNames().getInfo())\n",
        "\n",
        "# ──────────────────────────────────────────────────────────────────────────────\n",
        "# 4. PREVIEW A FEW ROWS IN PANDAS\n",
        "# ------------------------------------------------------------------------------\n",
        "df_preview = geemap.ee_to_pandas(sample_rain.limit(5))\n",
        "print(df_preview)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "da36bb19",
      "metadata": {},
      "outputs": [],
      "source": [
        "# ──────────────────────────────────────────────────────────────────────────────\n",
        "# # EXPORT SAMPLED POINTS TO LOCAL CSV\n",
        "# • ee_object: the Earth Engine FeatureCollection of sampled values\n",
        "# • filename : local path where the CSV will be saved\n",
        "# ------------------------------------------------------------------------------\n",
        "geemap.ee_to_csv(\n",
        "    ee_object = sample_ndvi_rain,            # sampled NDVI+rain per grid cell\n",
        "    filename  = \"data/ndvi_rain_pts.csv\"      # output CSV file\n",
        ")\n",
        "print(\"✓ Exported sampled points to 'data/ndvi_rain_pts.csv'\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "db58723c",
      "metadata": {},
      "outputs": [],
      "source": [
        "# ──────────────────────────────────────────────────────────────────────────────\n",
        "# SAMPLE DISTANCE‐TO‐ROADS VALUES AT GRID POINTS\n",
        "#    • We use the previously defined `dist_to_roads` EE Image (distance band)\n",
        "#    • Sample at `grid_fc` centroids, producing a wide table keyed by grid_id\n",
        "# ------------------------------------------------------------------------------\n",
        "distRds_pts = dist_to_roads.sampleRegions(\n",
        "    collection   = grid_fc,   # EE FeatureCollection of grid centroids\n",
        "    scale        = 5000,      # ~5 km pixel resolution\n",
        "    tileScale    = 4,         # adjust to reduce computation per tile\n",
        "    geometries   = False      # omit geometry column for a flat table\n",
        ")\n",
        "# df_preview = geemap.ee_to_pandas(distRds_pts.limit(5))#,selectors=columns\n",
        "# print(df_preview)\n",
        "\n",
        "# ──────────────────────────────────────────────────────────────────────────────\n",
        "# EXPORT SAMPLED DISTANCE VALUES TO LOCAL CSV\n",
        "# • ee_object: the EE FeatureCollection of sampled distances\n",
        "# • filename : local path for the CSV\n",
        "# ------------------------------------------------------------------------------\n",
        "geemap.ee_to_csv(\n",
        "    ee_object = distRds_pts, \n",
        "    filename  = \"data/distRds_pts.csv\"\n",
        ")\n",
        "print(\"✓ Exported distance‐to‐roads samples to 'data/distRds_pts.csv'\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9d5e448f",
      "metadata": {},
      "outputs": [],
      "source": [
        "# # ──────────────────────────────────────────────────────────────────────────────\n",
        "# # SAMPLE ELEVATION VALUES AT GRID POINTS\n",
        "# #    • Use the clipped NASADEM elevation image (`elev`)\n",
        "# #    • Sample at each grid centroid in `grid_fc`\n",
        "# #    • Produces a wide table keyed by `grid_id` with elevation values\n",
        "# # ------------------------------------------------------------------------------\n",
        "# elev_pts = elev.sampleRegions(\n",
        "#     collection   = grid_fc,  # EE FeatureCollection of grid centroids\n",
        "#     scale        = 5000,     # pixel resolution in metres\n",
        "#     tileScale    = 4,        # tile subdivision for large computations\n",
        "#     geometries   = False     # exclude geometry from the output table\n",
        "# )\n",
        "\n",
        "# # ──────────────────────────────────────────────────────────────────────────────\n",
        "# # EXPORT SAMPLED ELEVATION TO LOCAL CSV\n",
        "# #    • ee_to_csv writes the FeatureCollection properties to CSV\n",
        "# # ------------------------------------------------------------------------------\n",
        "# geemap.ee_to_csv(\n",
        "#     ee_object = elev_pts,\n",
        "#     filename  = \"data/elev_pts.csv\"\n",
        "# )\n",
        "# print(\"✓ Exported elevation samples to 'data/elev_pts.csv'\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b3bd2795",
      "metadata": {
        "id": "b3bd2795"
      },
      "source": [
        "---\n",
        "#### 8. MODELLING RELATIONSHIPS\n",
        "> With our biodiversity and environmental data cubes in place, we now turn to explaining how and why patterns emerge. Four case studies illustrate a spectrum of modelling strategies matched to specific ecological questions and data structures:\n",
        "> \n",
        "> - **8.1 Predicting NDVI from Annual and Lagged Rainfall**: In this section we build a simple, interpretable regression model to quantify how both current-year and one-year-lag rainfall influence vegetation greenness (NDVI) across our grid cells. \n",
        "> - **8.2 Effect of Distance-to-Roads on Sampling Effort**: In this analysis we evaluate how accessibility—measured as distance from each grid‐cell centroid to the nearest major roads—influences sampling effort (total number of observations) across protected areas. We proceed in six main steps:\n",
        "> - **8.3 Key drivers of species diversity**: Gradient-Boosted Decision Trees (XGBoost) handles complex, non-additive effects of multiple environmental covariates on species richness; built-in feature importance and SHAP values support transparent driver ranking. Predictor stacks are sampled from GEE and modelled in Python (`xgboost`, `scikit-learn`).\n",
        "> \n",
        "> Across all case studies, you will learn to export predictor stacks from Earth Engine, fit and validate models in Python, quantify uncertainty through cross-validation or Bayesian posterior summaries, and return spatially explicit predictions to Earth Engine for interactive visualisation. By the end of this module, you will be able to select appropriate modelling frameworks, interpret key ecological drivers, and communicate results through reproducible, data-cube-based workflows.\n",
        "> \n",
        "> **https://geemap.org/notebooks/20_timeseries_inspector/#create-annual-composite-of-naip-imagery**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9b4f6962",
      "metadata": {
        "id": "9b4f6962"
      },
      "source": [
        "##### 8.1.\tPredicting NDVI from Annual and Lagged Rainfall\n",
        "In this section we build a simple, interpretable regression model to quantify how both current-year and one-year-lag rainfall influence vegetation greenness (NDVI) across our grid cells. \n",
        "\n",
        "1. **Aggregate to annual means**: For each cell and year, compute mean NDVI, mean rainfall, and mean one-year lag rainfall from the long-format table.\n",
        "2. **Drop incomplete records**: Remove the first observation per cell (where lagged rainfall is undefined).\n",
        "3. **Fit an OLS model**: Predict NDVI_mean using an intercept plus two predictors: rain_mean and rain_lag_mean.\n",
        "4. **Evaluate and report**: Report each coefficient with its p-value (and significance stars), overall 𝑅2 and RMSE to assess model fit.\n",
        "\n",
        "This straightforward linear approach lets us quantify the immediate (negative or positive) and delayed effects of precipitation on vegetation greenness, providing a clear baseline before exploring more flexible machine-learning or GAM methods.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7f5c2ef3",
      "metadata": {},
      "outputs": [],
      "source": [
        "# ──────────────────────────────────────────────────────────────────────────────\n",
        "# 1. Inspect the Earth Engine Image stack\n",
        "# ------------------------------------------------------------------------------\n",
        "print('Bands in ndvi_rain_yearly_stack:', ndvi_rain_yearly_stack.bandNames().getInfo())\n",
        "# Display detailed band metadata (name, id, etc.)\n",
        "print('ndvi_rain_yearly_stack image bands:', ndvi_rain_yearly_stack.getInfo()['bands'])  # type: ignore\n",
        "\n",
        "# ──────────────────────────────────────────────────────────────────────────────\n",
        "# 2. Load the exported CSV of sampled NDVI & rainfall points\n",
        "# ------------------------------------------------------------------------------\n",
        "df = pd.read_csv(\"data/ndvi_rain_pts.csv\")\n",
        "\n",
        "# ──────────────────────────────────────────────────────────────────────────────\n",
        "# 3. Identify and separate NDVI vs. Rain columns by prefix\n",
        "# ------------------------------------------------------------------------------\n",
        "ndvi_cols = [col for col in df.columns if col.startswith(\"ndvi_\")]\n",
        "rain_cols = [col for col in df.columns if col.startswith(\"rain_\")]\n",
        "\n",
        "# ──────────────────────────────────────────────────────────────────────────────\n",
        "# 4. Reshape to long format: one row per grid_id × year\n",
        "# ------------------------------------------------------------------------------\n",
        "# 4a. Melt NDVI columns\n",
        "df_ndvi = df.melt(\n",
        "    id_vars   = [\"grid_id\", \"lat\", \"lon\"],\n",
        "    value_vars= ndvi_cols,\n",
        "    var_name  = \"year\",\n",
        "    value_name= \"ndvi\"\n",
        ")\n",
        "# 4b. Melt Rain columns\n",
        "df_rain = df.melt(\n",
        "    id_vars   = [\"grid_id\", \"lat\", \"lon\"],\n",
        "    value_vars= rain_cols,\n",
        "    var_name  = \"year\",\n",
        "    value_name= \"rain\"\n",
        ")\n",
        "\n",
        "# ──────────────────────────────────────────────────────────────────────────────\n",
        "# 5. Clean the 'year' column by removing prefixes and converting to int\n",
        "# ------------------------------------------------------------------------------\n",
        "df_ndvi[\"year\"] = df_ndvi[\"year\"].str.replace(\"ndvi_\", \"\").astype(int)\n",
        "df_rain[\"year\"] = df_rain[\"year\"].str.replace(\"rain_\", \"\").astype(int)\n",
        "\n",
        "# ──────────────────────────────────────────────────────────────────────────────\n",
        "# 6. Merge NDVI and Rain long tables on grid_id, lat, lon, and year\n",
        "# ------------------------------------------------------------------------------\n",
        "df_long = pd.merge(\n",
        "    df_ndvi,\n",
        "    df_rain,\n",
        "    on=[\"grid_id\", \"lat\", \"lon\", \"year\"]\n",
        ")\n",
        "\n",
        "# ──────────────────────────────────────────────────────────────────────────────\n",
        "# 7. Inspect the combined long-format DataFrame\n",
        "# ------------------------------------------------------------------------------\n",
        "print(df_long.head())\n",
        "\n",
        "# (Optional) Write the long table to disk for downstream analysis\n",
        "# df_long.to_csv(\"data/ndvi_rain_pts_long.csv\", index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "79229b36",
      "metadata": {},
      "outputs": [],
      "source": [
        "# ──────────────────────────────────────────────────────────────────────────────\n",
        "# Quick hexbin plot of NDVI vs. Rainfall for all grid‐year points\n",
        "# • Uses hexagonal binning to efficiently visualize large point clouds\n",
        "# • ‘gridsize’ controls the number of bins per axis (higher ⇒ finer detail)\n",
        "# • ‘mincnt=1’ filters out empty bins for clarity\n",
        "# ------------------------------------------------------------------------------\n",
        "plt.figure(figsize=(6, 6))\n",
        "\n",
        "# Create the hexbin; returns a QuadMesh for which we can draw a colorbar\n",
        "hb = plt.hexbin(\n",
        "    df_long[\"rain\"],    # x: annual rainfall (mm)\n",
        "    df_long[\"ndvi\"],    # y: annual mean NDVI\n",
        "    gridsize=200,       # number of hexagons along x-axis\n",
        "    cmap=\"viridis_r\",   # inverted viridis colormap\n",
        "    mincnt=1            # only draw bins with count ≥ 1\n",
        ")\n",
        "\n",
        "# Add colorbar showing count per hexagon\n",
        "cb = plt.colorbar(hb)\n",
        "cb.set_label(\"Number of observations\")\n",
        "\n",
        "# Axis labels and title\n",
        "plt.xlabel(\"Annual Rainfall (mm)\")\n",
        "plt.ylabel(\"Annual Mean NDVI\")\n",
        "plt.title(\"Hexbin: NDVI vs. Rainfall\")\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "33bd4476",
      "metadata": {},
      "outputs": [],
      "source": [
        "# ──────────────────────────────────────────────────────────────────────────────\n",
        "# 1. Sort & prepare the DataFrame\n",
        "#    • Ensure data are ordered by grid cell and year before lagging\n",
        "# ------------------------------------------------------------------------------\n",
        "df_long = df_long.sort_values(['grid_id', 'year']).reset_index(drop=True)\n",
        "\n",
        "# 2. Create a one‐year lag of rainfall per grid cell\n",
        "#    • For each grid_id, shift the 'rain' series by one year\n",
        "#    • First year per cell will have NaN lag → excluded later\n",
        "df_long['rain_lag1'] = df_long.groupby('grid_id')['rain'].shift(1)\n",
        "# print(df_long[['grid_id', 'year', 'rain', 'rain_lag1']].head())\n",
        "\n",
        "# ──────────────────────────────────────────────────────────────────────────────\n",
        "# 3. Build predictor (exog) and response (endog) arrays for regression\n",
        "#    • X includes current‐year rain and one‐year lag\n",
        "#    • Mask out rows where lag is NaN (first record per grid)\n",
        "# ------------------------------------------------------------------------------\n",
        "X = df_long[['rain', 'rain_lag1']].values\n",
        "y = df_long['ndvi'].values\n",
        "\n",
        "mask = ~np.isnan(X).any(axis=1)\n",
        "X_valid = X[mask]\n",
        "y_valid = y[mask]\n",
        "\n",
        "# ──────────────────────────────────────────────────────────────────────────────\n",
        "# 4. Fit an Ordinary Least Squares (OLS) model with statsmodels\n",
        "#    • Add a constant term for the intercept\n",
        "#    • Use named columns to preserve parameter labels\n",
        "# ------------------------------------------------------------------------------\n",
        "exog_df = pd.DataFrame(X_valid, columns=['rain', 'rain_lag1'])\n",
        "exog_df = sm.add_constant(exog_df)  # adds 'const' column\n",
        "\n",
        "ols = sm.OLS(y_valid, exog_df).fit()\n",
        "\n",
        "# ──────────────────────────────────────────────────────────────────────────────\n",
        "# 5. Extract results: coefficients, p‐values, R², RMSE\n",
        "# ------------------------------------------------------------------------------\n",
        "params = ols.params\n",
        "pvals  = ols.pvalues\n",
        "\n",
        "# Helper for significance stars\n",
        "def stars(p):\n",
        "    if p < 0.001: return '***'\n",
        "    if p < 0.01:  return '**'\n",
        "    if p < 0.05:  return '*'\n",
        "    return ''\n",
        "\n",
        "print(\"Regression results (with p-values):\")\n",
        "\n",
        "# Intercept\n",
        "ci, pi = params['const'], pvals['const']\n",
        "print(f\"  • {'intercept':12s} = {ci:8.4f}   p = {pi:.3f} {stars(pi)}\")\n",
        "\n",
        "# Slopes for rain and rain_lag1\n",
        "for name in ['rain', 'rain_lag1']:\n",
        "    slope, p = params[name], pvals[name]\n",
        "    print(f\"  • {name:12s} slope = {slope:8.4f}   p = {p:.3f} {stars(p)}\")\n",
        "\n",
        "# Model performance metrics\n",
        "r2   = ols.rsquared\n",
        "rmse = np.sqrt(mean_squared_error(y_valid, ols.fittedvalues))\n",
        "print(f\"  • R²   = {r2:.4f}\")\n",
        "print(f\"  • RMSE = {rmse:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8086ec17",
      "metadata": {},
      "outputs": [],
      "source": [
        "# ──────────────────────────────────────────────────────────────────────────────\n",
        "# 0. AGGREGATE TO ANNUAL MEANS per grid cell\n",
        "#    • For each grid_id & year, compute mean NDVI, mean rain, and mean lag‐rain\n",
        "# ------------------------------------------------------------------------------\n",
        "stats_df = (\n",
        "    df_long\n",
        "      .groupby(['grid_id', 'year'], as_index=False)\n",
        "      .agg(\n",
        "          ndvi_mean     = ('ndvi',      'mean'),\n",
        "          rain_mean     = ('rain',      'mean'),\n",
        "          rain_lag_mean = ('rain_lag1', 'mean')\n",
        "      )\n",
        ")\n",
        "# print(\"Annual summary (first rows):\")\n",
        "# print(stats_df.head())\n",
        "\n",
        "# ──────────────────────────────────────────────────────────────────────────────\n",
        "# 1. DROP YEARS WHERE LAG IS MISSING (first record per grid)\n",
        "# ------------------------------------------------------------------------------\n",
        "stats_df_noNA = stats_df.dropna(subset=['rain_lag_mean']).reset_index(drop=True)\n",
        "# print(\"After dropping missing lag (first rows):\")\n",
        "# print(stats_df_noNA.head())\n",
        "\n",
        "# ──────────────────────────────────────────────────────────────────────────────\n",
        "# 2. PREPARE DATA FOR OLS REGRESSION\n",
        "#    • Predict NDVI from current‐year rain and one‐year lag\n",
        "# ------------------------------------------------------------------------------\n",
        "X = stats_df_noNA[['rain_mean', 'rain_lag_mean']]\n",
        "y = stats_df_noNA['ndvi_mean']\n",
        "\n",
        "# Add intercept term\n",
        "X = sm.add_constant(X)  # adds 'const' column\n",
        "\n",
        "# ──────────────────────────────────────────────────────────────────────────────\n",
        "# 3. FIT THE OLS MODEL\n",
        "# ------------------------------------------------------------------------------\n",
        "ols = sm.OLS(y, X).fit()\n",
        "\n",
        "# ──────────────────────────────────────────────────────────────────────────────\n",
        "# 4. EXTRACT RESULTS: parameters, p‐values, R², RMSE\n",
        "# ------------------------------------------------------------------------------\n",
        "params = ols.params\n",
        "pvals  = ols.pvalues\n",
        "fitted = ols.fittedvalues\n",
        "\n",
        "# Helper to format significance stars\n",
        "def stars(p):\n",
        "    return '***' if p < 0.001 else '**' if p < 0.01 else '*' if p < 0.05 else ''\n",
        "\n",
        "print(\"\\nRegression results (with p-values):\")\n",
        "# Intercept\n",
        "ci, pi = params['const'], pvals['const']\n",
        "print(f\"  • {'intercept':12s} = {ci:8.4f}   p = {pi:.3f} {stars(pi)}\")\n",
        "\n",
        "# Slopes\n",
        "for name in ['rain_mean', 'rain_lag_mean']:\n",
        "    slope, p = params[name], pvals[name]\n",
        "    print(f\"  • {name:12s} slope = {slope:8.4f}   p = {p:.3f} {stars(p)}\")\n",
        "\n",
        "# Performance metrics\n",
        "r2   = ols.rsquared\n",
        "rmse = np.sqrt(mean_squared_error(y, fitted))\n",
        "print(f\"  • R²   = {r2:.4f}\")\n",
        "print(f\"  • RMSE = {rmse:.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6087f0fb",
      "metadata": {},
      "outputs": [],
      "source": [
        "# ──────────────────────────────────────────────────────────────────────────────\n",
        "# Scatter plot of annual mean NDVI vs. rainfall with linear trend line\n",
        "# ------------------------------------------------------------------------------\n",
        "plt.figure(figsize=(7, 5))\n",
        "\n",
        "# 1. Scatter the data points\n",
        "plt.scatter(\n",
        "    stats_df_noNA['rain_mean'],      # x-axis: mean annual rainfall\n",
        "    stats_df_noNA['ndvi_mean'],      # y-axis: mean annual NDVI\n",
        "    color='lightblue',\n",
        "    s=10,                             # marker size\n",
        "    alpha=0.4,                        # transparency\n",
        "    label=f'NDVI vs Rain (R² = {r2:.2f})'\n",
        ")\n",
        "\n",
        "# 2. Fit a simple linear model y = m*x + b\n",
        "#    • Use numpy.polyfit to get slope (m) and intercept (b)\n",
        "x_vals = stats_df_noNA['rain_mean'].values\n",
        "y_vals = stats_df_noNA['ndvi_mean'].values\n",
        "m, b = np.polyfit(x_vals, y_vals, 1) # type: ignore\n",
        "\n",
        "# 3. Generate line endpoints over the x-range\n",
        "xs = np.array([x_vals.min(), x_vals.max()]) # type: ignore\n",
        "ys = m * xs + b\n",
        "\n",
        "# 4. Plot the fitted line\n",
        "plt.plot(\n",
        "    xs, ys,\n",
        "    color='blue',\n",
        "    linewidth=2,\n",
        "    label=f'y = {m:.3f}x + {b:.1f}'\n",
        ")\n",
        "\n",
        "# 5. Label axes and add legend\n",
        "plt.xlabel('Mean Annual Rainfall (mm)')\n",
        "plt.ylabel('Mean Annual NDVI')\n",
        "plt.legend(frameon=False)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a47db98d",
      "metadata": {
        "id": "a47db98d"
      },
      "source": [
        "##### 8.2.\tEffect of Distance-to-Roads on Sampling Effort\n",
        "In this analysis we evaluate how accessibility—measured as distance from each grid‐cell centroid to the nearest major roads—influences sampling effort (total number of observations) across protected areas. We proceed in six main steps:\n",
        "\n",
        "1. **Prepare the predictor raster**: We computed three Euclidean‐distance layers (dist_type1, dist_type2, dist_type3) corresponding to highways, primary, and secondary roads, then stacked them into a single multi‐band image (dist_stack).\n",
        "2. **Assemble the response data**: From our earlier species‐grid summary, we built a GeoDataFrame (gdf_pts) of cell centroids with their total observation counts (sum_cnt) and square‐root transformation (sqrt_sum_cnt) to stabilize variance.\n",
        "3. **Sample distances at each centroid**: Using sampleRegions(), we extracted the three distance bands at each centroid and converted the result to a pandas DataFrame (dist_df).\n",
        "4. **Derive a single predictor**: We calculated the mean distance across road types (dist_mean) while retaining the individual distances for Types 1–3.\n",
        "5. **Aggregate multi‐year effort into one raster**: We summed our multi‐band sampling‐effort GeoTIFF across time to produce a single “total effort” raster, which validates our per‐cell counts.\n",
        "6. **Fit a linear regression**: Finally, we modeled the square‐root of total observations (√obs_sum) as a function of mean distance and the three individual road‐type distances:\n",
        "`obs_sum ∼ 𝛽0 + 𝛽1 dist_mean + 𝛽2 dist_type1 + 𝛽3 dist_type2 + 𝛽4 dist_type3`   \n",
        "We report each coefficient, its p-value (with significance stars), the model 𝑅2, and the RMSE to assess fit.\n",
        "\n",
        "By framing distance‐to‐roads as our key accessibility predictor and using a square‐root transform on counts (rather than a full GAM), we provide a transparent baseline for understanding the strength and direction of sampling‐effort gradients relative to transport infrastructure.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "437618fd",
      "metadata": {
        "id": "437618fd"
      },
      "outputs": [],
      "source": [
        "# ──────────────────────────────────────────────────────────────────────────────\n",
        "# 1. Inspect the multi‐band distance‐to‐roads image\n",
        "# ------------------------------------------------------------------------------\n",
        "# print(\"Type dist_stack:\", dist_stack.name())\n",
        "# print(\"Bands in dist_stack:\", dist_stack.bandNames().getInfo())\n",
        "\n",
        "# ──────────────────────────────────────────────────────────────────────────────\n",
        "# 2. Prepare point FeatureCollection of grid centroids with effort & richness\n",
        "#    • gdf_pts is a GeoDataFrame with 'grid_id','centroid_lon','centroid_lat',\n",
        "#      'sum_cnt','sqrt_sum_cnt','sum_rich','sqrt_sum_rich'\n",
        "# ------------------------------------------------------------------------------\n",
        "pts_fc = geemap.geopandas_to_ee(\n",
        "    gdf_pts[['grid_id','centroid_lon','centroid_lat', # type: ignore\n",
        "             'sum_cnt','sqrt_sum_cnt','sum_rich','sqrt_sum_rich','geometry']],\n",
        "    geodesic=False\n",
        ")\n",
        "\n",
        "# ──────────────────────────────────────────────────────────────────────────────\n",
        "# 3. Sample distance‐to‐roads bands at each centroid\n",
        "#    • yields dist_type1, dist_type2, dist_type3 per grid_id\n",
        "# ------------------------------------------------------------------------------\n",
        "sample_dist = dist_stack.sampleRegions(\n",
        "    collection = pts_fc,\n",
        "    scale      = 5000,\n",
        "    tileScale  = 4,\n",
        "    geometries = False\n",
        ")\n",
        "\n",
        "# ──────────────────────────────────────────────────────────────────────────────\n",
        "# 4. Convert to pandas DataFrame and compute mean distance\n",
        "# ------------------------------------------------------------------------------\n",
        "dist_df = geemap.ee_to_pandas(sample_dist)\n",
        "# Compute average distance across types 1–3\n",
        "dist_df['dist_mean'] = dist_df[['dist_type1','dist_type2','dist_type3']].mean(axis=1) # type: ignore\n",
        "# print(dist_df.head())\n",
        "\n",
        "# ──────────────────────────────────────────────────────────────────────────────\n",
        "# 5. Sum multi‐band sampling‐effort GeoTIFF into a single‐band total\n",
        "# ------------------------------------------------------------------------------\n",
        "obs_tif = 'data/sampling_effort_1998_2023_masked_v2.tif'\n",
        "sum_tif = 'data/sampling_effort_total_1998_2023.tif'\n",
        "\n",
        "with rasterio.open(obs_tif) as src:\n",
        "    meta = src.meta.copy()\n",
        "    arr  = src.read()                  # (bands, rows, cols)\n",
        "\n",
        "total_effort = np.nansum(arr, axis=0)  # sum over bands → (rows, cols)\n",
        "meta.update({'count': 1, 'dtype': total_effort.dtype})\n",
        "\n",
        "with rasterio.open(sum_tif, 'w', **meta) as dst:\n",
        "    dst.write(total_effort, 1)\n",
        "\n",
        "print(f\"✓ Wrote summed effort raster → {sum_tif}\")\n",
        "\n",
        "# ──────────────────────────────────────────────────────────────────────────────\n",
        "# 6. Fit OLS regression: √(obs_sum) ~ dist_mean + dist_type1 + dist_type2 + dist_type3\n",
        "# ------------------------------------------------------------------------------\n",
        "# Prepare X and y from dist_df\n",
        "X = dist_df[['dist_mean','dist_type1','dist_type2','dist_type3']]\n",
        "y = dist_df['sqrt_sum_cnt']  # sqrt of total obs per cell\n",
        "\n",
        "# Add intercept and fit model\n",
        "X = sm.add_constant(X)\n",
        "ols = sm.OLS(y, X).fit()\n",
        "\n",
        "# Extract results\n",
        "params = ols.params\n",
        "pvals  = ols.pvalues\n",
        "r2     = ols.rsquared\n",
        "rmse   = np.sqrt(mean_squared_error(y, ols.fittedvalues))\n",
        "\n",
        "# Significance stars helper\n",
        "def stars(p):\n",
        "    return '***' if p<0.001 else '**' if p<0.01 else '*' if p<0.05 else ''\n",
        "\n",
        "# Report\n",
        "print(\"\\nRegression results (with p-values):\")\n",
        "print(f\"  • {'intercept':12s} = {params['const']:8.4f}   p = {pvals['const']:.3f} {stars(pvals['const'])}\")\n",
        "for name in ['dist_mean','dist_type1','dist_type2','dist_type3']:\n",
        "    print(f\"  • {name:12s} slope = {params[name]:8.4f}   p = {pvals[name]:.3f} {stars(pvals[name])}\")\n",
        "print(f\"  • R²   = {r2:.4f}\")\n",
        "print(f\"  • RMSE = {rmse:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "350130b0",
      "metadata": {},
      "outputs": [],
      "source": [
        "# ──────────────────────────────────────────────────────────────────────────────\n",
        "# 1. BUILD A FINER GRID AND SUMMARIZE OBSERVATIONS\n",
        "#    • res=0.25° (~28 km) grid in EPSG:4326\n",
        "# ------------------------------------------------------------------------------\n",
        "grid_counts, grid_polys = species_grid_summary(\n",
        "    df_url,\n",
        "    res=0.25,\n",
        "    unit=\"deg\"\n",
        ")\n",
        "\n",
        "# ──────────────────────────────────────────────────────────────────────────────\n",
        "# 2. AGGREGATE PER-CELL TOTALS & RICHNESS\n",
        "#    • sum_cnt: total observations per cell\n",
        "#    • sum_rich: unique species per cell\n",
        "# ------------------------------------------------------------------------------\n",
        "cent_df = (\n",
        "    grid_counts\n",
        "      .groupby(['grid_id','centroid_lat','centroid_lon'], as_index=False)\n",
        "      .agg(\n",
        "          sum_cnt  = ('count',   'sum'),\n",
        "          sum_rich = ('species', 'nunique')\n",
        "      )\n",
        ")\n",
        "\n",
        "# Square‐root transform for normality\n",
        "cent_df['sqrt_sum_cnt']  = np.sqrt(cent_df['sum_cnt'])\n",
        "cent_df['sqrt_sum_rich'] = np.sqrt(cent_df['sum_rich'])\n",
        "\n",
        "# ──────────────────────────────────────────────────────────────────────────────\n",
        "# 3. CONVERT CELL CENTROIDS TO EE POINTS\n",
        "#    • Retain key metrics for sampling\n",
        "# ------------------------------------------------------------------------------\n",
        "crs_wgs84 = \"EPSG:4326\"\n",
        "gdf_pts = gpd.GeoDataFrame(\n",
        "    cent_df,\n",
        "    geometry=gpd.points_from_xy(cent_df['centroid_lon'], cent_df['centroid_lat']),\n",
        "    crs=crs_wgs84\n",
        ")\n",
        "\n",
        "pts_fc = geemap.geopandas_to_ee(\n",
        "    gdf_pts[['grid_id','centroid_lon','centroid_lat',\n",
        "             'sum_cnt','sqrt_sum_cnt','sum_rich','sqrt_sum_rich','geometry']],\n",
        "    geodesic=False\n",
        ")\n",
        "\n",
        "# ──────────────────────────────────────────────────────────────────────────────\n",
        "# 4. SAMPLE DISTANCE‐TO‐ROADS AT EACH CENTROID\n",
        "#    • dist_stack has bands 'dist_type1','dist_type2','dist_type3'\n",
        "# ------------------------------------------------------------------------------\n",
        "sample_dist = dist_stack.sampleRegions(\n",
        "    collection=pts_fc,\n",
        "    scale=5000,\n",
        "    tileScale=4,\n",
        "    geometries=False\n",
        ")\n",
        "dist_df = geemap.ee_to_pandas(sample_dist)\n",
        "\n",
        "# Compute mean distance across the three road types\n",
        "dist_df['dist_mean'] = dist_df[['dist_type1','dist_type2','dist_type3']].mean(axis=1) # type: ignore\n",
        "\n",
        "# ──────────────────────────────────────────────────────────────────────────────\n",
        "# 5. FIT OLS: √(obs_sum) ~ dist_mean + dist_type1 + dist_type2 + dist_type3\n",
        "# ------------------------------------------------------------------------------\n",
        "X = dist_df[['dist_mean','dist_type1','dist_type2','dist_type3']]\n",
        "y = dist_df['sqrt_sum_cnt']\n",
        "\n",
        "# Add intercept\n",
        "X = sm.add_constant(X)\n",
        "ols = sm.OLS(y, X).fit()\n",
        "\n",
        "# Extract results\n",
        "params = ols.params\n",
        "pvals  = ols.pvalues\n",
        "r2     = ols.rsquared\n",
        "rmse   = np.sqrt(mean_squared_error(y, ols.fittedvalues))\n",
        "\n",
        "# Helper for significance stars\n",
        "def stars(p):\n",
        "    return '***' if p<0.001 else '**' if p<0.01 else '*' if p<0.05 else ''\n",
        "\n",
        "# ──────────────────────────────────────────────────────────────────────────────\n",
        "# 6. REPORT REGRESSION OUTPUT\n",
        "# ------------------------------------------------------------------------------\n",
        "print(\"Regression results (√obs_sum ~ distances):\\n\")\n",
        "print(f\"  • {'intercept':12s} = {params['const']:8.4f}   p = {pvals['const']:.3f} {stars(pvals['const'])}\")\n",
        "for name in ['dist_mean','dist_type1','dist_type2','dist_type3']:\n",
        "    print(f\"  • {name:12s} = {params[name]:8.4f}   p = {pvals[name]:.3f} {stars(pvals[name])}\")\n",
        "print(f\"\\n  • R²   = {r2:.4f}\")\n",
        "print(f\"  • RMSE = {rmse:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "87a29a89",
      "metadata": {},
      "source": [
        "-  **Overall fit is weak (R²≈0.05)**. Only about 5% of the variation in (√) sampling effort is explained by these distance measures—so there are lots of other factors at play.\n",
        "-  **Intercept ≈7.7 (p < 0.001)**. When all distances are zero (i.e. right on top of every road), the model’s baseline prediction of √(obs_sum) is about 7.7. Squaring that gives roughly 60 samples in a cell.\n",
        "-  **Average distance to roads** (dist_mean): slope ≈–0.0481 (p < 0.001).* For every 1-unit increase in the average distance from roads, √(sampling effort) drops by ~0.048. In other words, the farther you are, the fewer samples you tend to find.\n",
        "-  **Distance to Type 1 (major highways)**: slope ≈–0.0227 (p = 0.005). Being farther from big highways also reduces sampling, but only by about half as much per unit distance as the overall average effect.\n",
        "-  **Distance to Type 2 (primary roads)**: slope ≈–0.0141 (p = 0.41). This effect is small and statistically not different from zero—so distance to primary roads doesn’t seem to matter for how many samples get taken.\n",
        "-  **Distance to Type 3 (secondary roads)**: slope ≈–0.1075 (p = 0.008). This is the strongest individual effect: farther from secondary roads, sampling effort declines most steeply.\n",
        "-  **RMSE ≈7.0** (on the √ scale). The typical prediction error is about 7 in √(obs_sum) units, which translates to quite a wide spread once you square it back to counts.\n",
        "\n",
        "In other words, you take fewer biodiversity samples the further you get from roads—especially from the smaller, secondary roads. Major highways also help a bit. But these distances together explain only a small slice (~5%) of why some places get more sampling than others. There’s a lot more influencing where and how much people sample beyond just road proximity."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "51ca803d",
      "metadata": {
        "id": "51ca803d"
      },
      "source": [
        "##### 8.3.\tKey drivers of species diversity\n",
        "Gradient-Boosted Decision Trees (XGBoost) handles complex, non-additive effects of multiple environmental covariates on species richness; built-in feature importance and SHAP values support transparent driver ranking. Predictor stacks are sampled from GEE and modelled in Python (`xgboost`, `scikit-learn`).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "af84576d",
      "metadata": {
        "id": "af84576d"
      },
      "outputs": [],
      "source": [
        "# ──────────────────────────────────────────────────────────────────────────────\n",
        "# SAMPLE Bioclimatic Variables at Grid Centroids\n",
        "#    • bioclim: EE Image with WORLDCLIM/V1 BIO01–BIO19 bands\n",
        "#    • pts_fc : EE FeatureCollection of grid-cell centroids with metadata\n",
        "#    • Produces a wide table keyed by grid_id, with bio01–bio19 values\n",
        "# ------------------------------------------------------------------------------\n",
        "pts_bioclim = bioclim.sampleRegions(\n",
        "    collection   = pts_fc,   # grid centroids\n",
        "    scale        = 5000,     # sampling resolution in metres\n",
        "    tileScale    = 4,        # controls per‐tile complexity\n",
        "    geometries   = False     # omit geometry field in output\n",
        ")\n",
        "\n",
        "# Inspect the EE object name\n",
        "print('Type pts_bioclim:', pts_bioclim.name())\n",
        "\n",
        "# Convert the sampled FeatureCollection to a pandas DataFrame\n",
        "bioclim_df = geemap.ee_to_pandas(pts_bioclim)\n",
        "\n",
        "# Preview the first few rows\n",
        "print(bioclim_df.head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2dc69f6a",
      "metadata": {},
      "outputs": [],
      "source": [
        "# ──────────────────────────────────────────────────────────────────────────────\n",
        "# 1. PREPARE FEATURES & TARGET\n",
        "#    • Predictors: bio01–bio19\n",
        "#    • Response : sqrt_sum_rich (√ species richness per cell)\n",
        "# ------------------------------------------------------------------------------\n",
        "bio_cols = [f\"bio{str(i).zfill(2)}\" for i in range(1, 20)]\n",
        "X = bioclim_df[bio_cols]\n",
        "y = bioclim_df[\"sqrt_sum_rich\"]\n",
        "\n",
        "# ──────────────────────────────────────────────────────────────────────────────\n",
        "# 2. TRAIN/TEST SPLIT\n",
        "#    • 80% training, 20% testing\n",
        "#    • random_state for reproducibility\n",
        "# ------------------------------------------------------------------------------\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# ──────────────────────────────────────────────────────────────────────────────\n",
        "# 3. FIT XGBOOST REGRESSOR\n",
        "#    • 500 trees, max depth 4\n",
        "#    • learning_rate controls shrinkage\n",
        "#    • subsample & colsample_bytree for regularization\n",
        "# ------------------------------------------------------------------------------\n",
        "model = xgb.XGBRegressor(\n",
        "    n_estimators=500,\n",
        "    max_depth=4,\n",
        "    learning_rate=0.05,\n",
        "    subsample=0.8,\n",
        "    colsample_bytree=0.8,\n",
        "    random_state=42,\n",
        "    verbosity=0\n",
        ")\n",
        "model.fit(\n",
        "    X_train, y_train,\n",
        "    eval_set=[(X_test, y_test)],\n",
        "    verbose=False\n",
        ")\n",
        "\n",
        "# ──────────────────────────────────────────────────────────────────────────────\n",
        "# 4. EVALUATE PERFORMANCE\n",
        "#    • Predict on hold‐out set\n",
        "#    • Compute R² and RMSE\n",
        "# ------------------------------------------------------------------------------\n",
        "y_pred = model.predict(X_test)\n",
        "r2      = r2_score(y_test, y_pred)\n",
        "rmse    = np.sqrt(mean_squared_error(y_test, y_pred))\n",
        "print(f\"XGBoost performance: R² = {r2:.3f}, RMSE = {rmse:.3f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "10bc1ed2",
      "metadata": {},
      "outputs": [],
      "source": [
        "# ──────────────────────────────────────────────────────────────────────────────\n",
        "# 1. Compute permutation importances on the test set\n",
        "#    • Measures the decrease in R² when each feature is randomly shuffled\n",
        "#    • n_repeats controls stability of estimates\n",
        "# ------------------------------------------------------------------------------\n",
        "results = permutation_importance(\n",
        "    estimator    = model,      # trained XGBoost model\n",
        "    X            = X_test,     # features of the test set\n",
        "    y            = y_test,     # observed responses\n",
        "    n_repeats    = 10,\n",
        "    random_state = 42,\n",
        "    n_jobs       = -1\n",
        ")\n",
        "\n",
        "# ──────────────────────────────────────────────────────────────────────────────\n",
        "# 2. Convert to a pandas Series and sort descending\n",
        "#    • results.importances_mean is the average R² drop per feature\n",
        "# ------------------------------------------------------------------------------\n",
        "perm_imp = pd.Series(\n",
        "    results.importances_mean,  # type: ignore\n",
        "    index = bio_cols\n",
        ").sort_values(ascending=False)\n",
        "\n",
        "# ──────────────────────────────────────────────────────────────────────────────\n",
        "# 3. Display the top 10 predictor variables\n",
        "# ------------------------------------------------------------------------------\n",
        "# print(\"Top 10 drivers by permutation importance:\\n\", perm_imp.head(10))\n",
        "\n",
        "# ──────────────────────────────────────────────────────────────────────────────\n",
        "# 4. Visualize with a horizontal bar plot\n",
        "# ------------------------------------------------------------------------------\n",
        "plt.figure(figsize=(6, 4))\n",
        "perm_imp.head(10).plot.barh()\n",
        "plt.gca().invert_yaxis()  # highest importance on top\n",
        "plt.xlabel(\"Mean drop in R² after permutation\")\n",
        "plt.title(\"Top 10 Drivers of Species Richness\")\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c2676ee9",
      "metadata": {
        "id": "c2676ee9"
      },
      "source": [
        "---\n",
        "#### 9.\tINTERPRET RESULTS AND MAP PREDICTIONS\n",
        "> The final step translates model outputs into ecological insight and actionable spatial products. Begin by examining goodness-of-fit diagnostics—cross-validated RMSE, deviance, or posterior predictive checks—to judge model credibility. Next, interrogate driver importance and response curves: feature importance scores, SHAP values, or partial-dependence plots reveal how each environmental covariate shapes biodiversity patterns.\n",
        "> Predicted surfaces and their uncertainties are then exported back to Google Earth Engine and rendered with geemap or folium. Interactive layers allow users to toggle between baseline predictions, scenario maps, and confidence intervals, facilitating rapid exploration of hotspots, refugia, or data-poor areas. Complex temporal outputs (e.g. yearly richness trends) can be animated to show change through time, while static high-resolution maps are generated for reports and policy briefs.\n",
        "> Finally, link these visualisations to conservation objectives—such as identifying priority sampling gaps, forecasting climate-driven shifts, or evaluating management interventions—ensuring that data-cube analyses inform real-world biodiversity decisions.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9a8191e2",
      "metadata": {},
      "outputs": [],
      "source": [
        "# ──────────────────────────────────────────────────────────────────────────────\n",
        "# 0. AGGREGATE TO ANNUAL MEANS per grid cell\n",
        "#    • For each grid_id & year, compute mean NDVI, mean rain, and mean lag‐rain\n",
        "# ------------------------------------------------------------------------------\n",
        "stats_df = (\n",
        "    df_long\n",
        "      .groupby(['grid_id', 'year'], as_index=False)\n",
        "      .agg(\n",
        "          ndvi_mean     = ('ndvi',      'mean'),\n",
        "          rain_mean     = ('rain',      'mean'),\n",
        "          rain_lag_mean = ('rain_lag1', 'mean')\n",
        "      )\n",
        ")\n",
        "# print(\"Annual summary (first rows):\")\n",
        "# print(stats_df.head())\n",
        "\n",
        "# ──────────────────────────────────────────────────────────────────────────────\n",
        "# 1. DROP YEARS WHERE LAG IS MISSING (first record per grid)\n",
        "# ------------------------------------------------------------------------------\n",
        "stats_df_noNA = stats_df.dropna(subset=['rain_lag_mean']).reset_index(drop=True)\n",
        "# print(\"After dropping missing lag (first rows):\")\n",
        "# print(stats_df_noNA.head())\n",
        "\n",
        "# ──────────────────────────────────────────────────────────────────────────────\n",
        "# 2. PREPARE DATA FOR OLS REGRESSION\n",
        "#    • Predict NDVI from current‐year rain and one‐year lag\n",
        "# ------------------------------------------------------------------------------\n",
        "X = stats_df_noNA[['rain_mean', 'rain_lag_mean']]\n",
        "y = stats_df_noNA['ndvi_mean']\n",
        "\n",
        "# Add intercept term\n",
        "X = sm.add_constant(X)  # adds 'const' column\n",
        "\n",
        "# ──────────────────────────────────────────────────────────────────────────────\n",
        "# 3. FIT THE OLS MODEL\n",
        "# ------------------------------------------------------------------------------\n",
        "ols = sm.OLS(y, X).fit()\n",
        "\n",
        "# ──────────────────────────────────────────────────────────────────────────────\n",
        "# 4. EXTRACT RESULTS: parameters, p‐values, R², RMSE\n",
        "# ------------------------------------------------------------------------------\n",
        "params = ols.params\n",
        "pvals  = ols.pvalues\n",
        "fitted = ols.fittedvalues\n",
        "\n",
        "# Helper to format significance stars\n",
        "def stars(p):\n",
        "    return '***' if p < 0.001 else '**' if p < 0.01 else '*' if p < 0.05 else ''\n",
        "\n",
        "print(\"\\nRegression results (with p-values):\")\n",
        "# Intercept\n",
        "ci, pi = params['const'], pvals['const']\n",
        "print(f\"  • {'intercept':12s} = {ci:8.4f}   p = {pi:.3f} {stars(pi)}\")\n",
        "\n",
        "# Slopes\n",
        "for name in ['rain_mean', 'rain_lag_mean']:\n",
        "    slope, p = params[name], pvals[name]\n",
        "    print(f\"  • {name:12s} slope = {slope:8.4f}   p = {p:.3f} {stars(p)}\")\n",
        "\n",
        "# Performance metrics\n",
        "r2   = ols.rsquared\n",
        "rmse = np.sqrt(mean_squared_error(y, fitted))\n",
        "print(f\"  • R²   = {r2:.4f}\")\n",
        "print(f\"  • RMSE = {rmse:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b74a243e",
      "metadata": {},
      "source": [
        "-  **Intercept ≈ 1200**: This is the model’s “baseline” NDVI value when both current-year and last-year rainfall are zero. In reality we never see zero rainfall everywhere, but it anchors the line.\n",
        "-  **Current-year rain slope ≈ –17 (p < 0.001)**: For every additional millimeter of rain this year, the model predicts NDVI will drop by about 17 units, all else being equal. That negative sign may seem counter-intuitive, but it often reflects timing or measurement issues—for example, very heavy rain can sometimes lower end-of-year greenness if clouds obscure satellite sensors, or if the “growing flush” happens later.\n",
        "-  **Last-year (lagged) rain slope ≈ +22 (p < 0.001**): Conversely, every extra millimeter of rain last year is associated with an increase of about 22 NDVI units today. This suggests that wetter conditions in the previous year tend to leave behind healthier or more robust vegetation that shows up in the current NDVI measurements.\n",
        "-  **R² ≈ 0.74**: The model explains about 74 % of the variation in NDVI across your grid cells and years. In ecological data that’s quite high, meaning these two rainfall terms capture most of the NDVI signal.\n",
        "-  **RMSE ≈ 753**: On average, the model’s predictions are off by about 753 NDVI units. Given that MODIS NDVI ranges roughly 0–10 000, this is a moderate level of error—enough that you’d still see some scatter around the fitted surface, but not so large as to make the model unusable.\n",
        "\n",
        "In other words, Vegetation greenness this year tends to be lower when rainfall this same year is especially high, but it’s higher when the previous year was wet. Together, these two effects explain almost three-quarters of the variation in greenness we see across space and time, although individual predictions can still be off by a few hundred NDVI units."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "13f70c48",
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"Rain monthly bands:\", ndvi.first().bandNames().getInfo())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8bf4d346",
      "metadata": {
        "id": "8bf4d346"
      },
      "outputs": [],
      "source": [
        "# ──────────────────────────────────────────────────────────────────────────────\n",
        "# 1. Fit a simple linear regression: NDVI ~ rain (no lag)\n",
        "#    • X: mean annual rainfall per grid cell\n",
        "#    • y: mean annual NDVI per grid cell\n",
        "# ------------------------------------------------------------------------------\n",
        "X = stats_df_noNA['rain_mean'].values.reshape(-1, 1)  # feature array # type: ignore\n",
        "y = stats_df_noNA['ndvi_mean'].values                 # target array\n",
        "\n",
        "model     = LinearRegression().fit(X, y) # type: ignore\n",
        "slope     = model.coef_[0]\n",
        "intercept = model.intercept_\n",
        "r2        = r2_score(y, model.predict(X)) # type: ignore\n",
        "\n",
        "print(f\"Fitted model: NDVI = {slope:.4f}·rain + {intercept:.4f}   (R² = {r2:.3f})\")\n",
        "\n",
        "# ──────────────────────────────────────────────────────────────────────────────\n",
        "# 2. Convert slope & intercept to Earth Engine Numbers\n",
        "# ------------------------------------------------------------------------------\n",
        "s_ee  = ee.Number(slope) # type: ignore\n",
        "b0_ee = ee.Number(intercept) # type: ignore\n",
        "\n",
        "# ──────────────────────────────────────────────────────────────────────────────\n",
        "# 3. Select one year's rainfall image from the stacked EE Image\n",
        "#    • Here we pick the band 'rain_2008' as an example\n",
        "# ------------------------------------------------------------------------------\n",
        "rain_img = ndvi_rain_yearly_stack.select('rain_2008')\n",
        "\n",
        "# ──────────────────────────────────────────────────────────────────────────────\n",
        "# 4. Apply the linear model in EE: NDVI_pred = slope * rain + intercept\n",
        "#    • Rename the output band to 'ndvi_pred'\n",
        "# ------------------------------------------------------------------------------\n",
        "ndvi_pred = (\n",
        "    rain_img\n",
        "      .multiply(s_ee)    # slope × rainfall\n",
        "      .add(b0_ee)        # + intercept\n",
        "      .rename('ndvi_pred')\n",
        ")\n",
        "\n",
        "# ──────────────────────────────────────────────────────────────────────────────\n",
        "# 5a. Add the mean NDVI layer to the interactive map\n",
        "# ------------------------------------------------------------------------------\n",
        "m.addLayer(\n",
        "    ndvi_rain_yearly_stack.select('ndvi_2008').clip(south_africa),\n",
        "    ndvi_vis,\n",
        "    name=\"2008 NDVI (MOD13A3)\"\n",
        ")\n",
        "\n",
        "# ──────────────────────────────────────────────────────────────────────────────\n",
        "# 5b. Add the predicted NDVI layer to the interactive map `m`\n",
        "#    • Use the same visualization parameters as actual NDVI for comparison\n",
        "# ------------------------------------------------------------------------------\n",
        "m.addLayer(\n",
        "    ndvi_pred.clip(south_africa.geometry()),\n",
        "    ndvi_vis,  # reuse NDVI color scheme defined earlier\n",
        "    name=f\"Predicted NDVI (R²={r2:.2f})\"\n",
        ")\n",
        "\n",
        "# Display the map (in notebook or VS Code)\n",
        "m"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "95076063",
      "metadata": {},
      "source": [
        "---\n",
        "#### 10.\tSUPPLEMENTARY MATERIAL"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f312bc59",
      "metadata": {},
      "source": [
        "##### Palettes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f18d40d3",
      "metadata": {},
      "outputs": [],
      "source": [
        "elev_vis = {'min': 0, 'max': 2000,\n",
        "            'palette': ['#081d58', '#253494', '#225ea8',\n",
        "                        '#1d91c0', '#41b6c4', '#7fcdbb',\n",
        "                        '#c7e9b4', '#edf8b1', '#ffffd9']}\n",
        "\n",
        "rain_vis = {'min': 0, 'max': 22,\n",
        "            'palette': [\n",
        "                '#ffffd9', '#edf8b1', '#c7e9b4',\n",
        "                '#7fcdbb', '#41b6c4', '#1d91c0',\n",
        "                '#225ea8', '#253494', '#081d58'\n",
        "            ]}"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "bd853b11",
        "bb520c84",
        "KkjSft4_h9dc",
        "TdEOplXziMt1"
      ],
      "include_colab_link": true,
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
